{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMDeploy 部署 InternVL 浦语灵笔实践\n",
    "\n",
    "在本 notebook 中，我们将会使用 LMDeploy 进行 Mini-InternVL-Chat-2B-V1-5 和 InternLM-XComposer2-VL-1.8B 的部署与推理过程。\n",
    "\n",
    "首先来安装 LMDeploy："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install lmdeploy[all]==0.4.2\n",
    "!pip install timm\n",
    "!lmdeploy version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-InternVL-Chat-2B-V1-5\n",
    "\n",
    "我们将会使用 LMDeploy 启动 Mini-InternVL-Chat-2B-V1-5 的 Gradio 服务，并且用 `pipeline` 实现离线推理。\n",
    "\n",
    "首先我们来启动 Gradio 服务，然后打开下方所示的 share link。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!lmdeploy serve gradio OpenGVLab/Mini-InternVL-Chat-2B-V1-5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意结束上一单元格中服务！然后是使用 `pipeline` 进行离线推理。\n",
    "\n",
    "![image](https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/tests/data/tiger.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from lmdeploy.vl import load_image\n",
    "from lmdeploy import pipeline\n",
    "\n",
    "pipe = pipeline('OpenGVLab/Mini-InternVL-Chat-2B-V1-5')\n",
    "\n",
    "image = load_image('https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/tests/data/tiger.jpeg')\n",
    "response = pipe(('请描述图中内容', image))\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InternLM-XComposer2-VL-1.8B\n",
    "\n",
    "我们将会使用 LMDeploy 启动 MInternLM-XComposer2-VL-1.8B 的 Gradio 服务，并且用 `pipeline` 实现离线推理。\n",
    "\n",
    "首先我们来启动 Gradio 服务，然后打开下方所示的 share link。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!lmdeploy serve gradio internlm/internlm-xcomposer2-vl-1_8b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意结束上一单元格中服务！然后是使用 `pipeline` 进行离线推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from lmdeploy.vl import load_image\n",
    "from lmdeploy import pipeline\n",
    "\n",
    "pipe = pipeline('internlm/internlm-xcomposer2-vl-1_8b')\n",
    "\n",
    "image = load_image('https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/tests/data/tiger.jpeg')\n",
    "response = pipe(('请描述图中内容', image))\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
