# XTunerå¾®è°ƒä¸ªäººå°åŠ©æ‰‹è®¤çŸ¥

åœ¨æœ¬èŠ‚ä¸­ï¼Œå°†ä¸€æ­¥æ­¥å¸¦é¢†å¤§å®¶ä½“éªŒå¦‚ä½•ä½¿ç”¨ XTuner å®Œæˆä¸ªäººå°åŠ©æ‰‹çš„å¾®è°ƒï¼

## åŸºæœ¬æ¦‚å¿µ

åœ¨è¿›è¡Œå¾®è°ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£ä¸€äº›åŸºæœ¬æ¦‚å¿µã€‚

### Fintuneç®€ä»‹

å¾®è°ƒï¼ˆfine-tuningï¼‰æ˜¯ä¸€ç§åŸºäºé¢„è®­ç»ƒæ¨¡å‹ï¼Œé€šè¿‡å°‘é‡çš„è°ƒæ•´ï¼ˆfine-tuneï¼‰æ¥é€‚åº”æ–°çš„ä»»åŠ¡æˆ–æ•°æ®çš„æ–¹æ³•ã€‚

å¾®è°ƒæ˜¯åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œå°†æ¨¡å‹ä¸­ä¸€äº›å±‚çš„æƒé‡å‚æ•°è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”æ–°çš„æ•°æ®é›†æˆ–ä»»åŠ¡ã€‚

é¢„è®­ç»ƒæ¨¡å‹éƒ¨åˆ†å·²ç»åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šå¾—åˆ°äº†è®­ç»ƒï¼Œå®ƒä»¬é€šå¸¸æ˜¯è¾ƒä¸ºé€šç”¨ä¸”é«˜æ€§èƒ½çš„æ¨¡å‹ï¼Œå› æ­¤å¯ä»¥å¾ˆå¥½åœ°ä½œä¸ºæ–°ä»»åŠ¡çš„èµ·ç‚¹ã€‚å¾®è°ƒå¯ä»¥åŠ å¿«æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦ï¼Œé™ä½æ¨¡å‹è¿‡æ‹Ÿåˆçš„é£é™©ï¼Œå¹¶åœ¨ä¸æ¶ˆè€—è¿‡å¤šè®¡ç®—èµ„æºçš„æƒ…å†µä¸‹è·å–è¾ƒå¥½çš„æ¨¡å‹æ€§èƒ½ã€‚

#### Fintuneçš„ä¸¤ç§èŒƒå¼

åœ¨å¤§æ¨¡å‹çš„ä¸‹æ¸¸åº”ç”¨ä¸­ï¼Œç»å¸¸ä¼šç”¨åˆ°ä¸¤ç§å¾®è°ƒæ¨¡å¼ï¼š**å¢é‡é¢„è®­ç»ƒ** å’Œ **æŒ‡ä»¤è·Ÿéš** ã€‚

1. **å¢é‡é¢„è®­ç»ƒ**

å¢é‡é¢„è®­ç»ƒæ˜¯ä¸€ç§åœ¨å·²æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼ˆæ¯”å¦‚ï¼šInternLMåŸºåº§æ¨¡å‹ï¼‰çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨ç‰¹å®šé¢†åŸŸçš„æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒçš„æ–¹æ³•ã€‚å®ƒçš„ç›®çš„æ˜¯åœ¨ä¿æŒæ¨¡å‹åŸæœ‰èƒ½åŠ›çš„åŒæ—¶ï¼Œæ³¨å…¥æ–°çš„é¢†åŸŸçŸ¥è¯†ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ˆæ¯”å¦‚ï¼šInternLMå‚ç±»åŸºåº§æ¨¡å‹ï¼‰ã€‚å¢é‡é¢„è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿæ¥å—å°‘é‡çš„æ–°æ•°æ®è¿›è¡Œæ›´æ–°å¹¶é€‚åº”æ–°çš„ä»»åŠ¡ï¼Œè€Œä¸éœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹ï¼Œè¿™ç§æ–¹å¼å¯ä»¥å¾ˆå¥½åœ°åˆ©ç”¨ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†ï¼Œå¹¶åœ¨æ–°æ•°æ®ä¸Šè·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

2. **æŒ‡ä»¤è·Ÿéš**

æŒ‡ä»¤è·Ÿéšæ˜¯æŒ‡è®©æ¨¡å‹æ ¹æ®ç”¨æˆ·è¾“å…¥çš„æŒ‡ä»¤æ¥æ‰§è¡Œç›¸åº”çš„æ“ä½œã€‚æ¨¡å‹é€šè¿‡å¯¹å¤§é‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œç›¸åº”æ“ä½œçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ å¦‚ä½•å°†æŒ‡ä»¤åˆ†è§£ä¸ºå…·ä½“çš„å­ä»»åŠ¡ï¼Œå¹¶é€‰æ‹©åˆé€‚çš„æ¨¡å—æ¥æ‰§è¡Œè¿™äº›ä»»åŠ¡ï¼ˆæ¯”å¦‚ï¼šInternLMå‚ç±»å¯¹è¯æ¨¡å‹ï¼‰ã€‚

### å¾®è°ƒæŠ€æœ¯

å¤§å¤šæ•°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‚æ•°è§„æ¨¡å·¨å¤§ï¼Œä¸”è§„æ¨¡æ—¥ç›Šå¢å¤§ï¼Œå¯¼è‡´æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒæˆæœ¬é«˜æ˜‚ï¼Œç›´æ¥è®­ç»ƒéœ€è¦è€—è´¹å¤§é‡è®¡ç®—èµ„æºå’Œè´¹ç”¨ã€‚è¿‘å¹´æ¥ï¼Œå¦‚ä½•é«˜æ•ˆåœ°å¯¹å¤§æ¨¡å‹è¿›è¡Œå¾®è°ƒæˆä¸ºäº†ç ”ç©¶çƒ­ç‚¹ï¼Œè€ŒLoRAå’ŒQLoRAä¸¤ç§å¾®è°ƒæŠ€æœ¯å› å…¶é«˜æ•ˆæ€§å’Œå®ç”¨æ€§å—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚

#### LoRAç®€ä»‹

LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯ä¸€ç§ä½¿ç”¨ä½ç²¾åº¦æƒé‡å¯¹å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æŠ€æœ¯ï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨ä¸æ”¹å˜åŸæœ‰æ¨¡å‹æƒé‡çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æ·»åŠ å°‘é‡æ–°å‚æ•°æ¥è¿›è¡Œå¾®è°ƒã€‚è¿™ç§æ–¹æ³•é™ä½äº†æ¨¡å‹çš„å­˜å‚¨éœ€æ±‚ï¼Œä¹Ÿé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå®ç°äº†å¯¹å¤§æ¨¡å‹çš„å¿«é€Ÿé€‚åº”ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹æ€§èƒ½ã€‚

ç„¶è€Œï¼Œç”±äºä½¿ç”¨äº†ä½ç²¾åº¦æƒé‡ï¼ŒLoRAçš„ä¸€ä¸ªæ½œåœ¨çš„ç¼ºç‚¹æ˜¯åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šä¸¢å¤±ä¸€äº›åŸå§‹æ¨¡å‹çš„é«˜é˜¶ç‰¹å¾ä¿¡æ¯ï¼Œå› æ­¤å¯èƒ½ä¼šé™ä½æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

#### QLoRAç®€ä»‹

QLoRAï¼ˆQuantized LoRAï¼‰å¾®è°ƒæŠ€æœ¯æ˜¯å¯¹LoRAçš„ä¸€ç§æ”¹è¿›ï¼Œå®ƒé€šè¿‡å¼•å…¥é«˜ç²¾åº¦æƒé‡å’Œå¯å­¦ä¹ çš„ä½ç§©é€‚é…å™¨æ¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚å¹¶ä¸”åœ¨LoRAçš„åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†é‡åŒ–æŠ€æœ¯ã€‚é€šè¿‡å°†é¢„è®­ç»ƒæ¨¡å‹é‡åŒ–ä¸ºint4æ ¼å¼ï¼Œå¯ä»¥è¿›ä¸€æ­¥å‡å°‘å¾®è°ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—é‡ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥å‡å°‘æ¨¡å‹çš„å­˜å‚¨ç©ºé—´ï¼Œè¿™å¯¹äºåœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šè¿è¡Œæ¨¡å‹éå¸¸æœ‰ç”¨ã€‚æœ€ç»ˆï¼Œå¯ä»¥ä½¿æˆ‘ä»¬åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šè¿›è¡Œæ¨¡å‹çš„å¾®è°ƒè®­ç»ƒã€‚

###  XTunerç®€ä»‹

XTuner çš„å®˜æ–¹ä»“åº“æ˜¯ï¼šhttps://github.com/InternLM/xtuner ï¼ˆæ¬¢è¿Starï¼‰ï¼

XTuner ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹&å¤šæ¨¡æ€æ¨¡å‹å¾®è°ƒå·¥å…·ç®±ã€‚*ç”±* *MMRazor* *å’Œ* *MMDeploy* *è”åˆå¼€å‘ã€‚*

- ğŸ¤“ **å‚»ç“œåŒ–ï¼š** ä»¥ é…ç½®æ–‡ä»¶ çš„å½¢å¼å°è£…äº†å¤§éƒ¨åˆ†å¾®è°ƒåœºæ™¯ï¼Œ**0åŸºç¡€çš„éä¸“ä¸šäººå‘˜ä¹Ÿèƒ½ä¸€é”®å¼€å§‹å¾®è°ƒ**ã€‚
- ğŸƒ **è½»é‡çº§ï¼š** å¯¹äº 7B å‚æ•°é‡çš„LLMï¼Œ**å¾®è°ƒæ‰€éœ€çš„æœ€å°æ˜¾å­˜ä»…ä¸º 8GB** ï¼š **æ¶ˆè´¹çº§æ˜¾å¡âœ…ï¼Œcolabâœ…**

#### åŠŸèƒ½äº®ç‚¹

- é€‚é…å¤šç§ç”Ÿæ€
  - æ”¯æŒå¤šç§å¾®è°ƒç®—æ³•
  - é€‚é…å¤šç§å¼€æºç”Ÿæ€ï¼ˆHuggingFaceã€ModelScopeç­‰ï¼‰
  - è‡ªåŠ¨ä¼˜åŒ–åŠ é€Ÿå™¨
- é€‚é…å¤šç§ç¡¬ä»¶

#### å¸¸ç”¨å‘½ä»¤

ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸ç”¨çš„å‘½ä»¤ã€‚

- æŸ¥çœ‹å¸®åŠ©


```bash
xtuner help
```

- æŸ¥çœ‹ç‰ˆæœ¬


```bash
xtuner version
```

- åˆ—å‡ºæ‰€æœ‰é¢„å®šä¹‰é…ç½®æ–‡ä»¶


```bash
xtuner list-cfg
```

- åˆ—å‡ºåŒ…å«æŒ‡å®šåç§°çš„é¢„å®šä¹‰é…ç½®æ–‡ä»¶


```bash
xtuner list-cfg -p $NAME
```

- å¤åˆ¶é…ç½®æ–‡ä»¶


```bash
xtuner copy-cfg $CONFIG $SAVE_PATH
```

- æ‰§è¡Œå¾®è°ƒè®­ç»ƒ


```bash
xtuner train $CONFIG
```

- å°† pth æ ¼å¼çš„æ¨¡å‹æ–‡ä»¶è½¬æ¢æˆ HuggingFace æ ¼å¼çš„æ¨¡å‹


```bash
xtuner convert pth_to_hf $CONFIG $PATH_TO_PTH_MODEL $SAVE_PATH_TO_HF_MODEL
```

- å°†åŸå§‹æ¨¡å‹ä¸å¾®è°ƒç»“æœè¿›è¡Œåˆå¹¶


```bash
xtuner convert merge $LLM $ADAPTER $SAVE_PATH
```

## åˆ›å»ºå¼€å‘æœº

é¦–å…ˆæˆ‘ä»¬éœ€è¦å‰å¾€ [InternStudio](https://studio.intern-ai.org.cn/) ä¸­åˆ›å»ºä¸€å°å¼€å‘æœºè¿›è¡Œä½¿ç”¨ã€‚

<detail>

- ç™»å½•InternStudioåï¼Œåœ¨æ§åˆ¶å°ç‚¹å‡» â€œåˆ›å»ºå¼€å‘æœºâ€ æŒ‰é’®å¯ä»¥è¿›å…¥åˆ°å¼€å‘æœºçš„åˆ›å»ºç•Œé¢ã€‚

![](images/image-01.png)

- åœ¨ â€œåˆ›å»ºå¼€å‘æœºâ€ ç•Œé¢ï¼Œé€‰æ‹©å¼€å‘æœºç±»å‹ï¼šä¸ªäººå¼€å‘æœºï¼Œè¾“å…¥å¼€å‘æœºåç§°ï¼šXTunerå¾®è°ƒï¼Œé€‰æ‹©å¼€å‘æœºé•œåƒï¼šCuda11.7-condaã€‚

![](images/image-02.png)

- åœ¨é•œåƒè¯¦æƒ…ç•Œé¢ï¼Œç‚¹å‡» â€œä½¿ç”¨â€ é“¾æ¥ï¼Œç¡®è®¤ä½¿ç”¨è¯¥é•œåƒã€‚

![](images/image-03.png)

- èµ„æºé…ç½®å¯ä»¥é€‰æ‹© 30% ï¼Œç„¶åç‚¹å‡» â€œç«‹å³åˆ›å»ºâ€ æŒ‰é’®åˆ›å»ºå¼€å‘æœºã€‚

![](images/image-04.png)

- åˆ›å»ºå®Œæˆåï¼Œåœ¨å¼€å‘æœºåˆ—è¡¨ä¸­å¯ä»¥çœ‹åˆ°åˆšåˆ›å»ºçš„å¼€å‘æœºï¼Œç‚¹å‡» â€œè¿›å…¥å¼€å‘æœºâ€ é“¾æ¥å¯ä»¥è¿æ¥è¿›å…¥åˆ°å¼€å‘æœºã€‚

![](images/image-05.png)

## å‡†å¤‡å·¥ä½œ

1. **ç¯å¢ƒå®‰è£…**ï¼šæˆ‘ä»¬æƒ³è¦ç”¨ç®€å•æ˜“ä¸Šæ‰‹çš„å¾®è°ƒå·¥å…·åŒ… XTuner æ¥å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„è¯ï¼Œç¬¬ä¸€æ­¥æ˜¯å®‰è£… XTuner ï¼å®‰è£…åŸºç¡€çš„å·¥å…·æ˜¯ä¸€åˆ‡çš„å‰æï¼Œåªæœ‰å®‰è£…äº† XTuner æˆ‘ä»¬æ‰èƒ½å¤Ÿå»æ‰§è¡Œåç»­çš„æ“ä½œã€‚

2. **å‰æœŸå‡†å¤‡**ï¼šåœ¨å®Œæˆ XTuner çš„å®‰è£…åï¼Œæˆ‘ä»¬ä¸‹ä¸€æ­¥å°±éœ€è¦å»æ˜ç¡®æˆ‘ä»¬è‡ªå·±çš„å¾®è°ƒç›®æ ‡äº†ã€‚æˆ‘ä»¬æƒ³è¦åˆ©ç”¨å¾®è°ƒåšä¸€äº›ä»€ä¹ˆäº‹æƒ…å‘¢ï¼Œç„¶åä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡ç›¸å…³çš„ç¡¬ä»¶èµ„æºå’Œæ•°æ®ã€‚

3. **å¯åŠ¨å¾®è°ƒ**ï¼šåœ¨ç¡®å®šäº†è‡ªå·±çš„å¾®è°ƒç›®æ ‡åï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨ XTuner çš„é…ç½®åº“ä¸­æ‰¾åˆ°åˆé€‚çš„é…ç½®æ–‡ä»¶å¹¶è¿›è¡Œå¯¹åº”çš„ä¿®æ”¹ã€‚ä¿®æ”¹å®Œæˆåå³å¯ä¸€é”®å¯åŠ¨è®­ç»ƒï¼è®­ç»ƒå¥½çš„æ¨¡å‹ä¹Ÿå¯ä»¥ä»…ä»…é€šè¿‡åœ¨ç»ˆç«¯è¾“å…¥ä¸€è¡Œå‘½ä»¤æ¥å®Œæˆè½¬æ¢å’Œéƒ¨ç½²å·¥ä½œï¼

### åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

åœ¨å®‰è£… XTuner ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒã€‚åˆ›å»ºä¸€ä¸ªåä¸º `xtuner0121` çš„è™šæ‹Ÿç¯å¢ƒï¼Œå¯ä»¥ç›´æ¥æ‰§è¡Œå‘½ä»¤ã€‚


```bash
conda create -n xtuner0121 python=3.10 -y
```

å¦‚æœæ˜¯åœ¨å¼€å‘æœºä¸­ï¼Œä¹Ÿå¯ä»¥ç›´æ¥æ‰§è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œåˆ›å»ºï¼š


```bash
studio-conda -t xtuner0121 -o internlm-base
```

è™šæ‹Ÿç¯å¢ƒåˆ›å»ºå®Œæˆåï¼Œéœ€è¦æ¿€æ´»è™šæ‹Ÿç¯å¢ƒã€‚

```bash
conda activate xtuner0121
```

### å®‰è£… XTuner

è™šæ‹Ÿç¯å¢ƒåˆ›å»ºå®Œæˆåï¼Œå°±å¯ä»¥å®‰è£… XTuner äº†ã€‚é¦–å…ˆï¼Œä» Github ä¸Šä¸‹è½½æºç ã€‚


```bash
git clone -b v0.1.21  https://github.com/InternLM/xtuner
```

å…¶æ¬¡ï¼Œè¿›å…¥æºç ç›®å½•ï¼Œæ‰§è¡Œå®‰è£…ã€‚

> å¦‚æœé€Ÿåº¦å¤ªæ…¢å¯ä»¥æ¢æˆ `pip install -e '.[all]' -i https://mirrors.aliyun.com/pypi/simple/`


```bash
cd xtuner && pip install -e '.[all]'
```

æœ€åï¼Œæˆ‘ä»¬å¯ä»¥éªŒè¯ä¸€ä¸‹å®‰è£…ç»“æœã€‚


```bash
xtuner version
```

å¯¹äºå¾ˆå¤šåˆå­¦è€…è€Œè¨€ï¼Œæˆ‘ä»¬å¯èƒ½ä¸å¤ªç†Ÿæ‚‰ XTuner çš„ç”¨æ³•ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¥æŸ¥çœ‹ç›¸å…³çš„å¸®åŠ©ã€‚


```bash
xtuner help
```

å¯¹äºå¾ˆå¤šçš„åˆå­¦è€…è€Œè¨€ï¼Œå®‰è£…å¥½ç¯å¢ƒæ„å‘³ç€æˆåŠŸäº†ä¸€å¤§åŠï¼å› æ­¤æˆ‘ä»¬æ¥ä¸‹æ¥å°±å¯ä»¥è¿›å…¥æˆ‘ä»¬çš„ä¸‹ä¸€æ­¥ï¼Œå‡†å¤‡å¥½æˆ‘ä»¬éœ€è¦çš„æ¨¡å‹ã€æ•°æ®é›†å’Œé…ç½®æ–‡ä»¶ï¼Œå¹¶è¿›è¡Œå¾®è°ƒè®­ç»ƒï¼

### æ¨¡å‹å‡†å¤‡

è½¯ä»¶å®‰è£…å¥½åï¼Œæˆ‘ä»¬å°±å¯ä»¥å‡†å¤‡è¦å¾®è°ƒçš„æ¨¡å‹äº†ã€‚

> å¯¹äºå­¦ä¹ è€Œè¨€ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ InternLM æ¨å‡ºçš„1.8Bçš„å°æ¨¡å‹æ¥å®Œæˆæ­¤æ¬¡å¾®è°ƒæ¼”ç¤ºã€‚

å¯¹äºåœ¨ InternStudio ä¸Šè¿è¡Œçš„å°ä¼™ä¼´ä»¬ï¼Œå¯ä»¥ä¸ç”¨é€šè¿‡ HuggingFaceã€OpenXLab æˆ–è€… Modelscope è¿›è¡Œæ¨¡å‹çš„ä¸‹è½½ï¼Œåœ¨å¼€å‘æœºä¸­å·²ç»ä¸ºæˆ‘ä»¬æä¾›äº†æ¨¡å‹çš„æœ¬åœ°æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨å°±å¯ä»¥äº†ã€‚

> æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç ä¸€é”®é€šè¿‡ç¬¦å·é“¾æ¥çš„æ–¹å¼é“¾æ¥åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¿™æ ·æ—¢èŠ‚çœäº†ç©ºé—´ï¼Œä¹Ÿä¾¿äºç®¡ç†ã€‚


```bash
mkdir -p Shanghai_AI_Laboratory

ln -s /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b Shanghai_AI_Laboratory/internlm2-chat-1_8b
```

æ‰§è¡Œä¸Šè¿°æ“ä½œåï¼Œ`Shanghai_AI_Laboratory/internlm2-chat-1_8b` å°†ç›´æ¥æˆä¸ºä¸€ä¸ªç¬¦å·é“¾æ¥ï¼Œè¿™ä¸ªé“¾æ¥æŒ‡å‘ `/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b` çš„ä½ç½®ã€‚

è¿™æ„å‘³ç€ï¼Œå½“æˆ‘ä»¬è®¿é—® `Shanghai_AI_Laboratory/internlm2-chat-1_8b` æ—¶ï¼Œå®é™…ä¸Šå°±æ˜¯åœ¨è®¿é—® `/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b` ç›®å½•ä¸‹çš„å†…å®¹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬æ— éœ€å¤åˆ¶ä»»ä½•æ•°æ®ï¼Œå°±å¯ä»¥ç›´æ¥åˆ©ç”¨ç°æœ‰çš„æ¨¡å‹æ–‡ä»¶è¿›è¡Œåç»­çš„å¾®è°ƒæ“ä½œï¼Œä»è€ŒèŠ‚çœå­˜å‚¨ç©ºé—´å¹¶ç®€åŒ–æ–‡ä»¶ç®¡ç†ã€‚

å¦‚æœè‡ªå·±æƒ³è¦å¾®è°ƒçš„æ¨¡å‹åœ¨å¼€å‘æœºä¸­æ²¡æ‰¾åˆ°ï¼Œä¹Ÿå¯ä»¥è‡ªå·±ä¸‹è½½ç›¸å…³æ¨¡å‹æ–‡ä»¶ã€‚


```python
from modelscope import snapshot_download
model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm2-1_8b', cache_dir="./")
```

æ¨¡å‹æ–‡ä»¶å‡†å¤‡å¥½åï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™ä¸ªæ ·å­çš„ã€‚

<details>
<summary>ç›®å½•ç»“æ„</summary>

```
â”œâ”€â”€ Shanghai_AI_Laboratory
â”‚   â”œâ”€â”€ internlm2-1_8b
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ configuration.json
â”‚   â”‚   â”œâ”€â”€ configuration_internlm2.py
â”‚   â”‚   â”œâ”€â”€ generation_config.json
â”‚   â”‚   â”œâ”€â”€ modeling_internlm2.py
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2.py
â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2_fast.py
â”‚   â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”‚   â”œâ”€â”€ tokenizer.model
â”‚   â”‚   â””â”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ internlm2-chat-1_8b -> /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ configuration.json
â”‚       â”œâ”€â”€ configuration_internlm2.py
â”‚       â”œâ”€â”€ generation_config.json
â”‚       â”œâ”€â”€ model-00001-of-00002.safetensors
â”‚       â”œâ”€â”€ model-00002-of-00002.safetensors
â”‚       â”œâ”€â”€ model.safetensors.index.json
â”‚       â”œâ”€â”€ modeling_internlm2.py
â”‚       â”œâ”€â”€ special_tokens_map.json
â”‚       â”œâ”€â”€ tokenization_internlm2.py
â”‚       â”œâ”€â”€ tokenization_internlm2_fast.py
â”‚       â”œâ”€â”€ tokenizer.model
â”‚       â””â”€â”€ tokenizer_config.json
```
</details>


> åœ¨ç›®å½•ç»“æ„ä¸­å¯ä»¥çœ‹å‡ºï¼Œ`internlm2-chat-1_8b` æ˜¯ä¸€ä¸ªç¬¦å·é“¾æ¥ã€‚


```bash
tree -l
```

## å¢é‡é¢„è®­ç»ƒå¾®è°ƒ

æœ¬èŠ‚æˆ‘ä»¬å…ˆæ¥äº†è§£ä¸€ä¸‹å¢é‡é¢„è®­ç»ƒï¼Œè¿™é‡Œæˆ‘ä»¬ä»¥ä¸€ä¸ªæ–‡æœ¬ç»­å†™æ¡ˆä¾‹æ¥çœ‹çœ‹æ•ˆæœã€‚

|  | å¾®è°ƒå‰ | å¾®è°ƒå |
| --- | --- | --- |
| è¾“å…¥ | ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯ | ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯ |
| è¾“å‡º| ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯ä¸Šå‘¨äº”ï¼Œä¸Šå‘¨äº”æˆ‘ä»¬å­¦ä¹ äº†ä¸€ä¸ªæ–°çš„çŸ¥è¯†ï¼Œé‚£å°±æ˜¯å…³äºæœºå™¨å­¦ä¹ çš„æ¦‚ç‡ç»Ÿè®¡ã€‚â€¦â€¦ | ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤æ¨å‡ºçš„ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç³»åˆ—æ´»åŠ¨çš„ç¬¬ä¸‰æ‰¹æ¬¡ï¼Œå°†äº2024å¹´7æœˆæ­£å¼è¿›è¡Œã€‚â€¦â€¦ |

- å¯¼å…¥å¿…è¦çš„åº“


```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
```

- å®šä¹‰æ¨¡å‹åŠ è½½æ–¹æ³•


```python
def load_model(model_path):
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True).cuda()
    model = model.eval()
    return tokenizer, model
```

- å®šä¹‰æ–‡æœ¬ç»­å†™æ–¹æ³•


```python
def generate(user_input):
    gen_kwargs = {"max_length": 128, "top_p": 0.8, "temperature": 0.8, "do_sample": True, "repetition_penalty": 1.0}

    inputs = tokenizer([user_input], return_tensors="pt")
    for k,v in inputs.items():
        inputs[k] = v.cuda()
    output = model.generate(**inputs, **gen_kwargs)
    output = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)
    return output
```

### åŸºåº§æ¨¡å‹æ¨ç†

æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹åŸºåº§æ¨¡å‹çš„æ¨ç†ç»“æœã€‚

- åŠ è½½æ¨¡å‹


```python
tokenizer, model = load_model("Shanghai_AI_Laboratory/internlm2-1_8b")
```

- æ–‡æœ¬ç»­å†™


```python
generate("ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯")
```

- é‡Šæ”¾ç¼“å­˜


```python
del tokenizer, model

torch.cuda.empty_cache()
```

### å¢é‡é¢„è®­ç»ƒ

ç„¶åæˆ‘ä»¬å¯¹åŸºåº§æ¨¡å‹è¿›è¡Œå¢é‡é¢„è®­ç»ƒï¼Œè®©æ¨¡å‹å¢åŠ æ–°çš„çŸ¥è¯†ã€‚

#### å‡†å¤‡æ•°æ®æ–‡ä»¶

ä¸ºäº†è®©æ¨¡å‹å­¦ä¹ åˆ°æ–°çš„çŸ¥è¯†ï¼Œæˆ‘ä»¬éœ€è¦å°†æ–°çš„çŸ¥è¯†æ•°æ®æ•´ç†æˆæŒ‡å®šæ ¼å¼æ–‡ä»¶ï¼Œå½¢æˆæ•°æ®é›†ï¼Œç„¶åè®©æ¨¡å‹æ¥å­¦ä¹ è¿™äº›æ–°æ•°æ®ã€‚è¿™é‡Œæˆ‘ä»¬å‡†å¤‡ä¸€ä¸ªç®€å•çš„æ•°æ®é›† `datas/pretrain.json`ï¼Œä»…åŒ…å«ä¸€æ¡çŸ¥è¯†ï¼Œç„¶åè®©æ•°æ®é‡å¤å¤šæ¬¡ã€‚

> ç½‘ä¸Šæœ‰å¤§é‡çš„å¼€æºæ•°æ®é›†å¯ä»¥ä¾›æˆ‘ä»¬è¿›è¡Œä½¿ç”¨ï¼Œæœ‰äº›æ—¶å€™æˆ‘ä»¬å¯ä»¥åœ¨å¼€æºæ•°æ®é›†çš„åŸºç¡€ä¸Šæ·»åŠ ä¸€äº›æˆ‘ä»¬è‡ªå·±ç‹¬æœ‰çš„æ•°æ®é›†ï¼Œä¹Ÿå¯èƒ½ä¼šæœ‰å¾ˆå¥½çš„æ•ˆæœã€‚


```python
[
    {
        "text": "ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤æ¨å‡ºçš„ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç³»åˆ—æ´»åŠ¨çš„ç¬¬ä¸‰æ‰¹æ¬¡ï¼Œå°†äº2024å¹´7æœˆæ­£å¼è¿›è¡Œã€‚"
    }
]
```

å‡†å¤‡å¥½æ•°æ®æ–‡ä»¶åï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚

<details>
<summary>ç›®å½•ç»“æ„</summary>

```
â”œâ”€â”€ Shanghai_AI_Laboratory
â”‚   â”œâ”€â”€ internlm2-1_8b
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ configuration.json
â”‚   â”‚   â”œâ”€â”€ configuration_internlm2.py
â”‚   â”‚   â”œâ”€â”€ generation_config.json
â”‚   â”‚   â”œâ”€â”€ modeling_internlm2.py
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2.py
â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2_fast.py
â”‚   â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”‚   â”œâ”€â”€ tokenizer.model
â”‚   â”‚   â””â”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ internlm2-chat-1_8b -> /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ configuration.json
â”‚       â”œâ”€â”€ configuration_internlm2.py
â”‚       â”œâ”€â”€ generation_config.json
â”‚       â”œâ”€â”€ model-00001-of-00002.safetensors
â”‚       â”œâ”€â”€ model-00002-of-00002.safetensors
â”‚       â”œâ”€â”€ model.safetensors.index.json
â”‚       â”œâ”€â”€ modeling_internlm2.py
â”‚       â”œâ”€â”€ special_tokens_map.json
â”‚       â”œâ”€â”€ tokenization_internlm2.py
â”‚       â”œâ”€â”€ tokenization_internlm2_fast.py
â”‚       â”œâ”€â”€ tokenizer.model
â”‚       â””â”€â”€ tokenizer_config.json
â”œâ”€â”€ datas
â”‚   â””â”€â”€ pretrain.json
```

</details>



```bash
tree -l
```

#### å‡†å¤‡é…ç½®æ–‡ä»¶

åœ¨å‡†å¤‡å¥½äº†æ¨¡å‹å’Œæ•°æ®é›†åï¼Œæˆ‘ä»¬å°±è¦æ ¹æ®æˆ‘ä»¬é€‰æ‹©çš„å¾®è°ƒæ–¹æ³•ç»“åˆå¾®è°ƒæ–¹æ¡ˆæ¥æ‰¾åˆ°ä¸æˆ‘ä»¬æœ€åŒ¹é…çš„é…ç½®æ–‡ä»¶äº†ï¼Œä»è€Œå‡å°‘æˆ‘ä»¬å¯¹é…ç½®æ–‡ä»¶çš„ä¿®æ”¹é‡ã€‚

> é…ç½®æ–‡ä»¶å…¶å®æ˜¯ä¸€ç§ç”¨äºå®šä¹‰å’Œæ§åˆ¶æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­å„ä¸ªæ–¹é¢çš„å‚æ•°å’Œè®¾ç½®çš„å·¥å…·ã€‚

##### åˆ—å‡ºæ”¯æŒçš„é…ç½®æ–‡ä»¶

XTuner æä¾›å¤šä¸ªå¼€ç®±å³ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ã€‚

> `xtuner list-cfg` å‘½ä»¤ç”¨äºåˆ—å‡ºå†…ç½®çš„æ‰€æœ‰é…ç½®æ–‡ä»¶ã€‚å‚æ•° `-p` æˆ– `--pattern` è¡¨ç¤ºæ¨¡å¼åŒ¹é…ï¼Œåé¢è·Ÿç€çš„å†…å®¹å°†ä¼šåœ¨æ‰€æœ‰çš„é…ç½®æ–‡ä»¶é‡Œè¿›è¡Œæ¨¡ç³ŠåŒ¹é…æœç´¢ï¼Œç„¶åè¿”å›æœ€æœ‰å¯èƒ½å¾—å†…å®¹ã€‚æ¯”å¦‚æˆ‘ä»¬è¿™é‡Œå¾®è°ƒçš„æ˜¯ä¹¦ç”ŸÂ·æµ¦è¯­çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥åŒ¹é…æœç´¢ `internlm2`ã€‚


```bash
xtuner list-cfg -p internlm2
```

<details>
<summary>é…ç½®æ–‡ä»¶åçš„è§£é‡Š</summary>

ä»¥ **internlm2_1_8b_full_custom_pretrain_e1** å’Œ **internlm2_chat_1_8b_qlora_alpaca_e3** ä¸¾ä¾‹ï¼š

| é…ç½®æ–‡ä»¶ internlm2_1_8b_full_custom_pretrain_e1 | é…ç½®æ–‡ä»¶ internlm2_chat_1_8b_qlora_alpaca_e3 | è¯´æ˜ |
| -------- | -------- | ------------- |
| internlm2_1_8b | internlm2_chat_1_8b | æ¨¡å‹åç§° |
| full    | qlora    | ä½¿ç”¨çš„ç®—æ³•     |
| custom_pretrain   | alpaca   | æ•°æ®é›†åç§°     |
| e1       | e3       | æŠŠæ•°æ®é›†è·‘å‡ æ¬¡  |

</details>

##### å¤åˆ¶ä¸€ä¸ªé¢„è®¾çš„é…ç½®æ–‡ä»¶

ç”±äºæˆ‘ä»¬æ˜¯å¯¹åŸºåº§æ¨¡å‹è¿›è¡Œå¢é‡é¢„è®­ç»ƒï¼Œæ‰€ä»¥ä¸æˆ‘ä»¬çš„éœ€æ±‚æœ€åŒ¹é…çš„é…ç½®æ–‡ä»¶æ˜¯ `internlm2_1_8b_full_custom_pretrain_e1`ï¼Œè¿™é‡Œå°±å¤åˆ¶è¯¥é…ç½®æ–‡ä»¶ã€‚

> `xtuner copy-cfg` å‘½ä»¤ç”¨äºå¤åˆ¶ä¸€ä¸ªå†…ç½®çš„é…ç½®æ–‡ä»¶ã€‚è¯¥å‘½ä»¤éœ€è¦ä¸¤ä¸ªå‚æ•°ï¼š`CONFIG` ä»£è¡¨éœ€è¦å¤åˆ¶çš„é…ç½®æ–‡ä»¶åç§°ï¼Œ`SAVE_PATH` ä»£è¡¨å¤åˆ¶çš„ç›®æ ‡è·¯å¾„ã€‚åœ¨æˆ‘ä»¬çš„è¾“å…¥çš„è¿™ä¸ªå‘½ä»¤ä¸­ï¼Œæˆ‘ä»¬çš„ `CONFIG` å¯¹åº”çš„æ˜¯ä¸Šé¢æœç´¢åˆ°çš„ `internlm2_1_8b_full_custom_pretrain_e1` ,è€Œ `SAVE_PATH` åˆ™æ˜¯å½“å‰ç›®å½• `.`ã€‚


```bash
xtuner copy-cfg internlm2_1_8b_full_custom_pretrain_e1 .
```

å¤åˆ¶å¥½é…ç½®æ–‡ä»¶åï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚

<details>
<summary>ç›®å½•ç»“æ„</summary>

```
â”œâ”€â”€ Shanghai_AI_Laboratory
â”‚   â”œâ”€â”€ internlm2-1_8b
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ configuration.json
â”‚   â”‚   â”œâ”€â”€ configuration_internlm2.py
â”‚   â”‚   â”œâ”€â”€ generation_config.json
â”‚   â”‚   â”œâ”€â”€ modeling_internlm2.py
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2.py
â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2_fast.py
â”‚   â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”‚   â”œâ”€â”€ tokenizer.model
â”‚   â”‚   â””â”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ internlm2-chat-1_8b -> /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ configuration.json
â”‚       â”œâ”€â”€ configuration_internlm2.py
â”‚       â”œâ”€â”€ generation_config.json
â”‚       â”œâ”€â”€ model-00001-of-00002.safetensors
â”‚       â”œâ”€â”€ model-00002-of-00002.safetensors
â”‚       â”œâ”€â”€ model.safetensors.index.json
â”‚       â”œâ”€â”€ modeling_internlm2.py
â”‚       â”œâ”€â”€ special_tokens_map.json
â”‚       â”œâ”€â”€ tokenization_internlm2.py
â”‚       â”œâ”€â”€ tokenization_internlm2_fast.py
â”‚       â”œâ”€â”€ tokenizer.model
â”‚       â””â”€â”€ tokenizer_config.json
â”œâ”€â”€ datas
â”‚   â””â”€â”€ pretrain.json
â”œâ”€â”€ internlm2_1_8b_full_custom_pretrain_e1_copy.py
```

</details>

##### å¯¹é…ç½®æ–‡ä»¶è¿›è¡Œä¿®æ”¹

åœ¨é€‰æ‹©äº†ä¸€ä¸ªæœ€åŒ¹é…çš„é…ç½®æ–‡ä»¶å¹¶å‡†å¤‡å¥½å…¶ä»–å†…å®¹åï¼Œä¸‹é¢æˆ‘ä»¬è¦åšçš„äº‹æƒ…å°±æ˜¯æ ¹æ®æˆ‘ä»¬è‡ªå·±çš„å†…å®¹å¯¹è¯¥é…ç½®æ–‡ä»¶è¿›è¡Œè°ƒæ•´ï¼Œä½¿å…¶èƒ½å¤Ÿæ»¡è¶³æˆ‘ä»¬å®é™…è®­ç»ƒçš„è¦æ±‚ã€‚

<details>
<summary><b>é…ç½®æ–‡ä»¶ä»‹ç»</b></summary>
 
æ‰“å¼€é…ç½®æ–‡ä»¶åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ•´ä½“çš„é…ç½®æ–‡ä»¶åˆ†ä¸ºäº”éƒ¨åˆ†ï¼š

1. **PART 1 Settings**ï¼šæ¶µç›–äº†æ¨¡å‹åŸºæœ¬è®¾ç½®ï¼Œå¦‚é¢„è®­ç»ƒæ¨¡å‹çš„é€‰æ‹©ã€æ•°æ®é›†ä¿¡æ¯å’Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸€äº›åŸºæœ¬å‚æ•°ï¼ˆå¦‚æ‰¹å¤§å°ã€å­¦ä¹ ç‡ç­‰ï¼‰ã€‚

2. **PART 2 Model & Tokenizer**ï¼šæŒ‡å®šäº†ç”¨äºè®­ç»ƒçš„æ¨¡å‹å’Œåˆ†è¯å™¨çš„å…·ä½“ç±»å‹åŠå…¶é…ç½®ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„å’Œæ˜¯å¦å¯ç”¨ç‰¹å®šåŠŸèƒ½ï¼ˆå¦‚å¯å˜é•¿åº¦æ³¨æ„åŠ›ï¼‰ï¼Œè¿™æ˜¯æ¨¡å‹è®­ç»ƒçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚

3. **PART 3 Dataset & Dataloader**ï¼šæè¿°äº†æ•°æ®å¤„ç†çš„ç»†èŠ‚ï¼ŒåŒ…æ‹¬å¦‚ä½•åŠ è½½æ•°æ®é›†ã€é¢„å¤„ç†æ­¥éª¤ã€æ‰¹å¤„ç†å¤§å°ç­‰ï¼Œç¡®ä¿äº†æ¨¡å‹èƒ½å¤Ÿæ¥æ”¶åˆ°æ­£ç¡®æ ¼å¼å’Œè´¨é‡çš„æ•°æ®ã€‚

4. **PART 4 Scheduler & Optimizer**ï¼šé…ç½®äº†ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„å…³é”®å‚æ•°ï¼Œå¦‚å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥å’Œä¼˜åŒ–å™¨çš„é€‰æ‹©ï¼Œè¿™äº›æ˜¯å½±å“æ¨¡å‹è®­ç»ƒæ•ˆæœå’Œé€Ÿåº¦çš„é‡è¦å› ç´ ã€‚

5. **PART 5 Runtime**ï¼šå®šä¹‰äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„é¢å¤–è®¾ç½®ï¼Œå¦‚æ—¥å¿—è®°å½•ã€æ¨¡å‹ä¿å­˜ç­–ç•¥å’Œè‡ªå®šä¹‰é’©å­ç­‰ï¼Œä»¥æ”¯æŒè®­ç»ƒæµç¨‹çš„ç›‘æ§ã€è°ƒè¯•å’Œç»“æœçš„ä¿å­˜ã€‚

ä¸€èˆ¬æ¥è¯´æˆ‘ä»¬éœ€è¦æ›´æ”¹çš„éƒ¨åˆ†å…¶å®åªåŒ…æ‹¬å‰ä¸‰éƒ¨åˆ†ï¼Œè€Œä¸”ä¿®æ”¹çš„ä¸»è¦åŸå› æ˜¯æˆ‘ä»¬ä¿®æ”¹äº†é…ç½®æ–‡ä»¶ä¸­è§„å®šçš„æ¨¡å‹ã€æ•°æ®é›†ã€‚åä¸¤éƒ¨åˆ†éƒ½æ˜¯ XTuner å®˜æ–¹å¸®æˆ‘ä»¬ä¼˜åŒ–å¥½çš„ä¸œè¥¿ï¼Œä¸€èˆ¬è€Œè¨€åªæœ‰åœ¨é­”æ”¹çš„æƒ…å†µä¸‹æ‰éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚

</details>

ä¸‹é¢æˆ‘ä»¬å°†æ ¹æ®é¡¹ç›®çš„éœ€æ±‚ä¸€æ­¥æ­¥çš„è¿›è¡Œä¿®æ”¹å’Œè°ƒæ•´å§ï¼

åœ¨ PART 1 çš„éƒ¨åˆ†ï¼Œç”±äºæˆ‘ä»¬ä¸å†éœ€è¦åœ¨ HuggingFace ä¸Šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å…ˆè¦æ›´æ¢æ¨¡å‹çš„è·¯å¾„ä»¥åŠæ•°æ®é›†çš„è·¯å¾„ä¸ºæˆ‘ä»¬æœ¬åœ°çš„è·¯å¾„ã€‚

ä¸ºäº†è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿå®æ—¶è§‚å¯Ÿåˆ°æ¨¡å‹çš„å˜åŒ–æƒ…å†µï¼ŒXTuner è´´å¿ƒçš„æ¨å‡ºäº†ä¸€ä¸ª `evaluation_inputs` çš„å‚æ•°æ¥è®©æˆ‘ä»¬èƒ½å¤Ÿè®¾ç½®å¤šä¸ªé—®é¢˜æ¥ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–æ˜¯æœç€æˆ‘ä»¬æƒ³è¦çš„æ–¹å‘å‰è¿›çš„ã€‚æˆ‘ä»¬å¯ä»¥æ·»åŠ è‡ªå·±çš„è¾“å…¥ã€‚

åœ¨ PART 2 çš„éƒ¨åˆ†ï¼Œç”±äºæˆ‘ä»¬å¤åˆ¶çš„é…ç½®æ–‡ä»¶æ˜¯å…¨å‚æ•°å¾®è°ƒçš„é…ç½®ï¼Œè€Œæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ `QLoRA` ç®—æ³•è¿›è¡Œå¾®è°ƒï¼Œæ‰€ä»¥å¯ä»¥æ·»åŠ  `QLoRA` ç®—æ³•çš„é…ç½®ã€‚

```diff
+ from peft import LoraConfig

+ import torch

- from transformers import AutoModelForCausalLM, AutoTokenizer
+ from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

#######################################################################
#                          PART 1  Settings                           #
#######################################################################
- pretrained_model_name_or_path = 'internlm/internlm2-1_8b'
+ pretrained_model_name_or_path = 'Shanghai_AI_Laboratory/internlm2-1_8b'

- data_files = ['/path/to/json/file.json']
+ data_files = ['datas/pretrain.json']

- evaluation_inputs = ['ä¸Šæµ·æ˜¯', 'Shanghai is']
+ evaluation_inputs = ['ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯', 'ä¸Šæµ·æ˜¯', 'Shanghai is']

#######################################################################
#                      PART 2  Model & Tokenizer                      #
#######################################################################
model = dict(
    type=SupervisedFinetune,
    use_varlen_attn=use_varlen_attn,
    llm=dict(
        type=AutoModelForCausalLM.from_pretrained,
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        trust_remote_code=True,
+       quantization_config=dict(
+           type=BitsAndBytesConfig,
+           load_in_4bit=True,
+           load_in_8bit=False,
+           llm_int8_threshold=6.0,
+           llm_int8_has_fp16_weight=False,
+           bnb_4bit_compute_dtype=torch.float16,
+           bnb_4bit_use_double_quant=True,
+           bnb_4bit_quant_type='nf4')
    ),
+   lora=dict(
+       type=LoraConfig,
+       r=64,
+       lora_alpha=16,
+       lora_dropout=0.1,
+       bias='none',
+       task_type='CAUSAL_LM')
)
```

é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥å¯¹ä¸€äº›é‡è¦çš„å‚æ•°è¿›è¡Œè°ƒæ•´ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡ï¼ˆlrï¼‰ã€è®­ç»ƒçš„è½®æ•°ï¼ˆmax_epochsï¼‰ç­‰ç­‰ã€‚

<details>
<summary>å¸¸ç”¨å‚æ•°ä»‹ç»</summary>

| å‚æ•°å                  | è§£é‡Š                                                     |
| ----------------------- | -------------------------------------------------------- |
| **data_path**           | æ•°æ®è·¯å¾„æˆ– HuggingFace ä»“åº“å                             |
| **max_length**          | å•æ¡æ•°æ®æœ€å¤§ Token æ•°ï¼Œè¶…è¿‡åˆ™æˆªæ–­                         |
| **pack_to_max_length**  | æ˜¯å¦å°†å¤šæ¡çŸ­æ•°æ®æ‹¼æ¥åˆ° max_lengthï¼Œæé«˜ GPU åˆ©ç”¨ç‡        |
| **accumulative_counts** | æ¢¯åº¦ç´¯ç§¯ï¼Œæ¯å¤šå°‘æ¬¡ backward æ›´æ–°ä¸€æ¬¡å‚æ•°                  |
| **sequence_parallel_size** | å¹¶è¡Œåºåˆ—å¤„ç†çš„å¤§å°ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒæ—¶çš„åºåˆ—å¹¶è¡Œ              |
| **batch_size**          | æ¯ä¸ªè®¾å¤‡ä¸Šçš„æ‰¹é‡å¤§å°                                      |
| **dataloader_num_workers** | æ•°æ®åŠ è½½å™¨ä¸­å·¥ä½œè¿›ç¨‹çš„æ•°é‡                                |
| **max_epochs**          | è®­ç»ƒçš„æœ€å¤§è½®æ•°                                             |
| **optim_type**          | ä¼˜åŒ–å™¨ç±»å‹ï¼Œä¾‹å¦‚ AdamW                                    |
| **lr**                  | å­¦ä¹ ç‡                                                    |
| **betas**               | ä¼˜åŒ–å™¨ä¸­çš„ beta å‚æ•°ï¼Œæ§åˆ¶åŠ¨é‡å’Œå¹³æ–¹æ¢¯åº¦çš„ç§»åŠ¨å¹³å‡         |
| **weight_decay**        | æƒé‡è¡°å‡ç³»æ•°ï¼Œç”¨äºæ­£åˆ™åŒ–å’Œé¿å…è¿‡æ‹Ÿåˆ                      |
| **max_norm**            | æ¢¯åº¦è£å‰ªçš„æœ€å¤§èŒƒæ•°ï¼Œç”¨äºé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸                      |
| **warmup_ratio**        | é¢„çƒ­çš„æ¯”ä¾‹ï¼Œå­¦ä¹ ç‡åœ¨è¿™ä¸ªæ¯”ä¾‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­çº¿æ€§å¢åŠ åˆ°åˆå§‹å­¦ä¹ ç‡ |
| **save_steps**          | ä¿å­˜æ¨¡å‹çš„æ­¥æ•°é—´éš”                                         |
| **save_total_limit**    | ä¿å­˜çš„æ¨¡å‹æ€»æ•°é™åˆ¶ï¼Œè¶…è¿‡é™åˆ¶æ—¶åˆ é™¤æ—§çš„æ¨¡å‹æ–‡ä»¶             |
| **prompt_template**     | æ¨¡æ¿æç¤ºï¼Œç”¨äºå®šä¹‰ç”Ÿæˆæ–‡æœ¬çš„æ ¼å¼æˆ–ç»“æ„                    |
| ...... | ...... |

> å¦‚æœæƒ³å……åˆ†åˆ©ç”¨æ˜¾å¡èµ„æºï¼Œå¯ä»¥å°† `max_length` å’Œ `batch_size` è¿™ä¸¤ä¸ªå‚æ•°è°ƒå¤§ã€‚

</details>

ä¿®æ”¹å®Œåçš„å®Œæ•´çš„é…ç½®æ–‡ä»¶æ˜¯ï¼š

<details>
<summary>internlm2_1_8b_full_custom_pretrain_e1_copy.py</summary>

```python
# Copyright (c) OpenMMLab. All rights reserved.
"""Data format:

[
  {
      "text": "xxx"
  },
  {
      "text": "xxx"
  },
  ...
]
"""  # noqa: E501

from datasets import load_dataset
from mmengine.dataset import DefaultSampler
from mmengine.hooks import (CheckpointHook, DistSamplerSeedHook, IterTimerHook,
                            LoggerHook, ParamSchedulerHook)
from mmengine.optim import AmpOptimWrapper, CosineAnnealingLR, LinearLR
from peft import LoraConfig
import torch
from torch.optim import AdamW
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

from xtuner.dataset import process_hf_dataset
from xtuner.dataset.collate_fns import default_collate_fn
from xtuner.dataset.map_fns import pretrain_map_fn
from xtuner.engine.hooks import (DatasetInfoHook, EvaluateChatHook,
                                 VarlenAttnArgsToMessageHubHook)
from xtuner.engine.runner import TrainLoop
from xtuner.model import SupervisedFinetune

#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Model
pretrained_model_name_or_path = 'Shanghai_AI_Laboratory/internlm2-1_8b'
use_varlen_attn = False

# Data
data_files = ['datas/pretrain.json']
max_length = 2048
pack_to_max_length = True

# Scheduler & Optimizer
batch_size = 1  # per_device
accumulative_counts = 16  # bs = 1 GPU * 1 batch_size_per_device * 16 acc
dataloader_num_workers = 0
max_epochs = 1
optim_type = AdamW
lr = 2e-5
betas = (0.9, 0.999)
weight_decay = 0
max_norm = 1  # grad clip
warmup_ratio = 0.03

# Save
save_steps = 500
save_total_limit = 2  # Maximum checkpoints to keep (-1 means unlimited)

# Evaluate the generation performance during the training
evaluation_freq = 500
SYSTEM = ''
evaluation_inputs = ['ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯', 'ä¸Šæµ·æ˜¯', 'Shanghai is']

#######################################################################
#                      PART 2  Model & Tokenizer                      #
#######################################################################
tokenizer = dict(
    type=AutoTokenizer.from_pretrained,
    pretrained_model_name_or_path=pretrained_model_name_or_path,
    trust_remote_code=True,
    padding_side='right')

model = dict(
    type=SupervisedFinetune,
    use_varlen_attn=use_varlen_attn,
    llm=dict(
        type=AutoModelForCausalLM.from_pretrained,
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        trust_remote_code=True,
        quantization_config=dict(
            type=BitsAndBytesConfig,
            load_in_4bit=True,
            load_in_8bit=False,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type='nf4')
    ),
    lora=dict(
        type=LoraConfig,
        r=64,
        lora_alpha=16,
        lora_dropout=0.1,
        bias='none',
        task_type='CAUSAL_LM')
)

#######################################################################
#                      PART 3  Dataset & Dataloader                   #
#######################################################################
train_dataset = dict(
    type=process_hf_dataset,
    dataset=dict(type=load_dataset, path='json', data_files=data_files),
    tokenizer=tokenizer,
    max_length=max_length,
    dataset_map_fn=pretrain_map_fn,
    template_map_fn=None,
    remove_unused_columns=True,
    shuffle_before_pack=False,
    pack_to_max_length=pack_to_max_length,
    use_varlen_attn=use_varlen_attn)

train_dataloader = dict(
    batch_size=batch_size,
    num_workers=dataloader_num_workers,
    dataset=train_dataset,
    sampler=dict(type=DefaultSampler, shuffle=True),
    collate_fn=dict(type=default_collate_fn, use_varlen_attn=use_varlen_attn))

#######################################################################
#                    PART 4  Scheduler & Optimizer                    #
#######################################################################
# optimizer
optim_wrapper = dict(
    type=AmpOptimWrapper,
    optimizer=dict(
        type=optim_type, lr=lr, betas=betas, weight_decay=weight_decay),
    clip_grad=dict(max_norm=max_norm, error_if_nonfinite=False),
    accumulative_counts=accumulative_counts,
    loss_scale='dynamic',
    dtype='float16')

# learning policy
# More information: https://github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/param_scheduler.md  # noqa: E501
param_scheduler = [
    dict(
        type=LinearLR,
        start_factor=1e-5,
        by_epoch=True,
        begin=0,
        end=warmup_ratio * max_epochs,
        convert_to_iter_based=True),
    dict(
        type=CosineAnnealingLR,
        eta_min=0.0,
        by_epoch=True,
        begin=warmup_ratio * max_epochs,
        end=max_epochs,
        convert_to_iter_based=True)
]

# train, val, test setting
train_cfg = dict(type=TrainLoop, max_epochs=max_epochs)

#######################################################################
#                           PART 5  Runtime                           #
#######################################################################
# Log the dialogue periodically during the training process, optional
custom_hooks = [
    dict(type=DatasetInfoHook, tokenizer=tokenizer),
    dict(
        type=EvaluateChatHook,
        tokenizer=tokenizer,
        every_n_iters=evaluation_freq,
        evaluation_inputs=evaluation_inputs,
        system=SYSTEM)
]

if use_varlen_attn:
    custom_hooks += [dict(type=VarlenAttnArgsToMessageHubHook)]

# configure default hooks
default_hooks = dict(
    # record the time of every iteration.
    timer=dict(type=IterTimerHook),
    # print log every 10 iterations.
    logger=dict(type=LoggerHook, log_metric_by_epoch=False, interval=10),
    # enable the parameter scheduler.
    param_scheduler=dict(type=ParamSchedulerHook),
    # save checkpoint per `save_steps`.
    checkpoint=dict(
        type=CheckpointHook,
        by_epoch=False,
        interval=save_steps,
        max_keep_ckpts=save_total_limit),
    # set sampler seed in distributed evrionment.
    sampler_seed=dict(type=DistSamplerSeedHook),
)

# configure environment
env_cfg = dict(
    # whether to enable cudnn benchmark
    cudnn_benchmark=False,
    # set multi process parameters
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
    # set distributed parameters
    dist_cfg=dict(backend='nccl'),
)

# set visualizer
visualizer = None

# set log level
log_level = 'INFO'

# load from which checkpoint
load_from = None

# whether to resume training from the loaded checkpoint
resume = False

# Defaults to use random seed and disable `deterministic`
randomness = dict(seed=None, deterministic=False)

# set log processor
log_processor = dict(by_epoch=False)
```

</details>

#### å¯åŠ¨å¾®è°ƒ

å®Œæˆäº†æ‰€æœ‰çš„å‡†å¤‡å·¥ä½œåï¼Œæˆ‘ä»¬å°±å¯ä»¥æ­£å¼çš„å¼€å§‹æˆ‘ä»¬ä¸‹ä¸€é˜¶æ®µçš„æ—…ç¨‹ï¼šXTuner å¯åŠ¨~ï¼

å½“æˆ‘ä»¬å‡†å¤‡å¥½äº†æ‰€æœ‰å†…å®¹ï¼Œæˆ‘ä»¬åªéœ€è¦å°†ä½¿ç”¨ `xtuner train` å‘½ä»¤ä»¤å³å¯å¼€å§‹è®­ç»ƒã€‚

> `xtuner train` å‘½ä»¤ç”¨äºå¯åŠ¨æ¨¡å‹å¾®è°ƒè¿›ç¨‹ã€‚è¯¥å‘½ä»¤éœ€è¦ä¸€ä¸ªå‚æ•°ï¼š`CONFIG` ç”¨äºæŒ‡å®šå¾®è°ƒé…ç½®æ–‡ä»¶ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¿®æ”¹å¥½çš„é…ç½®æ–‡ä»¶ `internlm2_1_8b_full_custom_pretrain_e1_copy.py`ã€‚  
> è®­ç»ƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„æ‰€æœ‰æ–‡ä»¶ï¼ŒåŒ…æ‹¬æ—¥å¿—ã€é…ç½®æ–‡ä»¶ã€æ£€æŸ¥ç‚¹æ–‡ä»¶ã€å¾®è°ƒåçš„æ¨¡å‹ç­‰ï¼Œé»˜è®¤ä¿å­˜åœ¨ `work_dirs` ç›®å½•ä¸‹ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡æ·»åŠ  `--work-dir` æŒ‡å®šç‰¹å®šçš„æ–‡ä»¶ä¿å­˜ä½ç½®ã€‚


```bash
xtuner train ./internlm2_1_8b_full_custom_pretrain_e1_copy.py
```

åœ¨è®­ç»ƒå®Œåï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚

<details>
<summary>ç›®å½•ç»“æ„</summary>

```
â”œâ”€â”€ work_dirs
â”‚   â””â”€â”€ internlm2_1_8b_full_custom_pretrain_e1_copy
â”‚       â”œâ”€â”€ 20240626_214522
â”‚       â”‚   â”œâ”€â”€ 20240626_214522.log
â”‚       â”‚   â””â”€â”€ vis_data
â”‚       â”‚       â”œâ”€â”€ 20240626_214522.json
â”‚       â”‚       â”œâ”€â”€ config.py
â”‚       â”‚       â”œâ”€â”€ eval_outputs_iter_1499.txt
â”‚       â”‚       â”œâ”€â”€ eval_outputs_iter_1999.txt
â”‚       â”‚       â”œâ”€â”€ eval_outputs_iter_2499.txt
â”‚       â”‚       â”œâ”€â”€ eval_outputs_iter_2623.txt
â”‚       â”‚       â”œâ”€â”€ eval_outputs_iter_499.txt
â”‚       â”‚       â”œâ”€â”€ eval_outputs_iter_999.txt
â”‚       â”‚       â””â”€â”€ scalars.json
â”‚       â”œâ”€â”€ internlm2_1_8b_full_custom_pretrain_e1_copy.py
â”‚       â”œâ”€â”€ iter_2500.pth
â”‚       â”œâ”€â”€ iter_2624.pth
â”‚       â””â”€â”€ last_checkpoint
```

</details>

#### æ¨¡å‹æ ¼å¼è½¬æ¢

æ¨¡å‹è½¬æ¢çš„æœ¬è´¨å…¶å®å°±æ˜¯å°†åŸæœ¬ä½¿ç”¨ Pytorch è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æƒé‡æ–‡ä»¶è½¬æ¢ä¸ºç›®å‰é€šç”¨çš„ HuggingFace æ ¼å¼æ–‡ä»¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¥å®ç°ä¸€é”®è½¬æ¢ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `xtuner convert pth_to_hf` å‘½ä»¤æ¥è¿›è¡Œæ¨¡å‹æ ¼å¼è½¬æ¢ã€‚

> `xtuner convert pth_to_hf` å‘½ä»¤ç”¨äºè¿›è¡Œæ¨¡å‹æ ¼å¼è½¬æ¢ã€‚è¯¥å‘½ä»¤éœ€è¦ä¸‰ä¸ªå‚æ•°ï¼š`CONFIG` è¡¨ç¤ºå¾®è°ƒçš„é…ç½®æ–‡ä»¶ï¼Œ `PATH_TO_PTH_MODEL` è¡¨ç¤ºå¾®è°ƒçš„æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„ï¼Œå³è¦è½¬æ¢çš„æ¨¡å‹æƒé‡ï¼Œ `SAVE_PATH_TO_HF_MODEL` è¡¨ç¤ºè½¬æ¢åçš„ HuggingFace æ ¼å¼æ–‡ä»¶çš„ä¿å­˜è·¯å¾„ã€‚

é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬å…¶å®è¿˜å¯ä»¥åœ¨è½¬æ¢çš„å‘½ä»¤ä¸­æ·»åŠ å‡ ä¸ªé¢å¤–çš„å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š
| å‚æ•°å | è§£é‡Š |
| ------------------- | ------------------------------------------------------ |
| --fp32     | ä»£è¡¨ä»¥fp32çš„ç²¾åº¦å¼€å¯ï¼Œå‡å¦‚ä¸è¾“å…¥åˆ™é»˜è®¤ä¸ºfp16                          |
| --max-shard-size {GB}        | ä»£è¡¨æ¯ä¸ªæƒé‡æ–‡ä»¶æœ€å¤§çš„å¤§å°ï¼ˆé»˜è®¤ä¸º2GBï¼‰                |


```bash
pth_file=`ls -t ./work_dirs/internlm2_1_8b_full_custom_pretrain_e1_copy/*.pth | head -n 1` && MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU xtuner convert pth_to_hf ./internlm2_1_8b_full_custom_pretrain_e1_copy.py ${pth_file} ./hf
```

æ¨¡å‹æ ¼å¼è½¬æ¢å®Œæˆåï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚

<details>
<summary>ç›®å½•ç»“æ„</summary>

```
â”œâ”€â”€ hf
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.bin
â”‚   â””â”€â”€ xtuner_config.py
```

</details>

è½¬æ¢å®Œæˆåï¼Œå¯ä»¥çœ‹åˆ°æ¨¡å‹è¢«è½¬æ¢ä¸º HuggingFace ä¸­å¸¸ç”¨çš„ .bin æ ¼å¼æ–‡ä»¶ï¼Œè¿™å°±ä»£è¡¨ç€æ–‡ä»¶æˆåŠŸè¢«è½¬åŒ–ä¸º HuggingFace æ ¼å¼äº†ã€‚

æ­¤æ—¶ï¼Œhf æ–‡ä»¶å¤¹å³ä¸ºæˆ‘ä»¬å¹³æ—¶æ‰€ç†è§£çš„æ‰€è°“ â€œLoRA æ¨¡å‹æ–‡ä»¶â€

> å¯ä»¥ç®€å•ç†è§£ï¼šLoRA æ¨¡å‹æ–‡ä»¶ = Adapter

#### æ¨¡å‹åˆå¹¶

å¯¹äº LoRA æˆ–è€… QLoRA å¾®è°ƒå‡ºæ¥çš„æ¨¡å‹å…¶å®å¹¶ä¸æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹ï¼Œè€Œæ˜¯ä¸€ä¸ªé¢å¤–çš„å±‚ï¼ˆAdapterï¼‰ï¼Œè®­ç»ƒå®Œçš„è¿™ä¸ªå±‚æœ€ç»ˆè¿˜æ˜¯è¦ä¸åŸæ¨¡å‹è¿›è¡Œåˆå¹¶æ‰èƒ½è¢«æ­£å¸¸çš„ä½¿ç”¨ã€‚

> å¯¹äºå…¨é‡å¾®è°ƒçš„æ¨¡å‹ï¼ˆfullï¼‰å…¶å®æ˜¯ä¸éœ€è¦è¿›è¡Œæ•´åˆè¿™ä¸€æ­¥çš„ï¼Œå› ä¸ºå…¨é‡å¾®è°ƒä¿®æ”¹çš„æ˜¯åŸæ¨¡å‹çš„æƒé‡è€Œéå¾®è°ƒä¸€ä¸ªæ–°çš„ Adapter ï¼Œå› æ­¤æ˜¯ä¸éœ€è¦è¿›è¡Œæ¨¡å‹æ•´åˆçš„ã€‚

åœ¨ XTuner ä¸­æä¾›äº†ä¸€é”®åˆå¹¶çš„å‘½ä»¤ `xtuner convert merge`ï¼Œåœ¨ä½¿ç”¨å‰æˆ‘ä»¬éœ€è¦å‡†å¤‡å¥½ä¸‰ä¸ªè·¯å¾„ï¼ŒåŒ…æ‹¬åŸæ¨¡å‹çš„è·¯å¾„ã€è®­ç»ƒå¥½çš„ Adapter å±‚çš„ï¼ˆæ¨¡å‹æ ¼å¼è½¬æ¢åçš„ï¼‰è·¯å¾„ä»¥åŠæœ€ç»ˆä¿å­˜çš„è·¯å¾„ã€‚

> `xtuner convert merge`å‘½ä»¤ç”¨äºåˆå¹¶æ¨¡å‹ã€‚è¯¥å‘½ä»¤éœ€è¦ä¸‰ä¸ªå‚æ•°ï¼š`LLM` è¡¨ç¤ºåŸæ¨¡å‹è·¯å¾„ï¼Œ`ADAPTER` è¡¨ç¤º Adapter å±‚çš„è·¯å¾„ï¼Œ `SAVE_PATH` è¡¨ç¤ºåˆå¹¶åçš„æ¨¡å‹æœ€ç»ˆçš„ä¿å­˜è·¯å¾„ã€‚

åœ¨æ¨¡å‹åˆå¹¶è¿™ä¸€æ­¥è¿˜æœ‰å…¶ä»–å¾ˆå¤šçš„å¯é€‰å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š
| å‚æ•°å | è§£é‡Š |
| ------------------- | ------------------------------------------------------ |
| --max-shard-size {GB} | ä»£è¡¨æ¯ä¸ªæƒé‡æ–‡ä»¶æœ€å¤§çš„å¤§å°ï¼ˆé»˜è®¤ä¸º2GBï¼‰                |
| --device {device_name} | è¿™é‡ŒæŒ‡çš„å°±æ˜¯deviceçš„åç§°ï¼Œå¯é€‰æ‹©çš„æœ‰cudaã€cpuå’Œautoï¼Œé»˜è®¤ä¸ºcudaå³ä½¿ç”¨gpuè¿›è¡Œè¿ç®— |
| --is-clip | è¿™ä¸ªå‚æ•°ä¸»è¦ç”¨äºç¡®å®šæ¨¡å‹æ˜¯ä¸æ˜¯CLIPæ¨¡å‹ï¼Œå‡å¦‚æ˜¯çš„è¯å°±è¦åŠ ä¸Šï¼Œä¸æ˜¯å°±ä¸éœ€è¦æ·»åŠ  |

> CLIPï¼ˆContrastive Languageâ€“Image Pre-trainingï¼‰æ¨¡å‹æ˜¯ OpenAI å¼€å‘çš„ä¸€ç§é¢„è®­ç»ƒæ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿç†è§£å›¾åƒå’Œæè¿°å®ƒä»¬çš„æ–‡æœ¬ä¹‹é—´çš„å…³ç³»ã€‚CLIP é€šè¿‡åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šå­¦ä¹ å›¾åƒå’Œå¯¹åº”æ–‡æœ¬ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œä»è€Œå®ç°äº†å¯¹å›¾åƒå†…å®¹çš„ç†è§£å’Œåˆ†ç±»ï¼Œç”šè‡³èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒã€‚


```bash
MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU xtuner convert merge Shanghai_AI_Laboratory/internlm2-1_8b ./hf ./merged --max-shard-size 2GB
```

æ¨¡å‹åˆå¹¶å®Œæˆåï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚

<details>
<summary>ç›®å½•ç»“æ„</summary>

```
â”œâ”€â”€ merged
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ configuration_internlm2.py
â”‚   â”œâ”€â”€ generation_config.json
â”‚   â”œâ”€â”€ modeling_internlm2.py
â”‚   â”œâ”€â”€ pytorch_model-00001-of-00002.bin
â”‚   â”œâ”€â”€ pytorch_model-00002-of-00002.bin
â”‚   â”œâ”€â”€ pytorch_model.bin.index.json
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”œâ”€â”€ tokenization_internlm2.py
â”‚   â”œâ”€â”€ tokenization_internlm2_fast.py
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ tokenizer.model
â”‚   â””â”€â”€ tokenizer_config.json
```

</details>

åœ¨æ¨¡å‹åˆå¹¶å®Œæˆåï¼Œæˆ‘ä»¬å°±å¯ä»¥çœ‹åˆ°æœ€ç»ˆçš„æ¨¡å‹å’ŒåŸæ¨¡å‹æ–‡ä»¶å¤¹éå¸¸ç›¸ä¼¼ï¼ŒåŒ…æ‹¬äº†åˆ†è¯å™¨ã€æƒé‡æ–‡ä»¶ã€é…ç½®ä¿¡æ¯ç­‰ç­‰ã€‚

### ç›®æ ‡æ¨¡å‹æ¨ç†

å½“æˆ‘ä»¬åˆå¹¶å®Œæˆåï¼Œæˆ‘ä»¬å°±èƒ½å¤Ÿæ­£å¸¸çš„è°ƒç”¨è¿™ä¸ªæ¨¡å‹è¿›è¡Œæ¨ç†äº†ã€‚


```python
tokenizer, model = load_model("./merged")
```


```python
generate("ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯")
```


```python
generate("æˆéƒ½æ˜¯")
```

å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡å¢é‡é¢„è®­ç»ƒï¼Œç¡®å®åœ¨åŸºåº§æ¨¡å‹çš„åŸºç¡€ä¸Šå­¦ä¹ åˆ°äº†æ–°çš„çŸ¥è¯†ã€‚


```python
del tokenizer, model

torch.cuda.empty_cache()
```

## æŒ‡ä»¤è·Ÿéšå¾®è°ƒï¼ˆå¾®è°ƒä¸ªäººå°åŠ©æ‰‹è®¤çŸ¥ï¼‰

æŒæ¡äº†å¢é‡é¢„è®­ç»ƒå¾®è°ƒåï¼Œå†æ¥è¿›è¡ŒæŒ‡ä»¤è·Ÿéšå¾®è°ƒå°±æ¯”è¾ƒç®€å•äº†ã€‚è¿™é‡Œæˆ‘ä»¬ç”¨ `internlm2-chat-1_8b` æ¨¡å‹ï¼Œé€šè¿‡ `QLoRA` çš„æ–¹å¼æ¥å¾®è°ƒä¸€ä¸ªè‡ªå·±çš„å°åŠ©æ‰‹è®¤çŸ¥ä½œä¸ºæ¡ˆä¾‹æ¥è¿›è¡Œæ¼”ç¤ºã€‚

å…ˆçœ‹çœ‹å¾®è°ƒæ•ˆæœï¼š

<table>
<thead>
<tr>
<td></td><td width="48%">å¾®è°ƒå‰</td><td width="48%">å¾®è°ƒå</td>
</tr>
</thead>
<tbody>
<tr>
<td>è¾“å…¥</td><td>è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±</td><td>è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±</td>
</tr>
<tr>
<td>è¾“å‡º</td><td>ä½ å¥½ï¼Œæˆ‘æ˜¯ä¹¦ç”ŸÂ·æµ¦è¯­ã€‚æˆ‘è‡´åŠ›äºå¸®åŠ©ç”¨æˆ·è§£å†³å„ç§è¯­è¨€ç›¸å…³çš„é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºè¯­è¨€å­¦ä¹ ã€ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ç­‰ã€‚æˆ‘ä½¿ç”¨äº†Transformeræ¨¡å‹å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œå¹¶ä½¿ç”¨äº†è¯­è¨€æ¨¡å‹ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œæ¬¢è¿éšæ—¶å‘æˆ‘æé—®ã€‚</td><td>æˆ‘æ˜¯ä¼é²œåŒå¿—çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„1.8Bå¤§æ¨¡å‹å“¦</td>
</tr>
<tr>
<td>ç½‘é¡µ</td><td><img src="images/image-11.png"/></td><td><img src="images/image-12.png"/></td>
</tr>
</tbody>
</table>

- å¯¼å…¥å¿…è¦çš„åº“


```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
```

- å®šä¹‰æ¨¡å‹åŠ è½½æ–¹æ³•


```python
def load_model(model_path):
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True).cuda()
    model = model.eval()
    return tokenizer, model
```

- å®šä¹‰å¯¹è¯æ–¹æ³•


```python
messages = []

def chat(input_text):
    length = 0
    for response, _ in model.stream_chat(tokenizer, input_text, messages):
        if response is not None:
            print(response[length:], flush=True, end="")
            length = len(response)
```

### å¾®è°ƒå‰çš„æ¨¡å‹å¯¹è¯

é¦–å…ˆæ¥çœ‹çœ‹ `internlm2-chat-1_8b` çš„å¯¹è¯æ¼”ç¤ºã€‚

- æ¨¡å‹åŠ è½½


```python
tokenizer, model = load_model("Shanghai_AI_Laboratory/internlm2-chat-1_8b")
```

- å¯¹è¯


```python
chat("è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
```

- é‡Šæ”¾ç¼“å­˜


```python
del tokenizer, model

torch.cuda.empty_cache()
```

### æŒ‡ä»¤è·Ÿéšå¾®è°ƒ

ä¸‹é¢æˆ‘ä»¬å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè®©æ¨¡å‹è®¤è¯†åˆ°è‡ªå·±çš„å¼Ÿä½ï¼Œäº†è§£å®ƒè‡ªå·±æ˜¯ä½ çš„ä¸€ä¸ªåŠ©æ‰‹ã€‚

#### å‡†æ•°æ®æ–‡ä»¶

ä¸ºäº†è®©æ¨¡å‹èƒ½å¤Ÿè®¤æ¸…è‡ªå·±çš„èº«ä»½å¼Ÿä½ï¼Œåœ¨è¯¢é—®è‡ªå·±æ˜¯è°çš„æ—¶å€™æŒ‰ç…§æˆ‘ä»¬é¢„æœŸçš„ç»“æœè¿›è¡Œå›å¤ï¼Œæˆ‘ä»¬å°±éœ€è¦é€šè¿‡åœ¨å¾®è°ƒæ•°æ®é›†ä¸­å¤§é‡åŠ å…¥è¿™æ ·çš„æ•°æ®ã€‚æˆ‘ä»¬å‡†å¤‡ä¸€ä¸ªæ•°æ®é›†æ–‡ä»¶`datas/assistant.json`ï¼Œæ–‡ä»¶å†…å®¹ä¸ºå¯¹è¯æ•°æ®ã€‚ä¸ºäº†å¢å¼ºå¾®è°ƒæ•ˆæœï¼Œå¯ä»¥å°†å¯¹è¯æ•°æ®å¤åˆ¶å¤šæ¡ã€‚


```python
[
    {"conversation": [{"input": "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±", "output": "æˆ‘æ˜¯ä¼é²œåŒå¿—çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„1.8Bå¤§æ¨¡å‹å“¦"}]},
    {"conversation": [{"input": "ä½ åœ¨å®æˆ˜è¥åšä»€ä¹ˆ", "output": "æˆ‘åœ¨è¿™é‡Œå¸®åŠ©ä¼é²œåŒå¿—å®ŒæˆXTunerå¾®è°ƒä¸ªäººå°åŠ©æ‰‹çš„ä»»åŠ¡"}]},
]
```

å‡†å¤‡å¥½æ•°æ®æ–‡ä»¶åï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚

<details>
<summary>ç›®å½•ç»“æ„</summary>

```
â”œâ”€â”€ Shanghai_AI_Laboratory
â”‚   â”œâ”€â”€ internlm2-1_8b
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ configuration.json
â”‚   â”‚   â”œâ”€â”€ configuration_internlm2.py
â”‚   â”‚   â”œâ”€â”€ generation_config.json
â”‚   â”‚   â”œâ”€â”€ modeling_internlm2.py
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2.py
â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2_fast.py
â”‚   â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”‚   â”œâ”€â”€ tokenizer.model
â”‚   â”‚   â””â”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ internlm2-chat-1_8b -> /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ configuration.json
â”‚       â”œâ”€â”€ configuration_internlm2.py
â”‚       â”œâ”€â”€ generation_config.json
â”‚       â”œâ”€â”€ model-00001-of-00002.safetensors
â”‚       â”œâ”€â”€ model-00002-of-00002.safetensors
â”‚       â”œâ”€â”€ model.safetensors.index.json
â”‚       â”œâ”€â”€ modeling_internlm2.py
â”‚       â”œâ”€â”€ special_tokens_map.json
â”‚       â”œâ”€â”€ tokenization_internlm2.py
â”‚       â”œâ”€â”€ tokenization_internlm2_fast.py
â”‚       â”œâ”€â”€ tokenizer.model
â”‚       â””â”€â”€ tokenizer_config.json
â”œâ”€â”€ datas
â”‚   â”œâ”€â”€ assistant.json
â”‚   â””â”€â”€ pretrain.json
â”œâ”€â”€ internlm2_1_8b_full_custom_pretrain_e1_copy.py
```

</details>


#### å‡†å¤‡é…ç½®æ–‡ä»¶

åœ¨å‡†å¤‡å¥½äº†æ¨¡å‹å’Œæ•°æ®é›†åï¼Œæˆ‘ä»¬å°±è¦æ ¹æ®æˆ‘ä»¬é€‰æ‹©çš„å¾®è°ƒæ–¹æ³•ç»“åˆå¾®è°ƒæ–¹æ¡ˆæ¥æ‰¾åˆ°ä¸æˆ‘ä»¬æœ€åŒ¹é…çš„é…ç½®æ–‡ä»¶äº†ï¼Œä»è€Œå‡å°‘æˆ‘ä»¬å¯¹é…ç½®æ–‡ä»¶çš„ä¿®æ”¹é‡ã€‚

è¿™é‡Œæˆ‘ä»¬é€‰æ‹©ä½¿ç”¨ `internlm2_chat_1_8b_qlora_alpaca_e3` é…ç½®æ–‡ä»¶ã€‚


```bash
xtuner copy-cfg internlm2_chat_1_8b_qlora_alpaca_e3 .
```

å¤åˆ¶å¥½é…ç½®æ–‡ä»¶åï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚

<details>
<summary>ç›®å½•ç»“æ„</summary>

```
â”œâ”€â”€ Shanghai_AI_Laboratory
â”‚   â”œâ”€â”€ internlm2-1_8b
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ configuration.json
â”‚   â”‚   â”œâ”€â”€ configuration_internlm2.py
â”‚   â”‚   â”œâ”€â”€ generation_config.json
â”‚   â”‚   â”œâ”€â”€ modeling_internlm2.py
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2.py
â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2_fast.py
â”‚   â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”‚   â”œâ”€â”€ tokenizer.model
â”‚   â”‚   â””â”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ internlm2-chat-1_8b -> /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ configuration.json
â”‚       â”œâ”€â”€ configuration_internlm2.py
â”‚       â”œâ”€â”€ generation_config.json
â”‚       â”œâ”€â”€ model-00001-of-00002.safetensors
â”‚       â”œâ”€â”€ model-00002-of-00002.safetensors
â”‚       â”œâ”€â”€ model.safetensors.index.json
â”‚       â”œâ”€â”€ modeling_internlm2.py
â”‚       â”œâ”€â”€ special_tokens_map.json
â”‚       â”œâ”€â”€ tokenization_internlm2.py
â”‚       â”œâ”€â”€ tokenization_internlm2_fast.py
â”‚       â”œâ”€â”€ tokenizer.model
â”‚       â””â”€â”€ tokenizer_config.json
â”œâ”€â”€ datas
â”‚   â”œâ”€â”€ assistant.json
â”‚   â””â”€â”€ pretrain.json
â”œâ”€â”€ internlm2_1_8b_full_custom_pretrain_e1_copy.py
â”œâ”€â”€ internlm2_1_8b_qlora_alpaca_e3_copy.py
```

</details>

ä¸‹é¢æˆ‘ä»¬å°†æ ¹æ®é¡¹ç›®çš„éœ€æ±‚ä¸€æ­¥æ­¥çš„è¿›è¡Œä¿®æ”¹å’Œè°ƒæ•´å§ï¼

åœ¨ PART 1 çš„éƒ¨åˆ†ï¼Œç”±äºæˆ‘ä»¬ä¸å†éœ€è¦åœ¨ HuggingFace ä¸Šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å…ˆè¦æ›´æ¢æ¨¡å‹çš„è·¯å¾„ä»¥åŠæ•°æ®é›†çš„è·¯å¾„ä¸ºæˆ‘ä»¬æœ¬åœ°çš„è·¯å¾„ã€‚

ä¸ºäº†è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿå®æ—¶è§‚å¯Ÿåˆ°æ¨¡å‹çš„å˜åŒ–æƒ…å†µï¼ŒXTuner è´´å¿ƒçš„æ¨å‡ºäº†ä¸€ä¸ª `evaluation_inputs` çš„å‚æ•°æ¥è®©æˆ‘ä»¬èƒ½å¤Ÿè®¾ç½®å¤šä¸ªé—®é¢˜æ¥ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–æ˜¯æœç€æˆ‘ä»¬æƒ³è¦çš„æ–¹å‘å‰è¿›çš„ã€‚æˆ‘ä»¬å¯ä»¥æ·»åŠ è‡ªå·±çš„è¾“å…¥ã€‚

åœ¨ PART 3 çš„éƒ¨åˆ†ï¼Œç”±äºæˆ‘ä»¬å‡†å¤‡çš„æ•°æ®é›†æ˜¯ JSON æ ¼å¼çš„æ•°æ®ï¼Œå¹¶ä¸”å¯¹è¯å†…å®¹å·²ç»æ˜¯ `input` å’Œ `output` çš„æ•°æ®å¯¹ï¼Œæ‰€ä»¥ä¸éœ€è¦è¿›è¡Œæ ¼å¼è½¬æ¢ã€‚

```diff
#######################################################################
#                          PART 1  Settings                           #
#######################################################################
- pretrained_model_name_or_path = 'internlm/internlm2-chat-1_8b'
+ pretrained_model_name_or_path = 'Shanghai_AI_Laboratory/internlm2-chat-1_8b'

- alpaca_en_path = 'tatsu-lab/alpaca'
+ alpaca_en_path = 'datas/assistant.json'

evaluation_inputs = [
-    'è¯·ç»™æˆ‘ä»‹ç»äº”ä¸ªä¸Šæµ·çš„æ™¯ç‚¹', 'Please tell me five scenic spots in Shanghai'
+    'è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', 'è¯·ç»™æˆ‘ä»‹ç»äº”ä¸ªä¸Šæµ·çš„æ™¯ç‚¹', 'Please tell me five scenic spots in Shanghai'
]
+ evaluation_inputs = ['ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯', 'ä¸Šæµ·æ˜¯', 'Shanghai is']

#######################################################################
#                      PART 3  Dataset & Dataloader                   #
#######################################################################
alpaca_en = dict(
    type=process_hf_dataset,
-   dataset=dict(type=load_dataset, path=alpaca_en_path),
+   dataset=dict(type=load_dataset, path='json', data_files=dict(train=alpaca_en_path)),
    tokenizer=tokenizer,
    max_length=max_length,
-   dataset_map_fn=alpaca_map_fn,
+   dataset_map_fn=None,
    template_map_fn=dict(
        type=template_map_fn_factory, template=prompt_template),
    remove_unused_columns=True,
    shuffle_before_pack=True,
    pack_to_max_length=pack_to_max_length,
    use_varlen_attn=use_varlen_attn)
```

ä¿®æ”¹å®Œåçš„å®Œæ•´çš„é…ç½®æ–‡ä»¶æ˜¯ï¼š

<details>
<summary>internlm2_chat_1_8b_qlora_alpaca_e3_copy.py</summary>

```python
# Copyright (c) OpenMMLab. All rights reserved.
import torch
from datasets import load_dataset
from mmengine.dataset import DefaultSampler
from mmengine.hooks import (CheckpointHook, DistSamplerSeedHook, IterTimerHook,
                            LoggerHook, ParamSchedulerHook)
from mmengine.optim import AmpOptimWrapper, CosineAnnealingLR, LinearLR
from peft import LoraConfig
from torch.optim import AdamW
from transformers import (AutoModelForCausalLM, AutoTokenizer,
                          BitsAndBytesConfig)

from xtuner.dataset import process_hf_dataset
from xtuner.dataset.collate_fns import default_collate_fn
from xtuner.dataset.map_fns import alpaca_map_fn, template_map_fn_factory
from xtuner.engine.hooks import (DatasetInfoHook, EvaluateChatHook,
                                 VarlenAttnArgsToMessageHubHook)
from xtuner.engine.runner import TrainLoop
from xtuner.model import SupervisedFinetune
from xtuner.parallel.sequence import SequenceParallelSampler
from xtuner.utils import PROMPT_TEMPLATE, SYSTEM_TEMPLATE

#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Model
pretrained_model_name_or_path = 'Shanghai_AI_Laboratory/internlm2-chat-1_8b'
use_varlen_attn = False

# Data
alpaca_en_path = 'datas/assistant.json'
prompt_template = PROMPT_TEMPLATE.internlm2_chat
max_length = 2048
pack_to_max_length = True

# parallel
sequence_parallel_size = 1

# Scheduler & Optimizer
batch_size = 1  # per_device
accumulative_counts = 16
accumulative_counts *= sequence_parallel_size
dataloader_num_workers = 0
max_epochs = 3
optim_type = AdamW
lr = 2e-4
betas = (0.9, 0.999)
weight_decay = 0
max_norm = 1  # grad clip
warmup_ratio = 0.03

# Save
save_steps = 500
save_total_limit = 2  # Maximum checkpoints to keep (-1 means unlimited)

# Evaluate the generation performance during the training
evaluation_freq = 500
SYSTEM = SYSTEM_TEMPLATE.alpaca
evaluation_inputs = [
    'è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', 'è¯·ç»™æˆ‘ä»‹ç»äº”ä¸ªä¸Šæµ·çš„æ™¯ç‚¹', 'Please tell me five scenic spots in Shanghai'
]

#######################################################################
#                      PART 2  Model & Tokenizer                      #
#######################################################################
tokenizer = dict(
    type=AutoTokenizer.from_pretrained,
    pretrained_model_name_or_path=pretrained_model_name_or_path,
    trust_remote_code=True,
    padding_side='right')

model = dict(
    type=SupervisedFinetune,
    use_varlen_attn=use_varlen_attn,
    llm=dict(
        type=AutoModelForCausalLM.from_pretrained,
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        quantization_config=dict(
            type=BitsAndBytesConfig,
            load_in_4bit=True,
            load_in_8bit=False,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type='nf4')),
    lora=dict(
        type=LoraConfig,
        r=64,
        lora_alpha=16,
        lora_dropout=0.1,
        bias='none',
        task_type='CAUSAL_LM'))

#######################################################################
#                      PART 3  Dataset & Dataloader                   #
#######################################################################
alpaca_en = dict(
    type=process_hf_dataset,
    dataset=dict(type=load_dataset, path='json', data_files=dict(train=alpaca_en_path)),
    tokenizer=tokenizer,
    max_length=max_length,
    dataset_map_fn=None,
    template_map_fn=dict(
        type=template_map_fn_factory, template=prompt_template),
    remove_unused_columns=True,
    shuffle_before_pack=True,
    pack_to_max_length=pack_to_max_length,
    use_varlen_attn=use_varlen_attn)

sampler = SequenceParallelSampler \
    if sequence_parallel_size > 1 else DefaultSampler
train_dataloader = dict(
    batch_size=batch_size,
    num_workers=dataloader_num_workers,
    dataset=alpaca_en,
    sampler=dict(type=sampler, shuffle=True),
    collate_fn=dict(type=default_collate_fn, use_varlen_attn=use_varlen_attn))

#######################################################################
#                    PART 4  Scheduler & Optimizer                    #
#######################################################################
# optimizer
optim_wrapper = dict(
    type=AmpOptimWrapper,
    optimizer=dict(
        type=optim_type, lr=lr, betas=betas, weight_decay=weight_decay),
    clip_grad=dict(max_norm=max_norm, error_if_nonfinite=False),
    accumulative_counts=accumulative_counts,
    loss_scale='dynamic',
    dtype='float16')

# learning policy
# More information: https://github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/param_scheduler.md  # noqa: E501
param_scheduler = [
    dict(
        type=LinearLR,
        start_factor=1e-5,
        by_epoch=True,
        begin=0,
        end=warmup_ratio * max_epochs,
        convert_to_iter_based=True),
    dict(
        type=CosineAnnealingLR,
        eta_min=0.0,
        by_epoch=True,
        begin=warmup_ratio * max_epochs,
        end=max_epochs,
        convert_to_iter_based=True)
]

# train, val, test setting
train_cfg = dict(type=TrainLoop, max_epochs=max_epochs)

#######################################################################
#                           PART 5  Runtime                           #
#######################################################################
# Log the dialogue periodically during the training process, optional
custom_hooks = [
    dict(type=DatasetInfoHook, tokenizer=tokenizer),
    dict(
        type=EvaluateChatHook,
        tokenizer=tokenizer,
        every_n_iters=evaluation_freq,
        evaluation_inputs=evaluation_inputs,
        system=SYSTEM,
        prompt_template=prompt_template)
]

if use_varlen_attn:
    custom_hooks += [dict(type=VarlenAttnArgsToMessageHubHook)]

# configure default hooks
default_hooks = dict(
    # record the time of every iteration.
    timer=dict(type=IterTimerHook),
    # print log every 10 iterations.
    logger=dict(type=LoggerHook, log_metric_by_epoch=False, interval=10),
    # enable the parameter scheduler.
    param_scheduler=dict(type=ParamSchedulerHook),
    # save checkpoint per `save_steps`.
    checkpoint=dict(
        type=CheckpointHook,
        by_epoch=False,
        interval=save_steps,
        max_keep_ckpts=save_total_limit),
    # set sampler seed in distributed evrionment.
    sampler_seed=dict(type=DistSamplerSeedHook),
)

# configure environment
env_cfg = dict(
    # whether to enable cudnn benchmark
    cudnn_benchmark=False,
    # set multi process parameters
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
    # set distributed parameters
    dist_cfg=dict(backend='nccl'),
)

# set visualizer
visualizer = None

# set log level
log_level = 'INFO'

# load from which checkpoint
load_from = None

# whether to resume training from the loaded checkpoint
resume = False

# Defaults to use random seed and disable `deterministic`
randomness = dict(seed=None, deterministic=False)

# set log processor
log_processor = dict(by_epoch=False)
```

</details>

#### å¯åŠ¨å¾®è°ƒ

å®Œæˆäº†æ‰€æœ‰çš„å‡†å¤‡å·¥ä½œåï¼Œæˆ‘ä»¬å°±å¯ä»¥æ­£å¼çš„å¼€å§‹æˆ‘ä»¬ä¸‹ä¸€é˜¶æ®µçš„æ—…ç¨‹ï¼šXTuner å¯åŠ¨~ï¼

å½“æˆ‘ä»¬å‡†å¤‡å¥½äº†æ‰€æœ‰å†…å®¹ï¼Œæˆ‘ä»¬åªéœ€è¦å°†ä½¿ç”¨ `xtuner train` å‘½ä»¤ä»¤å³å¯å¼€å§‹è®­ç»ƒã€‚


```bash
xtuner train ./internlm2_chat_1_8b_qlora_alpaca_e3_copy.py
```

åœ¨è®­ç»ƒå®Œåï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚

<details>
<summary>ç›®å½•ç»“æ„</summary>

```
â”œâ”€â”€ work_dirs
â”‚   â””â”€â”€ internlm2_chat_1_8b_qlora_alpaca_e3_copy
â”‚       â”œâ”€â”€ 20240626_222727
â”‚       â”‚   â”œâ”€â”€ 20240626_222727.log
â”‚       â”‚   â””â”€â”€ vis_data
â”‚       â”‚       â”œâ”€â”€ 20240626_222727.json
â”‚       â”‚       â”œâ”€â”€ config.py
â”‚       â”‚       â”œâ”€â”€ eval_outputs_iter_95.txt
â”‚       â”‚       â””â”€â”€ scalars.json
â”‚       â”œâ”€â”€ internlm2_chat_1_8b_qlora_alpaca_e3_copy.py
â”‚       â”œâ”€â”€ iter_96.pth
â”‚       â””â”€â”€ last_checkpoint
```

</details>


```bash
tree -l
```

#### æ¨¡å‹æ ¼å¼è½¬æ¢

æ¨¡å‹è½¬æ¢çš„æœ¬è´¨å…¶å®å°±æ˜¯å°†åŸæœ¬ä½¿ç”¨ Pytorch è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æƒé‡æ–‡ä»¶è½¬æ¢ä¸ºç›®å‰é€šç”¨çš„ HuggingFace æ ¼å¼æ–‡ä»¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¥å®ç°ä¸€é”®è½¬æ¢ã€‚


```bash
pth_file=`ls -t ./work_dirs/internlm2_chat_1_8b_qlora_alpaca_e3_copy/*.pth | head -n 1` && MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU xtuner convert pth_to_hf ./internlm2_chat_1_8b_qlora_alpaca_e3_copy.py ${pth_file} ./hf
```

æ¨¡å‹æ ¼å¼è½¬æ¢å®Œæˆåï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚

<details>
<summary>ç›®å½•ç»“æ„</summary>

```
â”œâ”€â”€ hf
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.bin
â”‚   â””â”€â”€ xtuner_config.py
```

</details>

#### æ¨¡å‹åˆå¹¶

å¯¹äº LoRA æˆ–è€… QLoRA å¾®è°ƒå‡ºæ¥çš„æ¨¡å‹å…¶å®å¹¶ä¸æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹ï¼Œè€Œæ˜¯ä¸€ä¸ªé¢å¤–çš„å±‚ï¼ˆAdapterï¼‰ï¼Œè®­ç»ƒå®Œçš„è¿™ä¸ªå±‚æœ€ç»ˆè¿˜æ˜¯è¦ä¸åŸæ¨¡å‹è¿›è¡Œåˆå¹¶æ‰èƒ½è¢«æ­£å¸¸çš„ä½¿ç”¨ã€‚


```bash
MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU xtuner convert merge Shanghai_AI_Laboratory/internlm2-chat-1_8b ./hf ./merged --max-shard-size 2GB
```

æ¨¡å‹åˆå¹¶å®Œæˆåï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚

<details>
<summary>ç›®å½•ç»“æ„</summary>

```
â”œâ”€â”€ merged
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ configuration_internlm2.py
â”‚   â”œâ”€â”€ generation_config.json
â”‚   â”œâ”€â”€ modeling_internlm2.py
â”‚   â”œâ”€â”€ pytorch_model-00001-of-00002.bin
â”‚   â”œâ”€â”€ pytorch_model-00002-of-00002.bin
â”‚   â”œâ”€â”€ pytorch_model.bin.index.json
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”œâ”€â”€ tokenization_internlm2.py
â”‚   â”œâ”€â”€ tokenization_internlm2_fast.py
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ tokenizer.model
â”‚   â””â”€â”€ tokenizer_config.json
```

</details>

### å¾®è°ƒåçš„æ¨¡å‹å¯¹è¯


```python
tokenizer, model = load_model("./merged")
```


```python
chat("è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
```


```python
chat("ä½ åœ¨å®æˆ˜è¥åšä»€ä¹ˆ")
```


```python
chat("ä»‹ç»ä¸€ä¸‹æˆéƒ½")
```


```python
del tokenizer, model

torch.cuda.empty_cache()
```

## DeepSpeedä»‹ç»

DeepSpeedæ˜¯ä¸€ä¸ªç”±å¾®è½¯å¼€å‘çš„å¼€æºæ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡å’Œé€Ÿåº¦ã€‚

XTuner ä¹Ÿå†…ç½®äº† `deepspeed` æ¥åŠ é€Ÿæ•´ä½“çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå…±æœ‰ä¸‰ç§ä¸åŒçš„ `deepspeed` ç±»å‹å¯è¿›è¡Œé€‰æ‹©ï¼Œåˆ†åˆ«æ˜¯ `deepspeed_zero1`, `deepspeed_zero2` å’Œ `deepspeed_zero3`ã€‚

<details>
<summary>DeepSpeedä¼˜åŒ–å™¨åŠå…¶é€‰æ‹©æ–¹æ³•</summary>

DeepSpeedæ˜¯ä¸€ä¸ªç”±å¾®è½¯å¼€å‘çš„å¼€æºæ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡å’Œé€Ÿåº¦ã€‚å®ƒé€šè¿‡å‡ ç§å…³é”®æŠ€æœ¯æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¨¡å‹åˆ†å‰²ã€æ¢¯åº¦ç´¯ç§¯ã€ä»¥åŠå†…å­˜å’Œå¸¦å®½ä¼˜åŒ–ç­‰ï¼Œèƒ½å¤Ÿé™ä½è®­ç»ƒè¶…å¤§è§„æ¨¡æ¨¡å‹çš„å¤æ‚æ€§å’Œèµ„æºéœ€æ±‚ï¼Œä½¿è®­ç»ƒå˜å¾—æ›´å¿«ã€æ›´é«˜æ•ˆã€‚DeepSpeedç‰¹åˆ«é€‚ç”¨äºéœ€è¦å·¨å¤§è®¡ç®—èµ„æºçš„å¤§å‹æ¨¡å‹å’Œæ•°æ®é›†ã€‚

åœ¨DeepSpeedä¸­ï¼Œå¼•å…¥äº†ZeROï¼ˆZero Redundancy Optimizerï¼‰æŠ€æœ¯ï¼Œæ˜¯ä¸€ç§æ—¨åœ¨é™ä½è®­ç»ƒå¤§å‹æ¨¡å‹æ‰€éœ€å†…å­˜å ç”¨çš„ä¼˜åŒ–å™¨ï¼Œé€šè¿‡åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­åˆ†å‰²ä¼˜åŒ–å™¨çš„çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°ï¼Œå‡å°‘å†—ä½™çš„å†…å­˜å ç”¨ï¼Œå…è®¸æ›´å¤§çš„æ¨¡å‹å’Œæ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ã€‚ZeRO åˆ†ä¸ºå‡ ä¸ªä¸åŒçš„çº§åˆ«ï¼Œä¸»è¦åŒ…æ‹¬ï¼š

- **deepspeed_zero1**ï¼šè¿™æ˜¯ZeROçš„åŸºæœ¬ç‰ˆæœ¬ï¼Œå®ƒä¼˜åŒ–äº†æ¨¡å‹å‚æ•°çš„å­˜å‚¨ï¼Œä¸»è¦é€šè¿‡åˆ†åŒºå­˜å‚¨ä¼˜åŒ–å™¨çŠ¶æ€æ¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚æ¯ä¸ªGPUè®¾å¤‡åªä¿å­˜ä¸€éƒ¨åˆ†ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œä»è€Œæ˜¾è‘—å‡å°‘å†…å­˜æ¶ˆè€—ã€‚

- **deepspeed_zero2**ï¼šåœ¨deepspeed_zero1çš„åŸºç¡€ä¸Šï¼Œdeepspeed_zero2è¿›ä¸€æ­¥ä¼˜åŒ–äº†æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€çš„å­˜å‚¨ï¼Œå°†æ¢¯åº¦ä¹Ÿè¿›è¡Œåˆ†åŒºå­˜å‚¨ã€‚è¿™æ ·ï¼Œæ¯ä¸ªGPUè®¾å¤‡åªéœ€è¦ä¿å­˜ä¸€éƒ¨åˆ†çš„ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ï¼Œè¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚

- **deepspeed_zero3**ï¼šè¿™æ˜¯ç›®å‰æœ€é«˜çº§çš„ä¼˜åŒ–ç­‰çº§ï¼Œå®ƒåŒ…æ‹¬äº†deepspeed_zero1å’Œdeepspeed_zero2çš„ä¼˜åŒ–ï¼Œé™¤äº†ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ï¼Œè¿˜å°†æ¨¡å‹å‚æ•°è¿›è¡Œåˆ†åŒºå­˜å‚¨ã€‚æ¯ä¸ªGPUè®¾å¤‡åªéœ€è¦ä¿å­˜ä¸€éƒ¨åˆ†çš„ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œæ¨¡å‹å‚æ•°ï¼Œä»è€Œæœ€å¤§é™åº¦åœ°å‡å°‘å†…å­˜ä½¿ç”¨ã€‚

é€‰æ‹©å“ªç§deepspeedç±»å‹ä¸»è¦å–å†³äºä½ çš„å…·ä½“éœ€æ±‚ï¼ŒåŒ…æ‹¬æ¨¡å‹çš„å¤§å°ã€å¯ç”¨çš„ç¡¬ä»¶èµ„æºï¼ˆç‰¹åˆ«æ˜¯GPUå†…å­˜ï¼‰ä»¥åŠè®­ç»ƒçš„æ•ˆç‡éœ€æ±‚ã€‚ä¸€èˆ¬æ¥è¯´ï¼š

- å¦‚æœä½ çš„æ¨¡å‹è¾ƒå°ï¼Œæˆ–è€…å†…å­˜èµ„æºå……è¶³ï¼Œå¯èƒ½ä¸éœ€è¦ä½¿ç”¨æœ€é«˜çº§åˆ«çš„ä¼˜åŒ–ã€‚
- å¦‚æœä½ éœ€è¦å¿«é€Ÿè®­ç»ƒæ¨¡å‹ï¼Œå¯èƒ½éœ€è¦æƒè¡¡å†…å­˜ä¼˜åŒ–å’Œè®¡ç®—æ•ˆç‡ã€‚deepspeed_zero1æä¾›äº†è¾ƒä½çš„å†…å­˜å ç”¨ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„è®¡ç®—æ•ˆç‡ã€‚
- å¦‚æœä½ æ­£åœ¨å°è¯•è®­ç»ƒéå¸¸å¤§çš„æ¨¡å‹ï¼Œæˆ–è€…ä½ çš„ç¡¬ä»¶èµ„æºæœ‰é™ï¼Œä½¿ç”¨deepspeed_zero2æˆ–deepspeed_zero3å¯èƒ½æ›´åˆé€‚ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥æ˜¾è‘—é™ä½å†…å­˜å ç”¨ï¼Œå…è®¸æ›´å¤§æ¨¡å‹çš„è®­ç»ƒã€‚
- é€‰æ‹©æ—¶ä¹Ÿè¦è€ƒè™‘åˆ°å®ç°çš„å¤æ‚æ€§å’Œè¿è¡Œæ—¶çš„å¼€é”€ï¼Œæ›´é«˜çº§çš„ä¼˜åŒ–å¯èƒ½éœ€è¦æ›´å¤æ‚çš„è®¾ç½®ï¼Œæ›´é¢‘ç¹çš„è·¨GPUé€šä¿¡ï¼Œè¿™å¯èƒ½éœ€è¦æ›´é«˜çš„ç½‘ç»œå¸¦å®½ï¼Œå¹¶å¯èƒ½å¢åŠ ä¸€äº›è®¡ç®—å¼€é”€ã€‚

</details>

## å¤šå¡å¾®è°ƒ

æ¨¡å‹çš„è§„æ¨¡å’Œå¤æ‚åº¦ä¸æ–­å¢åŠ ï¼Œå•å¼ GPUçš„æ˜¾å­˜å¾€å¾€æ— æ³•æ»¡è¶³å¤§æ¨¡å‹çš„è®­ç»ƒéœ€æ±‚ã€‚æ­¤æ—¶ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦å¤šå¡å¾®è°ƒï¼Œä»¥åº”å¯¹å¤§æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¾å­˜å’Œè®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚


XTuner ä¸­ä½¿ç”¨å¤šå¡å¾®è°ƒï¼Œåªéœ€è¦è®¾ç½® `NPROC_PER_NODE` ç¯å¢ƒå˜é‡ï¼Œå¹¶ä½¿ç”¨ `DeepSpeed` æ¥è¿›è¡ŒåŠ é€Ÿå°±å¯ä»¥äº†ï¼Œå…¶ä½™å‘½ä»¤å†…å®¹ä¸å•å¡å¾®è°ƒæ—¶ä¸€æ ·ã€‚

> ç”±äºå¼€å‘æœºåªæœ‰ä¸¤å¼ æ˜¾å¡ï¼Œæ‰€ä»¥æˆ‘ä»¬è®¾ç½®`NPROC_PER_NODE=2`ï¼Œå¹¶ä¸”é€‰æ‹©ä½¿ç”¨`deepspeed_zero3`ä¼˜åŒ–ç­‰çº§ã€‚


```bash
MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU NPROC_PER_NODE=2 xtuner train ./internlm2_chat_1_8b_qlora_alpaca_e3_copy.py --deepspeed deepspeed_zero3
```

åœ¨æ‰§è¡Œå¾®è°ƒçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤å¼ æ˜¾å¡éƒ½æœ‰å†…å­˜ä½¿ç”¨ã€‚

![](images/image-06.png)

åœ¨è®­ç»ƒå®Œåï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚

<details>
<summary>ç›®å½•ç»“æ„</summary>

```
â”œâ”€â”€ work_dirs
â”‚   â””â”€â”€ internlm2_chat_1_8b_qlora_alpaca_e3_copy
â”‚       â”œâ”€â”€ 20240627_205957
â”‚       â”‚   â”œâ”€â”€ 20240627_205957.log
â”‚       â”‚   â””â”€â”€ vis_data
â”‚       â”‚       â”œâ”€â”€ 20240627_205957.json
â”‚       â”‚       â”œâ”€â”€ config.py
â”‚       â”‚       â”œâ”€â”€ eval_outputs_iter_236.txt
â”‚       â”‚       â””â”€â”€ scalars.json
â”‚       â”œâ”€â”€ internlm2_chat_1_8b_qlora_alpaca_e3_copy.py
â”‚       â”œâ”€â”€ iter_237.pth
â”‚       â”‚   â”œâ”€â”€ bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
â”‚       â”‚   â”œâ”€â”€ bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
â”‚       â”‚   â”œâ”€â”€ zero_pp_rank_0_mp_rank_00_model_states.pt
â”‚       â”‚   â””â”€â”€ zero_pp_rank_1_mp_rank_00_model_states.pt
â”‚       â”œâ”€â”€ last_checkpoint
â”‚       â””â”€â”€ zero_to_fp32.py
```

</details>

å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡ `deepspeed` æ¥è®­ç»ƒåå¾—åˆ°çš„æƒé‡æ–‡ä»¶å’ŒåŸæœ¬çš„æƒé‡æ–‡ä»¶æ˜¯æœ‰æ‰€å·®åˆ«çš„ï¼ŒåŸæœ¬çš„ä»…ä»…æ˜¯ä¸€ä¸ª .pth çš„æ–‡ä»¶ï¼Œè€Œä½¿ç”¨äº† `deepspeed` åˆ™æ˜¯ä¸€ä¸ªåå­—å¸¦æœ‰ .pth çš„æ–‡ä»¶å¤¹ï¼Œåœ¨è¯¥æ–‡ä»¶å¤¹é‡Œä¿å­˜äº† .pt æ–‡ä»¶ã€‚è¿™ä¸¤è€…åœ¨å…·ä½“çš„ä½¿ç”¨ä¸Šå¹¶æ²¡æœ‰å¤ªå¤§çš„å·®åˆ«ï¼Œè½¬æ¢å’Œåˆå¹¶çš„è¿‡ç¨‹éƒ½æ˜¯ä¸€æ ·çš„ã€‚


```bash
pth_file=`ls -t ./work_dirs/internlm2_chat_1_8b_qlora_alpaca_e3_copy | grep pth | head -n 1` && MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU xtuner convert pth_to_hf ./internlm2_chat_1_8b_qlora_alpaca_e3_copy.py ./work_dirs/internlm2_chat_1_8b_qlora_alpaca_e3_copy/${pth_file} ./hf
```


```bash
MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU xtuner convert merge Shanghai_AI_Laboratory/internlm2-chat-1_8b ./hf ./merged --max-shard-size 2GB
```


```python
tokenizer, model = load_model("./merged")
```


```python
chat("è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
```


```python
chat("ä½ åœ¨å®æˆ˜è¥åšä»€ä¹ˆ")
```


```python
chat("ä»‹ç»ä¸€ä¸‹æˆéƒ½")
```


```python
del tokenizer, model

torch.cuda.empty_cache()
```

## åˆ†å¸ƒå¼å¾®è°ƒ

å¦‚æœæ¨¡å‹çš„è§„æ¨¡å’Œå¤æ‚åº¦ç»§ç»­å¢åŠ ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨åˆ†å¸ƒå¼å¾®è°ƒã€‚


```bash
apt-get install -y net-tools
ifconfig
```

åˆ†å¸ƒå¼å¾®è°ƒæ˜¯ä¸»ä»æ¶æ„çš„ã€‚ä¸»èŠ‚ç‚¹åè°ƒæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ï¼Œç®¡ç†æ•°æ®å’Œä»»åŠ¡åˆ°å·¥ä½œèŠ‚ç‚¹çš„åˆ†é…ã€‚å·¥ä½œèŠ‚ç‚¹æ‰§è¡Œè®­ç»ƒæ­¥éª¤çš„å®é™…è®¡ç®—ï¼Œå¤„ç†æ•°æ®çš„å­é›†å¹¶è®¡ç®—æ¢¯åº¦ã€‚æœ‰æ—¶å€™åœ¨ä¸€äº›æ¶æ„ä¸­è¿˜éœ€è¦å‚æ•°æœåŠ¡å™¨åè°ƒæ‰€æœ‰å·¥ä½œèŠ‚ç‚¹ä¹‹é—´çš„æ¨¡å‹æ›´æ–°åŒæ­¥ï¼Œç”¨äºèšåˆæ¥è‡ªå·¥ä½œèŠ‚ç‚¹çš„æ¢¯åº¦å¹¶æ›´æ–°æ¨¡å‹å‚æ•°ã€‚

> æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªèŠ‚ç‚¹è¿›è¡Œåˆ†å¸ƒå¼å¾®è°ƒï¼Œå®é™…ä¸Šéœ€è¦å¯åŠ¨ä¸‰ä¸ªèŠ‚ç‚¹ã€‚


```bash
MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU OMP_NUM_THREADS=1 MKL_NUM_THREADS=1 NPROC_PER_NODE=1 NNODES=2 xtuner train internlm2_chat_1_8b_qlora_alpaca_e3_copy.py

MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU OMP_NUM_THREADS=1 MKL_NUM_THREADS=1 NPROC_PER_NODE=1 NNODES=2 NODE_RANK=0 TRITON_CACHE_DIR=node0 PORT=20821 ADDR=192.168.230.182 xtuner train internlm2_chat_1_8b_qlora_alpaca_e3_copy.py --work-dir work_dir_node0

MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU OMP_NUM_THREADS=1 MKL_NUM_THREADS=1 NPROC_PER_NODE=1 NNODES=2 NODE_RANK=1 TRITON_CACHE_DIR=node1 PORT=20821 ADDR=192.168.230.182 xtuner train internlm2_chat_1_8b_qlora_alpaca_e3_copy.py --work-dir work_dir_node1
```

é¦–å…ˆå¯åŠ¨ä¸»èŠ‚ç‚¹ï¼Œç„¶åä¾æ¬¡å¯åŠ¨å…¶ä»–èŠ‚ç‚¹ã€‚ä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œéœ€è¦åœ¨ä¸€ä¸ªæ—¶é—´é˜ˆå€¼å†…å¯åŠ¨ç›¸å…³çš„èŠ‚ç‚¹ï¼Œå¦‚æœè¶…è¿‡æ—¶é—´é˜ˆå€¼è¿˜æ²¡å¯åŠ¨æ‰€æœ‰èŠ‚ç‚¹ï¼Œåˆ™å…¶ä»–èŠ‚ç‚¹ä¼šå› è¶…æ—¶è€ŒæŠ¥é”™é€€å‡ºã€‚

æ¯”å¦‚ï¼Œåœ¨ä¸¤ä¸ªèŠ‚ç‚¹çš„åˆ†å¸ƒå¼å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åªå¯åŠ¨ä¸»èŠ‚ç‚¹å’Œä¸€ä¸ªå·¥ä½œèŠ‚ç‚¹ï¼Œå¦ä¸€ä¸ªèŠ‚ç‚¹ä¸å¯åŠ¨ï¼Œåˆ™å·²å¯åŠ¨çš„èŠ‚ç‚¹ä¼šè¶…æ—¶æŠ¥é”™é€€å‡ºã€‚

![](images/image-07.png)

å¦‚æœæ‰€æœ‰èŠ‚ç‚¹éƒ½æ­£å¸¸å¯åŠ¨ã€è®­ç»ƒï¼Œåˆ™å¯ä»¥çœ‹åˆ°æ¯ä¸ªèŠ‚ç‚¹çš„æ˜¾å¡å‡æœ‰å†…å­˜ä½¿ç”¨ã€‚

![](images/image-08.png)

åœ¨è®­ç»ƒå®Œåï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ï¼Œè®­ç»ƒçš„æ¨¡å‹åœ¨å·¥ä½œèŠ‚ç‚¹ä¸Šã€‚

<details>
<summary>ç›®å½•ç»“æ„</summary>

```
â”œâ”€â”€ work_dir_node0
â”‚   â”œâ”€â”€ 20240627_213009
â”‚   â”‚   â”œâ”€â”€ 20240627_213009.log
â”‚   â”‚   â””â”€â”€ vis_data
â”‚   â”‚       â”œâ”€â”€ 20240627_213009.json
â”‚   â”‚       â”œâ”€â”€ config.py
â”‚   â”‚       â”œâ”€â”€ eval_outputs_iter_233.txt
â”‚   â”‚       â””â”€â”€ scalars.json
â”‚   â”œâ”€â”€ internlm2_chat_1_8b_qlora_alpaca_e3_copy.py
â”‚   â”œâ”€â”€ iter_234.pth
â”‚   â””â”€â”€ last_checkpoint
â”œâ”€â”€ work_dir_node1
â”‚   â””â”€â”€ 20240627_213009
â”œâ”€â”€ work_dirs
â”‚   â””â”€â”€ internlm2_chat_1_8b_qlora_alpaca_e3_copy
```

</details>

## Web Demo éƒ¨ç½²

é™¤äº†åœ¨ç»ˆç«¯ä¸­å¯¹æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Œæˆ‘ä»¬å…¶å®è¿˜å¯ä»¥åœ¨ç½‘é¡µç«¯çš„ Demo è¿›è¡Œå¯¹è¯ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…æ‰€éœ€è¦çš„ä¾èµ–ã€‚


```python
pip install streamlit
```

å…¶æ¬¡ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡ä¸€ä¸ªStreamlitç¨‹åºçš„è„šæœ¬ã€‚

Streamlitç¨‹åºçš„å®Œæ•´ä»£ç ã€‚

<details>
<summary>streamlit_demo.py</summary>

```python
import copy
import warnings
from dataclasses import asdict, dataclass
from typing import Callable, List, Optional

import streamlit as st
import torch
from torch import nn
from transformers.generation.utils import (LogitsProcessorList,
                                           StoppingCriteriaList)
from transformers.utils import logging

from transformers import AutoTokenizer, AutoModelForCausalLM  # isort: skip

logger = logging.get_logger(__name__)


model_name_or_path = "./merged"

@dataclass
class GenerationConfig:
    # this config is used for chat to provide more diversity
    max_length: int = 2048
    top_p: float = 0.75
    temperature: float = 0.1
    do_sample: bool = True
    repetition_penalty: float = 1.000


@torch.inference_mode()
def generate_interactive(
    model,
    tokenizer,
    prompt,
    generation_config: Optional[GenerationConfig] = None,
    logits_processor: Optional[LogitsProcessorList] = None,
    stopping_criteria: Optional[StoppingCriteriaList] = None,
    prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor],
                                                List[int]]] = None,
    additional_eos_token_id: Optional[int] = None,
    **kwargs,
):
    inputs = tokenizer([prompt], padding=True, return_tensors='pt')
    input_length = len(inputs['input_ids'][0])
    for k, v in inputs.items():
        inputs[k] = v.cuda()
    input_ids = inputs['input_ids']
    _, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]
    if generation_config is None:
        generation_config = model.generation_config
    generation_config = copy.deepcopy(generation_config)
    model_kwargs = generation_config.update(**kwargs)
    bos_token_id, eos_token_id = (  # noqa: F841  # pylint: disable=W0612
        generation_config.bos_token_id,
        generation_config.eos_token_id,
    )
    if isinstance(eos_token_id, int):
        eos_token_id = [eos_token_id]
    if additional_eos_token_id is not None:
        eos_token_id.append(additional_eos_token_id)
    has_default_max_length = kwargs.get(
        'max_length') is None and generation_config.max_length is not None
    if has_default_max_length and generation_config.max_new_tokens is None:
        warnings.warn(
            f"Using 'max_length''s default ({repr(generation_config.max_length)}) \
                to control the generation length. "
            'This behaviour is deprecated and will be removed from the \
                config in v5 of Transformers -- we'
            ' recommend using `max_new_tokens` to control the maximum \
                length of the generation.',
            UserWarning,
        )
    elif generation_config.max_new_tokens is not None:
        generation_config.max_length = generation_config.max_new_tokens + \
            input_ids_seq_length
        if not has_default_max_length:
            logger.warn(  # pylint: disable=W4902
                f"Both 'max_new_tokens' (={generation_config.max_new_tokens}) "
                f"and 'max_length'(={generation_config.max_length}) seem to "
                "have been set. 'max_new_tokens' will take precedence. "
                'Please refer to the documentation for more information. '
                '(https://huggingface.co/docs/transformers/main/'
                'en/main_classes/text_generation)',
                UserWarning,
            )

    if input_ids_seq_length >= generation_config.max_length:
        input_ids_string = 'input_ids'
        logger.warning(
            f"Input length of {input_ids_string} is {input_ids_seq_length}, "
            f"but 'max_length' is set to {generation_config.max_length}. "
            'This can lead to unexpected behavior. You should consider'
            " increasing 'max_new_tokens'.")

    # 2. Set generation parameters if not already defined
    logits_processor = logits_processor if logits_processor is not None \
        else LogitsProcessorList()
    stopping_criteria = stopping_criteria if stopping_criteria is not None \
        else StoppingCriteriaList()

    logits_processor = model._get_logits_processor(
        generation_config=generation_config,
        input_ids_seq_length=input_ids_seq_length,
        encoder_input_ids=input_ids,
        prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,
        logits_processor=logits_processor,
    )

    stopping_criteria = model._get_stopping_criteria(
        generation_config=generation_config,
        stopping_criteria=stopping_criteria)
    logits_warper = model._get_logits_warper(generation_config)

    unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)
    scores = None
    while True:
        model_inputs = model.prepare_inputs_for_generation(
            input_ids, **model_kwargs)
        # forward pass to get next token
        outputs = model(
            **model_inputs,
            return_dict=True,
            output_attentions=False,
            output_hidden_states=False,
        )

        next_token_logits = outputs.logits[:, -1, :]

        # pre-process distribution
        next_token_scores = logits_processor(input_ids, next_token_logits)
        next_token_scores = logits_warper(input_ids, next_token_scores)

        # sample
        probs = nn.functional.softmax(next_token_scores, dim=-1)
        if generation_config.do_sample:
            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
        else:
            next_tokens = torch.argmax(probs, dim=-1)

        # update generated ids, model inputs, and length for next step
        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
        model_kwargs = model._update_model_kwargs_for_generation(
            outputs, model_kwargs, is_encoder_decoder=False)
        unfinished_sequences = unfinished_sequences.mul(
            (min(next_tokens != i for i in eos_token_id)).long())

        output_token_ids = input_ids[0].cpu().tolist()
        output_token_ids = output_token_ids[input_length:]
        for each_eos_token_id in eos_token_id:
            if output_token_ids[-1] == each_eos_token_id:
                output_token_ids = output_token_ids[:-1]
        response = tokenizer.decode(output_token_ids)

        yield response
        # stop when each sentence is finished
        # or if we exceed the maximum length
        if unfinished_sequences.max() == 0 or stopping_criteria(
                input_ids, scores):
            break


def on_btn_click():
    del st.session_state.messages


@st.cache_resource
def load_model():
    model = (AutoModelForCausalLM.from_pretrained(model_name_or_path,
                                                  trust_remote_code=True).to(
                                                      torch.bfloat16).cuda())
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,
                                              trust_remote_code=True)
    return model, tokenizer


def prepare_generation_config():
    with st.sidebar:
        max_length = st.slider('Max Length',
                               min_value=8,
                               max_value=32768,
                               value=2048)
        top_p = st.slider('Top P', 0.0, 1.0, 0.75, step=0.01)
        temperature = st.slider('Temperature', 0.0, 1.0, 0.1, step=0.01)
        st.button('Clear Chat History', on_click=on_btn_click)

    generation_config = GenerationConfig(max_length=max_length,
                                         top_p=top_p,
                                         temperature=temperature)

    return generation_config


user_prompt = '<|im_start|>user\n{user}<|im_end|>\n'
robot_prompt = '<|im_start|>assistant\n{robot}<|im_end|>\n'
cur_query_prompt = '<|im_start|>user\n{user}<|im_end|>\n\
    <|im_start|>assistant\n'


def combine_history(prompt):
    messages = st.session_state.messages
    meta_instruction = ('')
    total_prompt = f"<s><|im_start|>system\n{meta_instruction}<|im_end|>\n"
    for message in messages:
        cur_content = message['content']
        if message['role'] == 'user':
            cur_prompt = user_prompt.format(user=cur_content)
        elif message['role'] == 'robot':
            cur_prompt = robot_prompt.format(robot=cur_content)
        else:
            raise RuntimeError
        total_prompt += cur_prompt
    total_prompt = total_prompt + cur_query_prompt.format(user=prompt)
    return total_prompt


def main():
    # torch.cuda.empty_cache()
    print('load model begin.')
    model, tokenizer = load_model()
    print('load model end.')


    st.title('InternLM2-Chat-1.8B')

    generation_config = prepare_generation_config()

    # Initialize chat history
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message['role'], avatar=message.get('avatar')):
            st.markdown(message['content'])

    # Accept user input
    if prompt := st.chat_input('What is up?'):
        # Display user message in chat message container
        with st.chat_message('user'):
            st.markdown(prompt)
        real_prompt = combine_history(prompt)
        # Add user message to chat history
        st.session_state.messages.append({
            'role': 'user',
            'content': prompt,
        })

        with st.chat_message('robot'):
            message_placeholder = st.empty()
            for cur_response in generate_interactive(
                    model=model,
                    tokenizer=tokenizer,
                    prompt=real_prompt,
                    additional_eos_token_id=92542,
                    **asdict(generation_config),
            ):
                # Display robot response in chat message container
                message_placeholder.markdown(cur_response + 'â–Œ')
            message_placeholder.markdown(cur_response)
        # Add robot response to chat history
        st.session_state.messages.append({
            'role': 'robot',
            'content': cur_response,  # pylint: disable=undefined-loop-variable
        })
        torch.cuda.empty_cache()


if __name__ == '__main__':
    main()

```

</details>

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥å¯åŠ¨åº”ç”¨ã€‚


```bash
streamlit run streamlit_demo.py
```

è¿è¡Œåï¼Œåœ¨è®¿é—®å‰ï¼Œæˆ‘ä»¬è¿˜éœ€è¦åšçš„å°±æ˜¯å°†ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ã€‚

é€šè¿‡å¦‚å›¾æ‰€ç¤ºçš„åœ°æ–¹ï¼Œè·å–å¼€å‘æœºçš„ç«¯å£å’Œå¯†ç ã€‚

![](images/image-09.png)

ç„¶ååœ¨æœ¬åœ°ä½¿ç”¨ PowerShell æˆ–è€…å‘½ä»¤è¡Œç»ˆç«¯ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

> å…¶ä¸­ï¼Œ`8501`æ˜¯Streamlitç¨‹åºçš„æœåŠ¡ç«¯å£ï¼Œ`40471`éœ€è¦æ›¿æ¢ä¸ºè‡ªå·±çš„å¼€å‘æœºçš„ç«¯å£ã€‚

```bash
ssh -CNg -L 8501:127.0.0.1:8501 root@ssh.intern-ai.org.cn -p 40471
```

ç„¶åå†è¾“å…¥å¼€å‘æœºçš„rootå¯†ç ã€‚

![](images/image-10.png)

æœ€åï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨æœ¬åœ°é€šè¿‡æµè§ˆå™¨è®¿é—®ï¼šhttp://127.0.0.1:8501 æ¥è¿›è¡Œå¯¹è¯äº†ã€‚

![](images/image-12.png)

## æ€»ç»“

ç»è¿‡æœ¬èŠ‚çš„å­¦ä¹ ï¼Œå¸¦é¢†ç€å¤§å®¶è·‘é€šäº† XTuner çš„å®Œæ•´æµç¨‹ï¼Œæˆ‘ä»¬å­¦ä¼šäº†å¢é‡é¢„è®­ç»ƒå¾®è°ƒã€æŒ‡ä»¤è·Ÿéšå¾®è°ƒã€å¤šå¡å¾®è°ƒå’Œåˆ†å¸ƒå¼å¾®è°ƒï¼Œå¹¶ä¸”è®­ç»ƒå‡ºäº†ä¸€ä¸ªè‡ªå·±å°åŠ©æ‰‹ï¼Œæ˜¯ä¸æ˜¯å¾ˆæœ‰æ„æ€ï¼

æ˜¯ä¸æ˜¯æ„Ÿè§‰å…¶å®å¾®è°ƒä¹Ÿä¸è¿‡å¦‚æ­¤ï¼äº‹å®ä¸Šç¡®å®æ˜¯è¿™æ ·çš„ï¼å…¶å®åœ¨å¾®è°ƒçš„æ—¶å€™æœ€é‡è¦çš„è¿˜æ˜¯è¦è‡ªå·±å‡†å¤‡ä¸€ä»½é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œè¿™ä¸ªæ‰æ˜¯ä½ èƒ½å¦çœŸå¾®è°ƒå‡ºæ•ˆæœæœ€æ ¸å¿ƒçš„åˆ©å™¨ã€‚

å½“æˆ‘ä»¬åœ¨æµ‹è¯•å®Œæ¨¡å‹è®¤ä¸ºå…¶æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚åï¼Œå°±å¯ä»¥å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–éƒ¨ç½²ç­‰æ“ä½œäº†ï¼Œè¿™éƒ¨åˆ†çš„å†…å®¹åœ¨ä¹‹åå…³äº LMDeploy çš„è¯¾ç¨‹ä¸­å°†ä¼šè¯¦ç»†çš„è¿›è¡Œè®²è§£ï¼Œæ•¬è¯·æœŸå¾…åç»­çš„è¯¾ç¨‹å§ï¼

