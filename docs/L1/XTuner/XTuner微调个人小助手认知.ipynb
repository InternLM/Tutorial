{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451252c1-8bf5-461c-aaa6-77fa509c69d5",
   "metadata": {},
   "source": [
    "# XTunerå¾®è°ƒä¸ªäººå°åŠ©æ‰‹è®¤çŸ¥\n",
    "\n",
    "åœ¨æœ¬èŠ‚ä¸­ï¼Œå°†ä¸€æ­¥æ­¥å¸¦é¢†å¤§å®¶ä½“éªŒå¦‚ä½•ä½¿ç”¨ XTuner å®Œæˆä¸ªäººå°åŠ©æ‰‹çš„å¾®è°ƒï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bd9a97-42ee-478b-989a-7b32a57cf035",
   "metadata": {},
   "source": [
    "## åŸºæœ¬æ¦‚å¿µ\n",
    "\n",
    "åœ¨è¿›è¡Œå¾®è°ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£ä¸€äº›åŸºæœ¬æ¦‚å¿µã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713e375-e017-4d64-823e-ef762819df64",
   "metadata": {},
   "source": [
    "### Fintuneç®€ä»‹\n",
    "\n",
    "å¾®è°ƒï¼ˆfine-tuningï¼‰æ˜¯ä¸€ç§åŸºäºé¢„è®­ç»ƒæ¨¡å‹ï¼Œé€šè¿‡å°‘é‡çš„è°ƒæ•´ï¼ˆfine-tuneï¼‰æ¥é€‚åº”æ–°çš„ä»»åŠ¡æˆ–æ•°æ®çš„æ–¹æ³•ã€‚\n",
    "\n",
    "å¾®è°ƒæ˜¯åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œå°†æ¨¡å‹ä¸­ä¸€äº›å±‚çš„æƒé‡å‚æ•°è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”æ–°çš„æ•°æ®é›†æˆ–ä»»åŠ¡ã€‚\n",
    "\n",
    "é¢„è®­ç»ƒæ¨¡å‹éƒ¨åˆ†å·²ç»åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šå¾—åˆ°äº†è®­ç»ƒï¼Œå®ƒä»¬é€šå¸¸æ˜¯è¾ƒä¸ºé€šç”¨ä¸”é«˜æ€§èƒ½çš„æ¨¡å‹ï¼Œå› æ­¤å¯ä»¥å¾ˆå¥½åœ°ä½œä¸ºæ–°ä»»åŠ¡çš„èµ·ç‚¹ã€‚å¾®è°ƒå¯ä»¥åŠ å¿«æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦ï¼Œé™ä½æ¨¡å‹è¿‡æ‹Ÿåˆçš„é£é™©ï¼Œå¹¶åœ¨ä¸æ¶ˆè€—è¿‡å¤šè®¡ç®—èµ„æºçš„æƒ…å†µä¸‹è·å–è¾ƒå¥½çš„æ¨¡å‹æ€§èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3a5f74-e1c8-4ea4-8c59-8ac9ba008e3f",
   "metadata": {},
   "source": [
    "#### Fintuneçš„ä¸¤ç§èŒƒå¼\n",
    "\n",
    "åœ¨å¤§æ¨¡å‹çš„ä¸‹æ¸¸åº”ç”¨ä¸­ï¼Œç»å¸¸ä¼šç”¨åˆ°ä¸¤ç§å¾®è°ƒæ¨¡å¼ï¼š**å¢é‡é¢„è®­ç»ƒ** å’Œ **æŒ‡ä»¤è·Ÿéš** ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9159660-8160-4d6f-8271-177cbb3ec7b8",
   "metadata": {},
   "source": [
    "1. **å¢é‡é¢„è®­ç»ƒ**\n",
    "\n",
    "å¢é‡é¢„è®­ç»ƒæ˜¯ä¸€ç§åœ¨å·²æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼ˆæ¯”å¦‚ï¼šInternLMåŸºåº§æ¨¡å‹ï¼‰çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨ç‰¹å®šé¢†åŸŸçš„æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒçš„æ–¹æ³•ã€‚å®ƒçš„ç›®çš„æ˜¯åœ¨ä¿æŒæ¨¡å‹åŸæœ‰èƒ½åŠ›çš„åŒæ—¶ï¼Œæ³¨å…¥æ–°çš„é¢†åŸŸçŸ¥è¯†ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ˆæ¯”å¦‚ï¼šInternLMå‚ç±»åŸºåº§æ¨¡å‹ï¼‰ã€‚å¢é‡é¢„è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿæ¥å—å°‘é‡çš„æ–°æ•°æ®è¿›è¡Œæ›´æ–°å¹¶é€‚åº”æ–°çš„ä»»åŠ¡ï¼Œè€Œä¸éœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹ï¼Œè¿™ç§æ–¹å¼å¯ä»¥å¾ˆå¥½åœ°åˆ©ç”¨ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†ï¼Œå¹¶åœ¨æ–°æ•°æ®ä¸Šè·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e574b1c-4c61-47d1-8462-7854bb53cee0",
   "metadata": {},
   "source": [
    "2. **æŒ‡ä»¤è·Ÿéš**\n",
    "\n",
    "æŒ‡ä»¤è·Ÿéšæ˜¯æŒ‡è®©æ¨¡å‹æ ¹æ®ç”¨æˆ·è¾“å…¥çš„æŒ‡ä»¤æ¥æ‰§è¡Œç›¸åº”çš„æ“ä½œã€‚æ¨¡å‹é€šè¿‡å¯¹å¤§é‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œç›¸åº”æ“ä½œçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ å¦‚ä½•å°†æŒ‡ä»¤åˆ†è§£ä¸ºå…·ä½“çš„å­ä»»åŠ¡ï¼Œå¹¶é€‰æ‹©åˆé€‚çš„æ¨¡å—æ¥æ‰§è¡Œè¿™äº›ä»»åŠ¡ï¼ˆæ¯”å¦‚ï¼šInternLMå‚ç±»å¯¹è¯æ¨¡å‹ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7d7e6c-f1a7-41d4-b172-6c9e45222915",
   "metadata": {},
   "source": [
    "### å¾®è°ƒæŠ€æœ¯\n",
    "\n",
    "å¤§å¤šæ•°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‚æ•°è§„æ¨¡å·¨å¤§ï¼Œä¸”è§„æ¨¡æ—¥ç›Šå¢å¤§ï¼Œå¯¼è‡´æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒæˆæœ¬é«˜æ˜‚ï¼Œç›´æ¥è®­ç»ƒéœ€è¦è€—è´¹å¤§é‡è®¡ç®—èµ„æºå’Œè´¹ç”¨ã€‚è¿‘å¹´æ¥ï¼Œå¦‚ä½•é«˜æ•ˆåœ°å¯¹å¤§æ¨¡å‹è¿›è¡Œå¾®è°ƒæˆä¸ºäº†ç ”ç©¶çƒ­ç‚¹ï¼Œè€ŒLoRAå’ŒQLoRAä¸¤ç§å¾®è°ƒæŠ€æœ¯å› å…¶é«˜æ•ˆæ€§å’Œå®ç”¨æ€§å—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aec914-2db7-415f-9561-c1875b021609",
   "metadata": {},
   "source": [
    "#### LoRAç®€ä»‹\n",
    "\n",
    "LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯ä¸€ç§ä½¿ç”¨ä½ç²¾åº¦æƒé‡å¯¹å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æŠ€æœ¯ï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨ä¸æ”¹å˜åŸæœ‰æ¨¡å‹æƒé‡çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æ·»åŠ å°‘é‡æ–°å‚æ•°æ¥è¿›è¡Œå¾®è°ƒã€‚è¿™ç§æ–¹æ³•é™ä½äº†æ¨¡å‹çš„å­˜å‚¨éœ€æ±‚ï¼Œä¹Ÿé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå®ç°äº†å¯¹å¤§æ¨¡å‹çš„å¿«é€Ÿé€‚åº”ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹æ€§èƒ½ã€‚\n",
    "\n",
    "ç„¶è€Œï¼Œç”±äºä½¿ç”¨äº†ä½ç²¾åº¦æƒé‡ï¼ŒLoRAçš„ä¸€ä¸ªæ½œåœ¨çš„ç¼ºç‚¹æ˜¯åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šä¸¢å¤±ä¸€äº›åŸå§‹æ¨¡å‹çš„é«˜é˜¶ç‰¹å¾ä¿¡æ¯ï¼Œå› æ­¤å¯èƒ½ä¼šé™ä½æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d9b318-17bd-4e22-97b4-9ae8d43db373",
   "metadata": {},
   "source": [
    "#### QLoRAç®€ä»‹\n",
    "\n",
    "QLoRAï¼ˆQuantized LoRAï¼‰å¾®è°ƒæŠ€æœ¯æ˜¯å¯¹LoRAçš„ä¸€ç§æ”¹è¿›ï¼Œå®ƒé€šè¿‡å¼•å…¥é«˜ç²¾åº¦æƒé‡å’Œå¯å­¦ä¹ çš„ä½ç§©é€‚é…å™¨æ¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚å¹¶ä¸”åœ¨LoRAçš„åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†é‡åŒ–æŠ€æœ¯ã€‚é€šè¿‡å°†é¢„è®­ç»ƒæ¨¡å‹é‡åŒ–ä¸ºint4æ ¼å¼ï¼Œå¯ä»¥è¿›ä¸€æ­¥å‡å°‘å¾®è°ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—é‡ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥å‡å°‘æ¨¡å‹çš„å­˜å‚¨ç©ºé—´ï¼Œè¿™å¯¹äºåœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šè¿è¡Œæ¨¡å‹éå¸¸æœ‰ç”¨ã€‚æœ€ç»ˆï¼Œå¯ä»¥ä½¿æˆ‘ä»¬åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šè¿›è¡Œæ¨¡å‹çš„å¾®è°ƒè®­ç»ƒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ec109-c96e-47d3-a004-54b3594a6dd2",
   "metadata": {},
   "source": [
    "###  XTunerç®€ä»‹\n",
    "\n",
    "XTuner çš„å®˜æ–¹ä»“åº“æ˜¯ï¼šhttps://github.com/InternLM/xtuner ï¼ˆæ¬¢è¿Starï¼‰ï¼\n",
    "\n",
    "XTuner ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹&å¤šæ¨¡æ€æ¨¡å‹å¾®è°ƒå·¥å…·ç®±ã€‚*ç”±* *MMRazor* *å’Œ* *MMDeploy* *è”åˆå¼€å‘ã€‚*\n",
    "\n",
    "- ğŸ¤“ **å‚»ç“œåŒ–ï¼š** ä»¥ é…ç½®æ–‡ä»¶ çš„å½¢å¼å°è£…äº†å¤§éƒ¨åˆ†å¾®è°ƒåœºæ™¯ï¼Œ**0åŸºç¡€çš„éä¸“ä¸šäººå‘˜ä¹Ÿèƒ½ä¸€é”®å¼€å§‹å¾®è°ƒ**ã€‚\n",
    "- ğŸƒ **è½»é‡çº§ï¼š** å¯¹äº 7B å‚æ•°é‡çš„LLMï¼Œ**å¾®è°ƒæ‰€éœ€çš„æœ€å°æ˜¾å­˜ä»…ä¸º 8GB** ï¼š **æ¶ˆè´¹çº§æ˜¾å¡âœ…ï¼Œcolabâœ…**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c504dff4-6bfc-4b49-9677-5bb69d2f6de2",
   "metadata": {},
   "source": [
    "#### åŠŸèƒ½äº®ç‚¹\n",
    "\n",
    "- é€‚é…å¤šç§ç”Ÿæ€\n",
    "  - æ”¯æŒå¤šç§å¾®è°ƒç®—æ³•\n",
    "  - é€‚é…å¤šç§å¼€æºç”Ÿæ€ï¼ˆHuggingFaceã€ModelScopeç­‰ï¼‰\n",
    "  - è‡ªåŠ¨ä¼˜åŒ–åŠ é€Ÿå™¨\n",
    "- é€‚é…å¤šç§ç¡¬ä»¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a4be9-0e62-4590-86b9-81c25ef992aa",
   "metadata": {},
   "source": [
    "#### å¸¸ç”¨å‘½ä»¤\n",
    "\n",
    "ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸ç”¨çš„å‘½ä»¤ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e402ad8",
   "metadata": {},
   "source": [
    "- æŸ¥çœ‹å¸®åŠ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d7fef-ec17-4f10-bdbb-6f2a1888f7dc",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!xtuner help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b19ae14",
   "metadata": {},
   "source": [
    "- æŸ¥çœ‹ç‰ˆæœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ceec9e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!xtuner version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fde918",
   "metadata": {},
   "source": [
    "- åˆ—å‡ºæ‰€æœ‰é¢„å®šä¹‰é…ç½®æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f01681",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!xtuner list-cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7eb41",
   "metadata": {},
   "source": [
    "- åˆ—å‡ºåŒ…å«æŒ‡å®šåç§°çš„é¢„å®šä¹‰é…ç½®æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f1e273",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!xtuner list-cfg -p $NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfe5df8",
   "metadata": {},
   "source": [
    "- å¤åˆ¶é…ç½®æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101d2f9",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!xtuner copy-cfg $CONFIG $SAVE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf714be4",
   "metadata": {},
   "source": [
    "- æ‰§è¡Œå¾®è°ƒè®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab38c7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!xtuner train $CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592bb35e",
   "metadata": {},
   "source": [
    "- å°† pth æ ¼å¼çš„æ¨¡å‹æ–‡ä»¶è½¬æ¢æˆ HuggingFace æ ¼å¼çš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f51a4",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!xtuner convert pth_to_hf $CONFIG $PATH_TO_PTH_MODEL $SAVE_PATH_TO_HF_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ce9918",
   "metadata": {},
   "source": [
    "- å°†åŸå§‹æ¨¡å‹ä¸å¾®è°ƒç»“æœè¿›è¡Œåˆå¹¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdea156",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!xtuner convert merge $LLM $ADAPTER $SAVE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a475b2-a2a7-4ce4-8b2e-90e2225f6e87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## åˆ›å»ºå¼€å‘æœº\n",
    "\n",
    "é¦–å…ˆæˆ‘ä»¬éœ€è¦å‰å¾€ [InternStudio](https://studio.intern-ai.org.cn/) ä¸­åˆ›å»ºä¸€å°å¼€å‘æœºè¿›è¡Œä½¿ç”¨ã€‚\n",
    "\n",
    "<detail>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0db21bf-26f2-41eb-ae7a-45c502689248",
   "metadata": {},
   "source": [
    "- ç™»å½•InternStudioåï¼Œåœ¨æ§åˆ¶å°ç‚¹å‡» â€œåˆ›å»ºå¼€å‘æœºâ€ æŒ‰é’®å¯ä»¥è¿›å…¥åˆ°å¼€å‘æœºçš„åˆ›å»ºç•Œé¢ã€‚\n",
    "\n",
    "![](images/image-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c894d98-5a00-47c3-b43c-9806fbe61337",
   "metadata": {},
   "source": [
    "- åœ¨ â€œåˆ›å»ºå¼€å‘æœºâ€ ç•Œé¢ï¼Œé€‰æ‹©å¼€å‘æœºç±»å‹ï¼šä¸ªäººå¼€å‘æœºï¼Œè¾“å…¥å¼€å‘æœºåç§°ï¼šXTunerå¾®è°ƒï¼Œé€‰æ‹©å¼€å‘æœºé•œåƒï¼šCuda11.7-condaã€‚\n",
    "\n",
    "![](images/image-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa96613-3304-4933-ae4e-75d98fd40268",
   "metadata": {},
   "source": [
    "- åœ¨é•œåƒè¯¦æƒ…ç•Œé¢ï¼Œç‚¹å‡» â€œä½¿ç”¨â€ é“¾æ¥ï¼Œç¡®è®¤ä½¿ç”¨è¯¥é•œåƒã€‚\n",
    "\n",
    "![](images/image-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97284ce8-8eb1-4df1-ab39-10da38df2cac",
   "metadata": {},
   "source": [
    "- èµ„æºé…ç½®å¯ä»¥é€‰æ‹© 30% ï¼Œç„¶åç‚¹å‡» â€œç«‹å³åˆ›å»ºâ€ æŒ‰é’®åˆ›å»ºå¼€å‘æœºã€‚\n",
    "\n",
    "![](images/image-04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce873c-92c5-43c8-b4d8-9838fe10a5c0",
   "metadata": {},
   "source": [
    "- åˆ›å»ºå®Œæˆåï¼Œåœ¨å¼€å‘æœºåˆ—è¡¨ä¸­å¯ä»¥çœ‹åˆ°åˆšåˆ›å»ºçš„å¼€å‘æœºï¼Œç‚¹å‡» â€œè¿›å…¥å¼€å‘æœºâ€ é“¾æ¥å¯ä»¥è¿æ¥è¿›å…¥åˆ°å¼€å‘æœºã€‚\n",
    "\n",
    "![](images/image-05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c080138",
   "metadata": {},
   "source": [
    "## å‡†å¤‡å·¥ä½œ\n",
    "\n",
    "1. **ç¯å¢ƒå®‰è£…**ï¼šæˆ‘ä»¬æƒ³è¦ç”¨ç®€å•æ˜“ä¸Šæ‰‹çš„å¾®è°ƒå·¥å…·åŒ… XTuner æ¥å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„è¯ï¼Œç¬¬ä¸€æ­¥æ˜¯å®‰è£… XTuner ï¼å®‰è£…åŸºç¡€çš„å·¥å…·æ˜¯ä¸€åˆ‡çš„å‰æï¼Œåªæœ‰å®‰è£…äº† XTuner æˆ‘ä»¬æ‰èƒ½å¤Ÿå»æ‰§è¡Œåç»­çš„æ“ä½œã€‚\n",
    "\n",
    "2. **å‰æœŸå‡†å¤‡**ï¼šåœ¨å®Œæˆ XTuner çš„å®‰è£…åï¼Œæˆ‘ä»¬ä¸‹ä¸€æ­¥å°±éœ€è¦å»æ˜ç¡®æˆ‘ä»¬è‡ªå·±çš„å¾®è°ƒç›®æ ‡äº†ã€‚æˆ‘ä»¬æƒ³è¦åˆ©ç”¨å¾®è°ƒåšä¸€äº›ä»€ä¹ˆäº‹æƒ…å‘¢ï¼Œç„¶åä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡ç›¸å…³çš„ç¡¬ä»¶èµ„æºå’Œæ•°æ®ã€‚\n",
    "\n",
    "3. **å¯åŠ¨å¾®è°ƒ**ï¼šåœ¨ç¡®å®šäº†è‡ªå·±çš„å¾®è°ƒç›®æ ‡åï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨ XTuner çš„é…ç½®åº“ä¸­æ‰¾åˆ°åˆé€‚çš„é…ç½®æ–‡ä»¶å¹¶è¿›è¡Œå¯¹åº”çš„ä¿®æ”¹ã€‚ä¿®æ”¹å®Œæˆåå³å¯ä¸€é”®å¯åŠ¨è®­ç»ƒï¼è®­ç»ƒå¥½çš„æ¨¡å‹ä¹Ÿå¯ä»¥ä»…ä»…é€šè¿‡åœ¨ç»ˆç«¯è¾“å…¥ä¸€è¡Œå‘½ä»¤æ¥å®Œæˆè½¬æ¢å’Œéƒ¨ç½²å·¥ä½œï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de6991f",
   "metadata": {},
   "source": [
    "### åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ\n",
    "\n",
    "åœ¨å®‰è£… XTuner ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒã€‚åˆ›å»ºä¸€ä¸ªåä¸º `xtuner0121` çš„è™šæ‹Ÿç¯å¢ƒï¼Œå¯ä»¥ç›´æ¥æ‰§è¡Œå‘½ä»¤ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead18d70",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!conda create -n xtuner0121 python=3.10 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b70777",
   "metadata": {},
   "source": [
    "å¦‚æœæ˜¯åœ¨å¼€å‘æœºä¸­ï¼Œä¹Ÿå¯ä»¥ç›´æ¥æ‰§è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œåˆ›å»ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d9799",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!studio-conda -t xtuner0121 -o internlm-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f48956",
   "metadata": {},
   "source": [
    "è™šæ‹Ÿç¯å¢ƒåˆ›å»ºå®Œæˆåï¼Œéœ€è¦æ¿€æ´»è™šæ‹Ÿç¯å¢ƒã€‚\n",
    "\n",
    "```bash\n",
    "conda activate xtuner0121\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38ba7d",
   "metadata": {},
   "source": [
    "### å®‰è£… XTuner\n",
    "\n",
    "è™šæ‹Ÿç¯å¢ƒåˆ›å»ºå®Œæˆåï¼Œå°±å¯ä»¥å®‰è£… XTuner äº†ã€‚é¦–å…ˆï¼Œä» Github ä¸Šä¸‹è½½æºç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728440a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!git clone -b v0.1.21  https://github.com/InternLM/xtuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328c1ef1",
   "metadata": {},
   "source": [
    "å…¶æ¬¡ï¼Œè¿›å…¥æºç ç›®å½•ï¼Œæ‰§è¡Œå®‰è£…ã€‚\n",
    "\n",
    "> å¦‚æœé€Ÿåº¦å¤ªæ…¢å¯ä»¥æ¢æˆ `pip install -e '.[all]' -i https://mirrors.aliyun.com/pypi/simple/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6dd99d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!cd xtuner && pip install -e '.[all]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0757a85",
   "metadata": {},
   "source": [
    "æœ€åï¼Œæˆ‘ä»¬å¯ä»¥éªŒè¯ä¸€ä¸‹å®‰è£…ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0629e6",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!xtuner version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24eabe5",
   "metadata": {},
   "source": [
    "å¯¹äºå¾ˆå¤šåˆå­¦è€…è€Œè¨€ï¼Œæˆ‘ä»¬å¯èƒ½ä¸å¤ªç†Ÿæ‚‰ XTuner çš„ç”¨æ³•ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¥æŸ¥çœ‹ç›¸å…³çš„å¸®åŠ©ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc7396",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!xtuner help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b25987e",
   "metadata": {},
   "source": [
    "å¯¹äºå¾ˆå¤šçš„åˆå­¦è€…è€Œè¨€ï¼Œå®‰è£…å¥½ç¯å¢ƒæ„å‘³ç€æˆåŠŸäº†ä¸€å¤§åŠï¼å› æ­¤æˆ‘ä»¬æ¥ä¸‹æ¥å°±å¯ä»¥è¿›å…¥æˆ‘ä»¬çš„ä¸‹ä¸€æ­¥ï¼Œå‡†å¤‡å¥½æˆ‘ä»¬éœ€è¦çš„æ¨¡å‹ã€æ•°æ®é›†å’Œé…ç½®æ–‡ä»¶ï¼Œå¹¶è¿›è¡Œå¾®è°ƒè®­ç»ƒï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f6bad6",
   "metadata": {},
   "source": [
    "### æ¨¡å‹å‡†å¤‡\n",
    "\n",
    "è½¯ä»¶å®‰è£…å¥½åï¼Œæˆ‘ä»¬å°±å¯ä»¥å‡†å¤‡è¦å¾®è°ƒçš„æ¨¡å‹äº†ã€‚\n",
    "\n",
    "> å¯¹äºå­¦ä¹ è€Œè¨€ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ InternLM æ¨å‡ºçš„1.8Bçš„å°æ¨¡å‹æ¥å®Œæˆæ­¤æ¬¡å¾®è°ƒæ¼”ç¤ºã€‚\n",
    "\n",
    "å¯¹äºåœ¨ InternStudio ä¸Šè¿è¡Œçš„å°ä¼™ä¼´ä»¬ï¼Œå¯ä»¥ä¸ç”¨é€šè¿‡ HuggingFaceã€OpenXLab æˆ–è€… Modelscope è¿›è¡Œæ¨¡å‹çš„ä¸‹è½½ï¼Œåœ¨å¼€å‘æœºä¸­å·²ç»ä¸ºæˆ‘ä»¬æä¾›äº†æ¨¡å‹çš„æœ¬åœ°æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨å°±å¯ä»¥äº†ã€‚\n",
    "\n",
    "> æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç ä¸€é”®é€šè¿‡ç¬¦å·é“¾æ¥çš„æ–¹å¼é“¾æ¥åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¿™æ ·æ—¢èŠ‚çœäº†ç©ºé—´ï¼Œä¹Ÿä¾¿äºç®¡ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d9828b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p Shanghai_AI_Laboratory\n",
    "\n",
    "!ln -s /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b Shanghai_AI_Laboratory/internlm2-chat-1_8b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75fcb97",
   "metadata": {},
   "source": [
    "æ‰§è¡Œä¸Šè¿°æ“ä½œåï¼Œ`Shanghai_AI_Laboratory/internlm2-chat-1_8b` å°†ç›´æ¥æˆä¸ºä¸€ä¸ªç¬¦å·é“¾æ¥ï¼Œè¿™ä¸ªé“¾æ¥æŒ‡å‘ `/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b` çš„ä½ç½®ã€‚\n",
    "\n",
    "è¿™æ„å‘³ç€ï¼Œå½“æˆ‘ä»¬è®¿é—® `Shanghai_AI_Laboratory/internlm2-chat-1_8b` æ—¶ï¼Œå®é™…ä¸Šå°±æ˜¯åœ¨è®¿é—® `/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b` ç›®å½•ä¸‹çš„å†…å®¹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬æ— éœ€å¤åˆ¶ä»»ä½•æ•°æ®ï¼Œå°±å¯ä»¥ç›´æ¥åˆ©ç”¨ç°æœ‰çš„æ¨¡å‹æ–‡ä»¶è¿›è¡Œåç»­çš„å¾®è°ƒæ“ä½œï¼Œä»è€ŒèŠ‚çœå­˜å‚¨ç©ºé—´å¹¶ç®€åŒ–æ–‡ä»¶ç®¡ç†ã€‚\n",
    "\n",
    "å¦‚æœè‡ªå·±æƒ³è¦å¾®è°ƒçš„æ¨¡å‹åœ¨å¼€å‘æœºä¸­æ²¡æ‰¾åˆ°ï¼Œä¹Ÿå¯ä»¥è‡ªå·±ä¸‹è½½ç›¸å…³æ¨¡å‹æ–‡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm2-1_8b', cache_dir=\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec6e564",
   "metadata": {},
   "source": [
    "æ¨¡å‹æ–‡ä»¶å‡†å¤‡å¥½åï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™ä¸ªæ ·å­çš„ã€‚\n",
    "\n",
    "<details>\n",
    "<summary>ç›®å½•ç»“æ„</summary>\n",
    "\n",
    "```\n",
    "â”œâ”€â”€ Shanghai_AI_Laboratory\n",
    "â”‚   â”œâ”€â”€ internlm2-1_8b\n",
    "â”‚   â”‚   â”œâ”€â”€ README.md\n",
    "â”‚   â”‚   â”œâ”€â”€ config.json\n",
    "â”‚   â”‚   â”œâ”€â”€ configuration.json\n",
    "â”‚   â”‚   â”œâ”€â”€ configuration_internlm2.py\n",
    "â”‚   â”‚   â”œâ”€â”€ generation_config.json\n",
    "â”‚   â”‚   â”œâ”€â”€ modeling_internlm2.py\n",
    "â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin\n",
    "â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2.py\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2_fast.py\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenizer.json\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenizer.model\n",
    "â”‚   â”‚   â””â”€â”€ tokenizer_config.json\n",
    "â”‚   â””â”€â”€ internlm2-chat-1_8b -> /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b\n",
    "â”‚       â”œâ”€â”€ README.md\n",
    "â”‚       â”œâ”€â”€ config.json\n",
    "â”‚       â”œâ”€â”€ configuration.json\n",
    "â”‚       â”œâ”€â”€ configuration_internlm2.py\n",
    "â”‚       â”œâ”€â”€ generation_config.json\n",
    "â”‚       â”œâ”€â”€ model-00001-of-00002.safetensors\n",
    "â”‚       â”œâ”€â”€ model-00002-of-00002.safetensors\n",
    "â”‚       â”œâ”€â”€ model.safetensors.index.json\n",
    "â”‚       â”œâ”€â”€ modeling_internlm2.py\n",
    "â”‚       â”œâ”€â”€ special_tokens_map.json\n",
    "â”‚       â”œâ”€â”€ tokenization_internlm2.py\n",
    "â”‚       â”œâ”€â”€ tokenization_internlm2_fast.py\n",
    "â”‚       â”œâ”€â”€ tokenizer.model\n",
    "â”‚       â””â”€â”€ tokenizer_config.json\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "> åœ¨ç›®å½•ç»“æ„ä¸­å¯ä»¥çœ‹å‡ºï¼Œ`internlm2-chat-1_8b` æ˜¯ä¸€ä¸ªç¬¦å·é“¾æ¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb7feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da820a2-86e8-4f8a-ada4-3750b6dbe445",
   "metadata": {},
   "source": [
    "## å¢é‡é¢„è®­ç»ƒå¾®è°ƒ\n",
    "\n",
    "æœ¬èŠ‚æˆ‘ä»¬å…ˆæ¥äº†è§£ä¸€ä¸‹å¢é‡é¢„è®­ç»ƒï¼Œè¿™é‡Œæˆ‘ä»¬ä»¥ä¸€ä¸ªæ–‡æœ¬ç»­å†™æ¡ˆä¾‹æ¥çœ‹çœ‹æ•ˆæœã€‚\n",
    "\n",
    "|  | å¾®è°ƒå‰ | å¾®è°ƒå |\n",
    "| --- | --- | --- |\n",
    "| è¾“å…¥ | ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯ | ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯ |\n",
    "| è¾“å‡º| ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯ä¸Šå‘¨äº”ï¼Œä¸Šå‘¨äº”æˆ‘ä»¬å­¦ä¹ äº†ä¸€ä¸ªæ–°çš„çŸ¥è¯†ï¼Œé‚£å°±æ˜¯å…³äºæœºå™¨å­¦ä¹ çš„æ¦‚ç‡ç»Ÿè®¡ã€‚â€¦â€¦ | ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤æ¨å‡ºçš„ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç³»åˆ—æ´»åŠ¨çš„ç¬¬ä¸‰æ‰¹æ¬¡ï¼Œå°†äº2024å¹´7æœˆæ­£å¼è¿›è¡Œã€‚â€¦â€¦ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f1238e",
   "metadata": {},
   "source": [
    "- å¯¼å…¥å¿…è¦çš„åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73995f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d97ce",
   "metadata": {},
   "source": [
    "- å®šä¹‰æ¨¡å‹åŠ è½½æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c275406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True).cuda()\n",
    "    model = model.eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0a7754",
   "metadata": {},
   "source": [
    "- å®šä¹‰æ–‡æœ¬ç»­å†™æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fd8995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(user_input):\n",
    "    gen_kwargs = {\"max_length\": 128, \"top_p\": 0.8, \"temperature\": 0.8, \"do_sample\": True, \"repetition_penalty\": 1.0}\n",
    "\n",
    "    inputs = tokenizer([user_input], return_tensors=\"pt\")\n",
    "    for k,v in inputs.items():\n",
    "        inputs[k] = v.cuda()\n",
    "    output = model.generate(**inputs, **gen_kwargs)\n",
    "    output = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac7a4e4",
   "metadata": {},
   "source": [
    "### åŸºåº§æ¨¡å‹æ¨ç†\n",
    "\n",
    "æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹åŸºåº§æ¨¡å‹çš„æ¨ç†ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d48c8a",
   "metadata": {},
   "source": [
    "- åŠ è½½æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cb798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model(\"Shanghai_AI_Laboratory/internlm2-1_8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f536a771",
   "metadata": {},
   "source": [
    "- æ–‡æœ¬ç»­å†™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ccf83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\"ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa279db",
   "metadata": {},
   "source": [
    "- é‡Šæ”¾ç¼“å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenizer, model\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694693a",
   "metadata": {},
   "source": [
    "### å¢é‡é¢„è®­ç»ƒ\n",
    "\n",
    "ç„¶åæˆ‘ä»¬å¯¹åŸºåº§æ¨¡å‹è¿›è¡Œå¢é‡é¢„è®­ç»ƒï¼Œè®©æ¨¡å‹å¢åŠ æ–°çš„çŸ¥è¯†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea34851",
   "metadata": {},
   "source": [
    "#### å‡†å¤‡æ•°æ®æ–‡ä»¶\n",
    "\n",
    "ä¸ºäº†è®©æ¨¡å‹å­¦ä¹ åˆ°æ–°çš„çŸ¥è¯†ï¼Œæˆ‘ä»¬éœ€è¦å°†æ–°çš„çŸ¥è¯†æ•°æ®æ•´ç†æˆæŒ‡å®šæ ¼å¼æ–‡ä»¶ï¼Œå½¢æˆæ•°æ®é›†ï¼Œç„¶åè®©æ¨¡å‹æ¥å­¦ä¹ è¿™äº›æ–°æ•°æ®ã€‚è¿™é‡Œæˆ‘ä»¬å‡†å¤‡ä¸€ä¸ªç®€å•çš„æ•°æ®é›† `datas/pretrain.json`ï¼Œä»…åŒ…å«ä¸€æ¡çŸ¥è¯†ï¼Œç„¶åè®©æ•°æ®é‡å¤å¤šæ¬¡ã€‚\n",
    "\n",
    "> ç½‘ä¸Šæœ‰å¤§é‡çš„å¼€æºæ•°æ®é›†å¯ä»¥ä¾›æˆ‘ä»¬è¿›è¡Œä½¿ç”¨ï¼Œæœ‰äº›æ—¶å€™æˆ‘ä»¬å¯ä»¥åœ¨å¼€æºæ•°æ®é›†çš„åŸºç¡€ä¸Šæ·»åŠ ä¸€äº›æˆ‘ä»¬è‡ªå·±ç‹¬æœ‰çš„æ•°æ®é›†ï¼Œä¹Ÿå¯èƒ½ä¼šæœ‰å¾ˆå¥½çš„æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1502071",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "        \"text\": \"ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤æ¨å‡ºçš„ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç³»åˆ—æ´»åŠ¨çš„ç¬¬ä¸‰æ‰¹æ¬¡ï¼Œå°†äº2024å¹´7æœˆæ­£å¼è¿›è¡Œã€‚\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e305c",
   "metadata": {},
   "source": [
    "å‡†å¤‡å¥½æ•°æ®æ–‡ä»¶åï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚\n",
    "\n",
    "<details>\n",
    "<summary>ç›®å½•ç»“æ„</summary>\n",
    "\n",
    "```\n",
    "â”œâ”€â”€ Shanghai_AI_Laboratory\n",
    "â”‚   â”œâ”€â”€ internlm2-1_8b\n",
    "â”‚   â”‚   â”œâ”€â”€ README.md\n",
    "â”‚   â”‚   â”œâ”€â”€ config.json\n",
    "â”‚   â”‚   â”œâ”€â”€ configuration.json\n",
    "â”‚   â”‚   â”œâ”€â”€ configuration_internlm2.py\n",
    "â”‚   â”‚   â”œâ”€â”€ generation_config.json\n",
    "â”‚   â”‚   â”œâ”€â”€ modeling_internlm2.py\n",
    "â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin\n",
    "â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2.py\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2_fast.py\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenizer.json\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenizer.model\n",
    "â”‚   â”‚   â””â”€â”€ tokenizer_config.json\n",
    "â”‚   â””â”€â”€ internlm2-chat-1_8b -> /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b\n",
    "â”‚       â”œâ”€â”€ README.md\n",
    "â”‚       â”œâ”€â”€ config.json\n",
    "â”‚       â”œâ”€â”€ configuration.json\n",
    "â”‚       â”œâ”€â”€ configuration_internlm2.py\n",
    "â”‚       â”œâ”€â”€ generation_config.json\n",
    "â”‚       â”œâ”€â”€ model-00001-of-00002.safetensors\n",
    "â”‚       â”œâ”€â”€ model-00002-of-00002.safetensors\n",
    "â”‚       â”œâ”€â”€ model.safetensors.index.json\n",
    "â”‚       â”œâ”€â”€ modeling_internlm2.py\n",
    "â”‚       â”œâ”€â”€ special_tokens_map.json\n",
    "â”‚       â”œâ”€â”€ tokenization_internlm2.py\n",
    "â”‚       â”œâ”€â”€ tokenization_internlm2_fast.py\n",
    "â”‚       â”œâ”€â”€ tokenizer.model\n",
    "â”‚       â””â”€â”€ tokenizer_config.json\n",
    "â”œâ”€â”€ datas\n",
    "â”‚   â””â”€â”€ pretrain.json\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e7e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9221ff",
   "metadata": {},
   "source": [
    "#### å‡†å¤‡é…ç½®æ–‡ä»¶\n",
    "\n",
    "åœ¨å‡†å¤‡å¥½äº†æ¨¡å‹å’Œæ•°æ®é›†åï¼Œæˆ‘ä»¬å°±è¦æ ¹æ®æˆ‘ä»¬é€‰æ‹©çš„å¾®è°ƒæ–¹æ³•ç»“åˆå¾®è°ƒæ–¹æ¡ˆæ¥æ‰¾åˆ°ä¸æˆ‘ä»¬æœ€åŒ¹é…çš„é…ç½®æ–‡ä»¶äº†ï¼Œä»è€Œå‡å°‘æˆ‘ä»¬å¯¹é…ç½®æ–‡ä»¶çš„ä¿®æ”¹é‡ã€‚\n",
    "\n",
    "> é…ç½®æ–‡ä»¶å…¶å®æ˜¯ä¸€ç§ç”¨äºå®šä¹‰å’Œæ§åˆ¶æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­å„ä¸ªæ–¹é¢çš„å‚æ•°å’Œè®¾ç½®çš„å·¥å…·ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f42980",
   "metadata": {},
   "source": [
    "##### åˆ—å‡ºæ”¯æŒçš„é…ç½®æ–‡ä»¶\n",
    "\n",
    "XTuner æä¾›å¤šä¸ªå¼€ç®±å³ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ã€‚\n",
    "\n",
    "> `xtuner list-cfg` å‘½ä»¤ç”¨äºåˆ—å‡ºå†…ç½®çš„æ‰€æœ‰é…ç½®æ–‡ä»¶ã€‚å‚æ•° `-p` æˆ– `--pattern` è¡¨ç¤ºæ¨¡å¼åŒ¹é…ï¼Œåé¢è·Ÿç€çš„å†…å®¹å°†ä¼šåœ¨æ‰€æœ‰çš„é…ç½®æ–‡ä»¶é‡Œè¿›è¡Œæ¨¡ç³ŠåŒ¹é…æœç´¢ï¼Œç„¶åè¿”å›æœ€æœ‰å¯èƒ½å¾—å†…å®¹ã€‚æ¯”å¦‚æˆ‘ä»¬è¿™é‡Œå¾®è°ƒçš„æ˜¯ä¹¦ç”ŸÂ·æµ¦è¯­çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥åŒ¹é…æœç´¢ `internlm2`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b865175",
   "metadata": {},
   "outputs": [],
   "source": [
    "!xtuner list-cfg -p internlm2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db749e93",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>é…ç½®æ–‡ä»¶åçš„è§£é‡Š</summary>\n",
    "\n",
    "ä»¥ **internlm2_1_8b_full_custom_pretrain_e1** å’Œ **internlm2_chat_1_8b_qlora_alpaca_e3** ä¸¾ä¾‹ï¼š\n",
    "\n",
    "| é…ç½®æ–‡ä»¶ internlm2_1_8b_full_custom_pretrain_e1 | é…ç½®æ–‡ä»¶ internlm2_chat_1_8b_qlora_alpaca_e3 | è¯´æ˜ |\n",
    "| -------- | -------- | ------------- |\n",
    "| internlm2_1_8b | internlm2_chat_1_8b | æ¨¡å‹åç§° |\n",
    "| full    | qlora    | ä½¿ç”¨çš„ç®—æ³•     |\n",
    "| custom_pretrain   | alpaca   | æ•°æ®é›†åç§°     |\n",
    "| e1       | e3       | æŠŠæ•°æ®é›†è·‘å‡ æ¬¡  |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab56350",
   "metadata": {},
   "source": [
    "##### å¤åˆ¶ä¸€ä¸ªé¢„è®¾çš„é…ç½®æ–‡ä»¶\n",
    "\n",
    "ç”±äºæˆ‘ä»¬æ˜¯å¯¹åŸºåº§æ¨¡å‹è¿›è¡Œå¢é‡é¢„è®­ç»ƒï¼Œæ‰€ä»¥ä¸æˆ‘ä»¬çš„éœ€æ±‚æœ€åŒ¹é…çš„é…ç½®æ–‡ä»¶æ˜¯ `internlm2_1_8b_full_custom_pretrain_e1`ï¼Œè¿™é‡Œå°±å¤åˆ¶è¯¥é…ç½®æ–‡ä»¶ã€‚\n",
    "\n",
    "> `xtuner copy-cfg` å‘½ä»¤ç”¨äºå¤åˆ¶ä¸€ä¸ªå†…ç½®çš„é…ç½®æ–‡ä»¶ã€‚è¯¥å‘½ä»¤éœ€è¦ä¸¤ä¸ªå‚æ•°ï¼š`CONFIG` ä»£è¡¨éœ€è¦å¤åˆ¶çš„é…ç½®æ–‡ä»¶åç§°ï¼Œ`SAVE_PATH` ä»£è¡¨å¤åˆ¶çš„ç›®æ ‡è·¯å¾„ã€‚åœ¨æˆ‘ä»¬çš„è¾“å…¥çš„è¿™ä¸ªå‘½ä»¤ä¸­ï¼Œæˆ‘ä»¬çš„ `CONFIG` å¯¹åº”çš„æ˜¯ä¸Šé¢æœç´¢åˆ°çš„ `internlm2_1_8b_full_custom_pretrain_e1` ,è€Œ `SAVE_PATH` åˆ™æ˜¯å½“å‰ç›®å½• `.`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfbdd74",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!xtuner copy-cfg internlm2_1_8b_full_custom_pretrain_e1 ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d52ee",
   "metadata": {},
   "source": [
    "å¤åˆ¶å¥½é…ç½®æ–‡ä»¶åï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚\n",
    "\n",
    "<details>\n",
    "<summary>ç›®å½•ç»“æ„</summary>\n",
    "\n",
    "```\n",
    "â”œâ”€â”€ Shanghai_AI_Laboratory\n",
    "â”‚   â”œâ”€â”€ internlm2-1_8b\n",
    "â”‚   â”‚   â”œâ”€â”€ README.md\n",
    "â”‚   â”‚   â”œâ”€â”€ config.json\n",
    "â”‚   â”‚   â”œâ”€â”€ configuration.json\n",
    "â”‚   â”‚   â”œâ”€â”€ configuration_internlm2.py\n",
    "â”‚   â”‚   â”œâ”€â”€ generation_config.json\n",
    "â”‚   â”‚   â”œâ”€â”€ modeling_internlm2.py\n",
    "â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin\n",
    "â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2.py\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2_fast.py\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenizer.json\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenizer.model\n",
    "â”‚   â”‚   â””â”€â”€ tokenizer_config.json\n",
    "â”‚   â””â”€â”€ internlm2-chat-1_8b -> /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b\n",
    "â”‚       â”œâ”€â”€ README.md\n",
    "â”‚       â”œâ”€â”€ config.json\n",
    "â”‚       â”œâ”€â”€ configuration.json\n",
    "â”‚       â”œâ”€â”€ configuration_internlm2.py\n",
    "â”‚       â”œâ”€â”€ generation_config.json\n",
    "â”‚       â”œâ”€â”€ model-00001-of-00002.safetensors\n",
    "â”‚       â”œâ”€â”€ model-00002-of-00002.safetensors\n",
    "â”‚       â”œâ”€â”€ model.safetensors.index.json\n",
    "â”‚       â”œâ”€â”€ modeling_internlm2.py\n",
    "â”‚       â”œâ”€â”€ special_tokens_map.json\n",
    "â”‚       â”œâ”€â”€ tokenization_internlm2.py\n",
    "â”‚       â”œâ”€â”€ tokenization_internlm2_fast.py\n",
    "â”‚       â”œâ”€â”€ tokenizer.model\n",
    "â”‚       â””â”€â”€ tokenizer_config.json\n",
    "â”œâ”€â”€ datas\n",
    "â”‚   â””â”€â”€ pretrain.json\n",
    "â”œâ”€â”€ internlm2_1_8b_full_custom_pretrain_e1_copy.py\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82985b1",
   "metadata": {},
   "source": [
    "##### å¯¹é…ç½®æ–‡ä»¶è¿›è¡Œä¿®æ”¹\n",
    "\n",
    "åœ¨é€‰æ‹©äº†ä¸€ä¸ªæœ€åŒ¹é…çš„é…ç½®æ–‡ä»¶å¹¶å‡†å¤‡å¥½å…¶ä»–å†…å®¹åï¼Œä¸‹é¢æˆ‘ä»¬è¦åšçš„äº‹æƒ…å°±æ˜¯æ ¹æ®æˆ‘ä»¬è‡ªå·±çš„å†…å®¹å¯¹è¯¥é…ç½®æ–‡ä»¶è¿›è¡Œè°ƒæ•´ï¼Œä½¿å…¶èƒ½å¤Ÿæ»¡è¶³æˆ‘ä»¬å®é™…è®­ç»ƒçš„è¦æ±‚ã€‚\n",
    "\n",
    "<details>\n",
    "<summary><b>é…ç½®æ–‡ä»¶ä»‹ç»</b></summary>\n",
    " \n",
    "æ‰“å¼€é…ç½®æ–‡ä»¶åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ•´ä½“çš„é…ç½®æ–‡ä»¶åˆ†ä¸ºäº”éƒ¨åˆ†ï¼š\n",
    "\n",
    "1. **PART 1 Settings**ï¼šæ¶µç›–äº†æ¨¡å‹åŸºæœ¬è®¾ç½®ï¼Œå¦‚é¢„è®­ç»ƒæ¨¡å‹çš„é€‰æ‹©ã€æ•°æ®é›†ä¿¡æ¯å’Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸€äº›åŸºæœ¬å‚æ•°ï¼ˆå¦‚æ‰¹å¤§å°ã€å­¦ä¹ ç‡ç­‰ï¼‰ã€‚\n",
    "\n",
    "2. **PART 2 Model & Tokenizer**ï¼šæŒ‡å®šäº†ç”¨äºè®­ç»ƒçš„æ¨¡å‹å’Œåˆ†è¯å™¨çš„å…·ä½“ç±»å‹åŠå…¶é…ç½®ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„å’Œæ˜¯å¦å¯ç”¨ç‰¹å®šåŠŸèƒ½ï¼ˆå¦‚å¯å˜é•¿åº¦æ³¨æ„åŠ›ï¼‰ï¼Œè¿™æ˜¯æ¨¡å‹è®­ç»ƒçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚\n",
    "\n",
    "3. **PART 3 Dataset & Dataloader**ï¼šæè¿°äº†æ•°æ®å¤„ç†çš„ç»†èŠ‚ï¼ŒåŒ…æ‹¬å¦‚ä½•åŠ è½½æ•°æ®é›†ã€é¢„å¤„ç†æ­¥éª¤ã€æ‰¹å¤„ç†å¤§å°ç­‰ï¼Œç¡®ä¿äº†æ¨¡å‹èƒ½å¤Ÿæ¥æ”¶åˆ°æ­£ç¡®æ ¼å¼å’Œè´¨é‡çš„æ•°æ®ã€‚\n",
    "\n",
    "4. **PART 4 Scheduler & Optimizer**ï¼šé…ç½®äº†ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„å…³é”®å‚æ•°ï¼Œå¦‚å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥å’Œä¼˜åŒ–å™¨çš„é€‰æ‹©ï¼Œè¿™äº›æ˜¯å½±å“æ¨¡å‹è®­ç»ƒæ•ˆæœå’Œé€Ÿåº¦çš„é‡è¦å› ç´ ã€‚\n",
    "\n",
    "5. **PART 5 Runtime**ï¼šå®šä¹‰äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„é¢å¤–è®¾ç½®ï¼Œå¦‚æ—¥å¿—è®°å½•ã€æ¨¡å‹ä¿å­˜ç­–ç•¥å’Œè‡ªå®šä¹‰é’©å­ç­‰ï¼Œä»¥æ”¯æŒè®­ç»ƒæµç¨‹çš„ç›‘æ§ã€è°ƒè¯•å’Œç»“æœçš„ä¿å­˜ã€‚\n",
    "\n",
    "ä¸€èˆ¬æ¥è¯´æˆ‘ä»¬éœ€è¦æ›´æ”¹çš„éƒ¨åˆ†å…¶å®åªåŒ…æ‹¬å‰ä¸‰éƒ¨åˆ†ï¼Œè€Œä¸”ä¿®æ”¹çš„ä¸»è¦åŸå› æ˜¯æˆ‘ä»¬ä¿®æ”¹äº†é…ç½®æ–‡ä»¶ä¸­è§„å®šçš„æ¨¡å‹ã€æ•°æ®é›†ã€‚åä¸¤éƒ¨åˆ†éƒ½æ˜¯ XTuner å®˜æ–¹å¸®æˆ‘ä»¬ä¼˜åŒ–å¥½çš„ä¸œè¥¿ï¼Œä¸€èˆ¬è€Œè¨€åªæœ‰åœ¨é­”æ”¹çš„æƒ…å†µä¸‹æ‰éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚\n",
    "\n",
    "</details>\n",
    "\n",
    "ä¸‹é¢æˆ‘ä»¬å°†æ ¹æ®é¡¹ç›®çš„éœ€æ±‚ä¸€æ­¥æ­¥çš„è¿›è¡Œä¿®æ”¹å’Œè°ƒæ•´å§ï¼\n",
    "\n",
    "åœ¨ PART 1 çš„éƒ¨åˆ†ï¼Œç”±äºæˆ‘ä»¬ä¸å†éœ€è¦åœ¨ HuggingFace ä¸Šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å…ˆè¦æ›´æ¢æ¨¡å‹çš„è·¯å¾„ä»¥åŠæ•°æ®é›†çš„è·¯å¾„ä¸ºæˆ‘ä»¬æœ¬åœ°çš„è·¯å¾„ã€‚\n",
    "\n",
    "ä¸ºäº†è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿå®æ—¶è§‚å¯Ÿåˆ°æ¨¡å‹çš„å˜åŒ–æƒ…å†µï¼ŒXTuner è´´å¿ƒçš„æ¨å‡ºäº†ä¸€ä¸ª `evaluation_inputs` çš„å‚æ•°æ¥è®©æˆ‘ä»¬èƒ½å¤Ÿè®¾ç½®å¤šä¸ªé—®é¢˜æ¥ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–æ˜¯æœç€æˆ‘ä»¬æƒ³è¦çš„æ–¹å‘å‰è¿›çš„ã€‚æˆ‘ä»¬å¯ä»¥æ·»åŠ è‡ªå·±çš„è¾“å…¥ã€‚\n",
    "\n",
    "åœ¨ PART 2 çš„éƒ¨åˆ†ï¼Œç”±äºæˆ‘ä»¬å¤åˆ¶çš„é…ç½®æ–‡ä»¶æ˜¯å…¨å‚æ•°å¾®è°ƒçš„é…ç½®ï¼Œè€Œæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ `QLoRA` ç®—æ³•è¿›è¡Œå¾®è°ƒï¼Œæ‰€ä»¥å¯ä»¥æ·»åŠ  `QLoRA` ç®—æ³•çš„é…ç½®ã€‚\n",
    "\n",
    "```diff\n",
    "+ from peft import LoraConfig\n",
    "\n",
    "+ import torch\n",
    "\n",
    "- from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "+ from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "#######################################################################\n",
    "#                          PART 1  Settings                           #\n",
    "#######################################################################\n",
    "- pretrained_model_name_or_path = 'internlm/internlm2-1_8b'\n",
    "+ pretrained_model_name_or_path = 'Shanghai_AI_Laboratory/internlm2-1_8b'\n",
    "\n",
    "- data_files = ['/path/to/json/file.json']\n",
    "+ data_files = ['datas/pretrain.json']\n",
    "\n",
    "- evaluation_inputs = ['ä¸Šæµ·æ˜¯', 'Shanghai is']\n",
    "+ evaluation_inputs = ['ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯', 'ä¸Šæµ·æ˜¯', 'Shanghai is']\n",
    "\n",
    "#######################################################################\n",
    "#                      PART 2  Model & Tokenizer                      #\n",
    "#######################################################################\n",
    "model = dict(\n",
    "    type=SupervisedFinetune,\n",
    "    use_varlen_attn=use_varlen_attn,\n",
    "    llm=dict(\n",
    "        type=AutoModelForCausalLM.from_pretrained,\n",
    "        pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "+       quantization_config=dict(\n",
    "+           type=BitsAndBytesConfig,\n",
    "+           load_in_4bit=True,\n",
    "+           load_in_8bit=False,\n",
    "+           llm_int8_threshold=6.0,\n",
    "+           llm_int8_has_fp16_weight=False,\n",
    "+           bnb_4bit_compute_dtype=torch.float16,\n",
    "+           bnb_4bit_use_double_quant=True,\n",
    "+           bnb_4bit_quant_type='nf4')\n",
    "    ),\n",
    "+   lora=dict(\n",
    "+       type=LoraConfig,\n",
    "+       r=64,\n",
    "+       lora_alpha=16,\n",
    "+       lora_dropout=0.1,\n",
    "+       bias='none',\n",
    "+       task_type='CAUSAL_LM')\n",
    ")\n",
    "```\n",
    "\n",
    "é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥å¯¹ä¸€äº›é‡è¦çš„å‚æ•°è¿›è¡Œè°ƒæ•´ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡ï¼ˆlrï¼‰ã€è®­ç»ƒçš„è½®æ•°ï¼ˆmax_epochsï¼‰ç­‰ç­‰ã€‚\n",
    "\n",
    "<details>\n",
    "<summary>å¸¸ç”¨å‚æ•°ä»‹ç»</summary>\n",
    "\n",
    "| å‚æ•°å                  | è§£é‡Š                                                     |\n",
    "| ----------------------- | -------------------------------------------------------- |\n",
    "| **data_path**           | æ•°æ®è·¯å¾„æˆ– HuggingFace ä»“åº“å                             |\n",
    "| **max_length**          | å•æ¡æ•°æ®æœ€å¤§ Token æ•°ï¼Œè¶…è¿‡åˆ™æˆªæ–­                         |\n",
    "| **pack_to_max_length**  | æ˜¯å¦å°†å¤šæ¡çŸ­æ•°æ®æ‹¼æ¥åˆ° max_lengthï¼Œæé«˜ GPU åˆ©ç”¨ç‡        |\n",
    "| **accumulative_counts** | æ¢¯åº¦ç´¯ç§¯ï¼Œæ¯å¤šå°‘æ¬¡ backward æ›´æ–°ä¸€æ¬¡å‚æ•°                  |\n",
    "| **sequence_parallel_size** | å¹¶è¡Œåºåˆ—å¤„ç†çš„å¤§å°ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒæ—¶çš„åºåˆ—å¹¶è¡Œ              |\n",
    "| **batch_size**          | æ¯ä¸ªè®¾å¤‡ä¸Šçš„æ‰¹é‡å¤§å°                                      |\n",
    "| **dataloader_num_workers** | æ•°æ®åŠ è½½å™¨ä¸­å·¥ä½œè¿›ç¨‹çš„æ•°é‡                                |\n",
    "| **max_epochs**          | è®­ç»ƒçš„æœ€å¤§è½®æ•°                                             |\n",
    "| **optim_type**          | ä¼˜åŒ–å™¨ç±»å‹ï¼Œä¾‹å¦‚ AdamW                                    |\n",
    "| **lr**                  | å­¦ä¹ ç‡                                                    |\n",
    "| **betas**               | ä¼˜åŒ–å™¨ä¸­çš„ beta å‚æ•°ï¼Œæ§åˆ¶åŠ¨é‡å’Œå¹³æ–¹æ¢¯åº¦çš„ç§»åŠ¨å¹³å‡         |\n",
    "| **weight_decay**        | æƒé‡è¡°å‡ç³»æ•°ï¼Œç”¨äºæ­£åˆ™åŒ–å’Œé¿å…è¿‡æ‹Ÿåˆ                      |\n",
    "| **max_norm**            | æ¢¯åº¦è£å‰ªçš„æœ€å¤§èŒƒæ•°ï¼Œç”¨äºé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸                      |\n",
    "| **warmup_ratio**        | é¢„çƒ­çš„æ¯”ä¾‹ï¼Œå­¦ä¹ ç‡åœ¨è¿™ä¸ªæ¯”ä¾‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­çº¿æ€§å¢åŠ åˆ°åˆå§‹å­¦ä¹ ç‡ |\n",
    "| **save_steps**          | ä¿å­˜æ¨¡å‹çš„æ­¥æ•°é—´éš”                                         |\n",
    "| **save_total_limit**    | ä¿å­˜çš„æ¨¡å‹æ€»æ•°é™åˆ¶ï¼Œè¶…è¿‡é™åˆ¶æ—¶åˆ é™¤æ—§çš„æ¨¡å‹æ–‡ä»¶             |\n",
    "| **prompt_template**     | æ¨¡æ¿æç¤ºï¼Œç”¨äºå®šä¹‰ç”Ÿæˆæ–‡æœ¬çš„æ ¼å¼æˆ–ç»“æ„                    |\n",
    "| ...... | ...... |\n",
    "\n",
    "> å¦‚æœæƒ³å……åˆ†åˆ©ç”¨æ˜¾å¡èµ„æºï¼Œå¯ä»¥å°† `max_length` å’Œ `batch_size` è¿™ä¸¤ä¸ªå‚æ•°è°ƒå¤§ã€‚\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8473fc9f",
   "metadata": {},
   "source": [
    "ä¿®æ”¹å®Œåçš„å®Œæ•´çš„é…ç½®æ–‡ä»¶æ˜¯ï¼š\n",
    "\n",
    "<details>\n",
    "<summary>internlm2_1_8b_full_custom_pretrain_e1_copy.py</summary>\n",
    "\n",
    "```python\n",
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "\"\"\"Data format:\n",
    "\n",
    "[\n",
    "  {\n",
    "      \"text\": \"xxx\"\n",
    "  },\n",
    "  {\n",
    "      \"text\": \"xxx\"\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "\"\"\"  # noqa: E501\n",
    "\n",
    "from datasets import load_dataset\n",
    "from mmengine.dataset import DefaultSampler\n",
    "from mmengine.hooks import (CheckpointHook, DistSamplerSeedHook, IterTimerHook,\n",
    "                            LoggerHook, ParamSchedulerHook)\n",
    "from mmengine.optim import AmpOptimWrapper, CosineAnnealingLR, LinearLR\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from xtuner.dataset import process_hf_dataset\n",
    "from xtuner.dataset.collate_fns import default_collate_fn\n",
    "from xtuner.dataset.map_fns import pretrain_map_fn\n",
    "from xtuner.engine.hooks import (DatasetInfoHook, EvaluateChatHook,\n",
    "                                 VarlenAttnArgsToMessageHubHook)\n",
    "from xtuner.engine.runner import TrainLoop\n",
    "from xtuner.model import SupervisedFinetune\n",
    "\n",
    "#######################################################################\n",
    "#                          PART 1  Settings                           #\n",
    "#######################################################################\n",
    "# Model\n",
    "pretrained_model_name_or_path = 'Shanghai_AI_Laboratory/internlm2-1_8b'\n",
    "use_varlen_attn = False\n",
    "\n",
    "# Data\n",
    "data_files = ['datas/pretrain.json']\n",
    "max_length = 2048\n",
    "pack_to_max_length = True\n",
    "\n",
    "# Scheduler & Optimizer\n",
    "batch_size = 1  # per_device\n",
    "accumulative_counts = 16  # bs = 1 GPU * 1 batch_size_per_device * 16 acc\n",
    "dataloader_num_workers = 0\n",
    "max_epochs = 1\n",
    "optim_type = AdamW\n",
    "lr = 2e-5\n",
    "betas = (0.9, 0.999)\n",
    "weight_decay = 0\n",
    "max_norm = 1  # grad clip\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Save\n",
    "save_steps = 500\n",
    "save_total_limit = 2  # Maximum checkpoints to keep (-1 means unlimited)\n",
    "\n",
    "# Evaluate the generation performance during the training\n",
    "evaluation_freq = 500\n",
    "SYSTEM = ''\n",
    "evaluation_inputs = ['ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯', 'ä¸Šæµ·æ˜¯', 'Shanghai is']\n",
    "\n",
    "#######################################################################\n",
    "#                      PART 2  Model & Tokenizer                      #\n",
    "#######################################################################\n",
    "tokenizer = dict(\n",
    "    type=AutoTokenizer.from_pretrained,\n",
    "    pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "    trust_remote_code=True,\n",
    "    padding_side='right')\n",
    "\n",
    "model = dict(\n",
    "    type=SupervisedFinetune,\n",
    "    use_varlen_attn=use_varlen_attn,\n",
    "    llm=dict(\n",
    "        type=AutoModelForCausalLM.from_pretrained,\n",
    "        pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=dict(\n",
    "            type=BitsAndBytesConfig,\n",
    "            load_in_4bit=True,\n",
    "            load_in_8bit=False,\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type='nf4')\n",
    "    ),\n",
    "    lora=dict(\n",
    "        type=LoraConfig,\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias='none',\n",
    "        task_type='CAUSAL_LM')\n",
    ")\n",
    "\n",
    "#######################################################################\n",
    "#                      PART 3  Dataset & Dataloader                   #\n",
    "#######################################################################\n",
    "train_dataset = dict(\n",
    "    type=process_hf_dataset,\n",
    "    dataset=dict(type=load_dataset, path='json', data_files=data_files),\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    dataset_map_fn=pretrain_map_fn,\n",
    "    template_map_fn=None,\n",
    "    remove_unused_columns=True,\n",
    "    shuffle_before_pack=False,\n",
    "    pack_to_max_length=pack_to_max_length,\n",
    "    use_varlen_attn=use_varlen_attn)\n",
    "\n",
    "train_dataloader = dict(\n",
    "    batch_size=batch_size,\n",
    "    num_workers=dataloader_num_workers,\n",
    "    dataset=train_dataset,\n",
    "    sampler=dict(type=DefaultSampler, shuffle=True),\n",
    "    collate_fn=dict(type=default_collate_fn, use_varlen_attn=use_varlen_attn))\n",
    "\n",
    "#######################################################################\n",
    "#                    PART 4  Scheduler & Optimizer                    #\n",
    "#######################################################################\n",
    "# optimizer\n",
    "optim_wrapper = dict(\n",
    "    type=AmpOptimWrapper,\n",
    "    optimizer=dict(\n",
    "        type=optim_type, lr=lr, betas=betas, weight_decay=weight_decay),\n",
    "    clip_grad=dict(max_norm=max_norm, error_if_nonfinite=False),\n",
    "    accumulative_counts=accumulative_counts,\n",
    "    loss_scale='dynamic',\n",
    "    dtype='float16')\n",
    "\n",
    "# learning policy\n",
    "# More information: https://github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/param_scheduler.md  # noqa: E501\n",
    "param_scheduler = [\n",
    "    dict(\n",
    "        type=LinearLR,\n",
    "        start_factor=1e-5,\n",
    "        by_epoch=True,\n",
    "        begin=0,\n",
    "        end=warmup_ratio * max_epochs,\n",
    "        convert_to_iter_based=True),\n",
    "    dict(\n",
    "        type=CosineAnnealingLR,\n",
    "        eta_min=0.0,\n",
    "        by_epoch=True,\n",
    "        begin=warmup_ratio * max_epochs,\n",
    "        end=max_epochs,\n",
    "        convert_to_iter_based=True)\n",
    "]\n",
    "\n",
    "# train, val, test setting\n",
    "train_cfg = dict(type=TrainLoop, max_epochs=max_epochs)\n",
    "\n",
    "#######################################################################\n",
    "#                           PART 5  Runtime                           #\n",
    "#######################################################################\n",
    "# Log the dialogue periodically during the training process, optional\n",
    "custom_hooks = [\n",
    "    dict(type=DatasetInfoHook, tokenizer=tokenizer),\n",
    "    dict(\n",
    "        type=EvaluateChatHook,\n",
    "        tokenizer=tokenizer,\n",
    "        every_n_iters=evaluation_freq,\n",
    "        evaluation_inputs=evaluation_inputs,\n",
    "        system=SYSTEM)\n",
    "]\n",
    "\n",
    "if use_varlen_attn:\n",
    "    custom_hooks += [dict(type=VarlenAttnArgsToMessageHubHook)]\n",
    "\n",
    "# configure default hooks\n",
    "default_hooks = dict(\n",
    "    # record the time of every iteration.\n",
    "    timer=dict(type=IterTimerHook),\n",
    "    # print log every 10 iterations.\n",
    "    logger=dict(type=LoggerHook, log_metric_by_epoch=False, interval=10),\n",
    "    # enable the parameter scheduler.\n",
    "    param_scheduler=dict(type=ParamSchedulerHook),\n",
    "    # save checkpoint per `save_steps`.\n",
    "    checkpoint=dict(\n",
    "        type=CheckpointHook,\n",
    "        by_epoch=False,\n",
    "        interval=save_steps,\n",
    "        max_keep_ckpts=save_total_limit),\n",
    "    # set sampler seed in distributed evrionment.\n",
    "    sampler_seed=dict(type=DistSamplerSeedHook),\n",
    ")\n",
    "\n",
    "# configure environment\n",
    "env_cfg = dict(\n",
    "    # whether to enable cudnn benchmark\n",
    "    cudnn_benchmark=False,\n",
    "    # set multi process parameters\n",
    "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n",
    "    # set distributed parameters\n",
    "    dist_cfg=dict(backend='nccl'),\n",
    ")\n",
    "\n",
    "# set visualizer\n",
    "visualizer = None\n",
    "\n",
    "# set log level\n",
    "log_level = 'INFO'\n",
    "\n",
    "# load from which checkpoint\n",
    "load_from = None\n",
    "\n",
    "# whether to resume training from the loaded checkpoint\n",
    "resume = False\n",
    "\n",
    "# Defaults to use random seed and disable `deterministic`\n",
    "randomness = dict(seed=None, deterministic=False)\n",
    "\n",
    "# set log processor\n",
    "log_processor = dict(by_epoch=False)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be808d46",
   "metadata": {},
   "source": [
    "#### å¯åŠ¨å¾®è°ƒ\n",
    "\n",
    "å®Œæˆäº†æ‰€æœ‰çš„å‡†å¤‡å·¥ä½œåï¼Œæˆ‘ä»¬å°±å¯ä»¥æ­£å¼çš„å¼€å§‹æˆ‘ä»¬ä¸‹ä¸€é˜¶æ®µçš„æ—…ç¨‹ï¼šXTuner å¯åŠ¨~ï¼\n",
    "\n",
    "å½“æˆ‘ä»¬å‡†å¤‡å¥½äº†æ‰€æœ‰å†…å®¹ï¼Œæˆ‘ä»¬åªéœ€è¦å°†ä½¿ç”¨ `xtuner train` å‘½ä»¤ä»¤å³å¯å¼€å§‹è®­ç»ƒã€‚\n",
    "\n",
    "> `xtuner train` å‘½ä»¤ç”¨äºå¯åŠ¨æ¨¡å‹å¾®è°ƒè¿›ç¨‹ã€‚è¯¥å‘½ä»¤éœ€è¦ä¸€ä¸ªå‚æ•°ï¼š`CONFIG` ç”¨äºæŒ‡å®šå¾®è°ƒé…ç½®æ–‡ä»¶ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¿®æ”¹å¥½çš„é…ç½®æ–‡ä»¶ `internlm2_1_8b_full_custom_pretrain_e1_copy.py`ã€‚  \n",
    "> è®­ç»ƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„æ‰€æœ‰æ–‡ä»¶ï¼ŒåŒ…æ‹¬æ—¥å¿—ã€é…ç½®æ–‡ä»¶ã€æ£€æŸ¥ç‚¹æ–‡ä»¶ã€å¾®è°ƒåçš„æ¨¡å‹ç­‰ï¼Œé»˜è®¤ä¿å­˜åœ¨ `work_dirs` ç›®å½•ä¸‹ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡æ·»åŠ  `--work-dir` æŒ‡å®šç‰¹å®šçš„æ–‡ä»¶ä¿å­˜ä½ç½®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac77d2d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!xtuner train ./internlm2_1_8b_full_custom_pretrain_e1_copy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74cdeb",
   "metadata": {},
   "source": [
    "åœ¨è®­ç»ƒå®Œåï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚\n",
    "\n",
    "<details>\n",
    "<summary>ç›®å½•ç»“æ„</summary>\n",
    "\n",
    "```\n",
    "â”œâ”€â”€ work_dirs\n",
    "â”‚   â””â”€â”€ internlm2_1_8b_full_custom_pretrain_e1_copy\n",
    "â”‚       â”œâ”€â”€ 20240626_214522\n",
    "â”‚       â”‚   â”œâ”€â”€ 20240626_214522.log\n",
    "â”‚       â”‚   â””â”€â”€ vis_data\n",
    "â”‚       â”‚       â”œâ”€â”€ 20240626_214522.json\n",
    "â”‚       â”‚       â”œâ”€â”€ config.py\n",
    "â”‚       â”‚       â”œâ”€â”€ eval_outputs_iter_1499.txt\n",
    "â”‚       â”‚       â”œâ”€â”€ eval_outputs_iter_1999.txt\n",
    "â”‚       â”‚       â”œâ”€â”€ eval_outputs_iter_2499.txt\n",
    "â”‚       â”‚       â”œâ”€â”€ eval_outputs_iter_2623.txt\n",
    "â”‚       â”‚       â”œâ”€â”€ eval_outputs_iter_499.txt\n",
    "â”‚       â”‚       â”œâ”€â”€ eval_outputs_iter_999.txt\n",
    "â”‚       â”‚       â””â”€â”€ scalars.json\n",
    "â”‚       â”œâ”€â”€ internlm2_1_8b_full_custom_pretrain_e1_copy.py\n",
    "â”‚       â”œâ”€â”€ iter_2500.pth\n",
    "â”‚       â”œâ”€â”€ iter_2624.pth\n",
    "â”‚       â””â”€â”€ last_checkpoint\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2672054f",
   "metadata": {},
   "source": [
    "#### æ¨¡å‹æ ¼å¼è½¬æ¢\n",
    "\n",
    "æ¨¡å‹è½¬æ¢çš„æœ¬è´¨å…¶å®å°±æ˜¯å°†åŸæœ¬ä½¿ç”¨ Pytorch è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æƒé‡æ–‡ä»¶è½¬æ¢ä¸ºç›®å‰é€šç”¨çš„ HuggingFace æ ¼å¼æ–‡ä»¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¥å®ç°ä¸€é”®è½¬æ¢ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df87d0d",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `xtuner convert pth_to_hf` å‘½ä»¤æ¥è¿›è¡Œæ¨¡å‹æ ¼å¼è½¬æ¢ã€‚\n",
    "\n",
    "> `xtuner convert pth_to_hf` å‘½ä»¤ç”¨äºè¿›è¡Œæ¨¡å‹æ ¼å¼è½¬æ¢ã€‚è¯¥å‘½ä»¤éœ€è¦ä¸‰ä¸ªå‚æ•°ï¼š`CONFIG` è¡¨ç¤ºå¾®è°ƒçš„é…ç½®æ–‡ä»¶ï¼Œ `PATH_TO_PTH_MODEL` è¡¨ç¤ºå¾®è°ƒçš„æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„ï¼Œå³è¦è½¬æ¢çš„æ¨¡å‹æƒé‡ï¼Œ `SAVE_PATH_TO_HF_MODEL` è¡¨ç¤ºè½¬æ¢åçš„ HuggingFace æ ¼å¼æ–‡ä»¶çš„ä¿å­˜è·¯å¾„ã€‚\n",
    "\n",
    "é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬å…¶å®è¿˜å¯ä»¥åœ¨è½¬æ¢çš„å‘½ä»¤ä¸­æ·»åŠ å‡ ä¸ªé¢å¤–çš„å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š\n",
    "| å‚æ•°å | è§£é‡Š |\n",
    "| ------------------- | ------------------------------------------------------ |\n",
    "| --fp32     | ä»£è¡¨ä»¥fp32çš„ç²¾åº¦å¼€å¯ï¼Œå‡å¦‚ä¸è¾“å…¥åˆ™é»˜è®¤ä¸ºfp16                          |\n",
    "| --max-shard-size {GB}        | ä»£è¡¨æ¯ä¸ªæƒé‡æ–‡ä»¶æœ€å¤§çš„å¤§å°ï¼ˆé»˜è®¤ä¸º2GBï¼‰                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82570d4e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pth_file=`ls -t ./work_dirs/internlm2_1_8b_full_custom_pretrain_e1_copy/*.pth | head -n 1` && MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU xtuner convert pth_to_hf ./internlm2_1_8b_full_custom_pretrain_e1_copy.py ${pth_file} ./hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a055caa3",
   "metadata": {},
   "source": [
    "æ¨¡å‹æ ¼å¼è½¬æ¢å®Œæˆåï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚\n",
    "\n",
    "<details>\n",
    "<summary>ç›®å½•ç»“æ„</summary>\n",
    "\n",
    "```\n",
    "â”œâ”€â”€ hf\n",
    "â”‚   â”œâ”€â”€ README.md\n",
    "â”‚   â”œâ”€â”€ adapter_config.json\n",
    "â”‚   â”œâ”€â”€ adapter_model.bin\n",
    "â”‚   â””â”€â”€ xtuner_config.py\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "è½¬æ¢å®Œæˆåï¼Œå¯ä»¥çœ‹åˆ°æ¨¡å‹è¢«è½¬æ¢ä¸º HuggingFace ä¸­å¸¸ç”¨çš„ .bin æ ¼å¼æ–‡ä»¶ï¼Œè¿™å°±ä»£è¡¨ç€æ–‡ä»¶æˆåŠŸè¢«è½¬åŒ–ä¸º HuggingFace æ ¼å¼äº†ã€‚\n",
    "\n",
    "æ­¤æ—¶ï¼Œhf æ–‡ä»¶å¤¹å³ä¸ºæˆ‘ä»¬å¹³æ—¶æ‰€ç†è§£çš„æ‰€è°“ â€œLoRA æ¨¡å‹æ–‡ä»¶â€\n",
    "\n",
    "> å¯ä»¥ç®€å•ç†è§£ï¼šLoRA æ¨¡å‹æ–‡ä»¶ = Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e8a04",
   "metadata": {},
   "source": [
    "#### æ¨¡å‹åˆå¹¶\n",
    "\n",
    "å¯¹äº LoRA æˆ–è€… QLoRA å¾®è°ƒå‡ºæ¥çš„æ¨¡å‹å…¶å®å¹¶ä¸æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹ï¼Œè€Œæ˜¯ä¸€ä¸ªé¢å¤–çš„å±‚ï¼ˆAdapterï¼‰ï¼Œè®­ç»ƒå®Œçš„è¿™ä¸ªå±‚æœ€ç»ˆè¿˜æ˜¯è¦ä¸åŸæ¨¡å‹è¿›è¡Œåˆå¹¶æ‰èƒ½è¢«æ­£å¸¸çš„ä½¿ç”¨ã€‚\n",
    "\n",
    "> å¯¹äºå…¨é‡å¾®è°ƒçš„æ¨¡å‹ï¼ˆfullï¼‰å…¶å®æ˜¯ä¸éœ€è¦è¿›è¡Œæ•´åˆè¿™ä¸€æ­¥çš„ï¼Œå› ä¸ºå…¨é‡å¾®è°ƒä¿®æ”¹çš„æ˜¯åŸæ¨¡å‹çš„æƒé‡è€Œéå¾®è°ƒä¸€ä¸ªæ–°çš„ Adapter ï¼Œå› æ­¤æ˜¯ä¸éœ€è¦è¿›è¡Œæ¨¡å‹æ•´åˆçš„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35947908",
   "metadata": {},
   "source": [
    "åœ¨ XTuner ä¸­æä¾›äº†ä¸€é”®åˆå¹¶çš„å‘½ä»¤ `xtuner convert merge`ï¼Œåœ¨ä½¿ç”¨å‰æˆ‘ä»¬éœ€è¦å‡†å¤‡å¥½ä¸‰ä¸ªè·¯å¾„ï¼ŒåŒ…æ‹¬åŸæ¨¡å‹çš„è·¯å¾„ã€è®­ç»ƒå¥½çš„ Adapter å±‚çš„ï¼ˆæ¨¡å‹æ ¼å¼è½¬æ¢åçš„ï¼‰è·¯å¾„ä»¥åŠæœ€ç»ˆä¿å­˜çš„è·¯å¾„ã€‚\n",
    "\n",
    "> `xtuner convert merge`å‘½ä»¤ç”¨äºåˆå¹¶æ¨¡å‹ã€‚è¯¥å‘½ä»¤éœ€è¦ä¸‰ä¸ªå‚æ•°ï¼š`LLM` è¡¨ç¤ºåŸæ¨¡å‹è·¯å¾„ï¼Œ`ADAPTER` è¡¨ç¤º Adapter å±‚çš„è·¯å¾„ï¼Œ `SAVE_PATH` è¡¨ç¤ºåˆå¹¶åçš„æ¨¡å‹æœ€ç»ˆçš„ä¿å­˜è·¯å¾„ã€‚\n",
    "\n",
    "åœ¨æ¨¡å‹åˆå¹¶è¿™ä¸€æ­¥è¿˜æœ‰å…¶ä»–å¾ˆå¤šçš„å¯é€‰å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š\n",
    "| å‚æ•°å | è§£é‡Š |\n",
    "| ------------------- | ------------------------------------------------------ |\n",
    "| --max-shard-size {GB} | ä»£è¡¨æ¯ä¸ªæƒé‡æ–‡ä»¶æœ€å¤§çš„å¤§å°ï¼ˆé»˜è®¤ä¸º2GBï¼‰                |\n",
    "| --device {device_name} | è¿™é‡ŒæŒ‡çš„å°±æ˜¯deviceçš„åç§°ï¼Œå¯é€‰æ‹©çš„æœ‰cudaã€cpuå’Œautoï¼Œé»˜è®¤ä¸ºcudaå³ä½¿ç”¨gpuè¿›è¡Œè¿ç®— |\n",
    "| --is-clip | è¿™ä¸ªå‚æ•°ä¸»è¦ç”¨äºç¡®å®šæ¨¡å‹æ˜¯ä¸æ˜¯CLIPæ¨¡å‹ï¼Œå‡å¦‚æ˜¯çš„è¯å°±è¦åŠ ä¸Šï¼Œä¸æ˜¯å°±ä¸éœ€è¦æ·»åŠ  |\n",
    "\n",
    "> CLIPï¼ˆContrastive Languageâ€“Image Pre-trainingï¼‰æ¨¡å‹æ˜¯ OpenAI å¼€å‘çš„ä¸€ç§é¢„è®­ç»ƒæ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿç†è§£å›¾åƒå’Œæè¿°å®ƒä»¬çš„æ–‡æœ¬ä¹‹é—´çš„å…³ç³»ã€‚CLIP é€šè¿‡åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šå­¦ä¹ å›¾åƒå’Œå¯¹åº”æ–‡æœ¬ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œä»è€Œå®ç°äº†å¯¹å›¾åƒå†…å®¹çš„ç†è§£å’Œåˆ†ç±»ï¼Œç”šè‡³èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad447926",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU xtuner convert merge Shanghai_AI_Laboratory/internlm2-1_8b ./hf ./merged --max-shard-size 2GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c167d",
   "metadata": {},
   "source": [
    "æ¨¡å‹åˆå¹¶å®Œæˆåï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚\n",
    "\n",
    "<details>\n",
    "<summary>ç›®å½•ç»“æ„</summary>\n",
    "\n",
    "```\n",
    "â”œâ”€â”€ merged\n",
    "â”‚   â”œâ”€â”€ config.json\n",
    "â”‚   â”œâ”€â”€ configuration_internlm2.py\n",
    "â”‚   â”œâ”€â”€ generation_config.json\n",
    "â”‚   â”œâ”€â”€ modeling_internlm2.py\n",
    "â”‚   â”œâ”€â”€ pytorch_model-00001-of-00002.bin\n",
    "â”‚   â”œâ”€â”€ pytorch_model-00002-of-00002.bin\n",
    "â”‚   â”œâ”€â”€ pytorch_model.bin.index.json\n",
    "â”‚   â”œâ”€â”€ special_tokens_map.json\n",
    "â”‚   â”œâ”€â”€ tokenization_internlm2.py\n",
    "â”‚   â”œâ”€â”€ tokenization_internlm2_fast.py\n",
    "â”‚   â”œâ”€â”€ tokenizer.json\n",
    "â”‚   â”œâ”€â”€ tokenizer.model\n",
    "â”‚   â””â”€â”€ tokenizer_config.json\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "åœ¨æ¨¡å‹åˆå¹¶å®Œæˆåï¼Œæˆ‘ä»¬å°±å¯ä»¥çœ‹åˆ°æœ€ç»ˆçš„æ¨¡å‹å’ŒåŸæ¨¡å‹æ–‡ä»¶å¤¹éå¸¸ç›¸ä¼¼ï¼ŒåŒ…æ‹¬äº†åˆ†è¯å™¨ã€æƒé‡æ–‡ä»¶ã€é…ç½®ä¿¡æ¯ç­‰ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ae7b2",
   "metadata": {},
   "source": [
    "### ç›®æ ‡æ¨¡å‹æ¨ç†\n",
    "\n",
    "å½“æˆ‘ä»¬åˆå¹¶å®Œæˆåï¼Œæˆ‘ä»¬å°±èƒ½å¤Ÿæ­£å¸¸çš„è°ƒç”¨è¿™ä¸ªæ¨¡å‹è¿›è¡Œæ¨ç†äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febaa17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model(\"./merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced1ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\"ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\"æˆéƒ½æ˜¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4314bc",
   "metadata": {},
   "source": [
    "å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡å¢é‡é¢„è®­ç»ƒï¼Œç¡®å®åœ¨åŸºåº§æ¨¡å‹çš„åŸºç¡€ä¸Šå­¦ä¹ åˆ°äº†æ–°çš„çŸ¥è¯†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1485e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenizer, model\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b556d-4012-4860-a342-8c9545616bd0",
   "metadata": {},
   "source": [
    "## æŒ‡ä»¤è·Ÿéšå¾®è°ƒï¼ˆå¾®è°ƒä¸ªäººå°åŠ©æ‰‹è®¤çŸ¥ï¼‰\n",
    "\n",
    "æŒæ¡äº†å¢é‡é¢„è®­ç»ƒå¾®è°ƒåï¼Œå†æ¥è¿›è¡ŒæŒ‡ä»¤è·Ÿéšå¾®è°ƒå°±æ¯”è¾ƒç®€å•äº†ã€‚è¿™é‡Œæˆ‘ä»¬ç”¨ `internlm2-chat-1_8b` æ¨¡å‹ï¼Œé€šè¿‡ `QLoRA` çš„æ–¹å¼æ¥å¾®è°ƒä¸€ä¸ªè‡ªå·±çš„å°åŠ©æ‰‹è®¤çŸ¥ä½œä¸ºæ¡ˆä¾‹æ¥è¿›è¡Œæ¼”ç¤ºã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e247e8e",
   "metadata": {},
   "source": [
    "å…ˆçœ‹çœ‹å¾®è°ƒæ•ˆæœï¼š\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td></td><td width=\"48%\">å¾®è°ƒå‰</td><td width=\"48%\">å¾®è°ƒå</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>è¾“å…¥</td><td>è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±</td><td>è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>è¾“å‡º</td><td>ä½ å¥½ï¼Œæˆ‘æ˜¯ä¹¦ç”ŸÂ·æµ¦è¯­ã€‚æˆ‘è‡´åŠ›äºå¸®åŠ©ç”¨æˆ·è§£å†³å„ç§è¯­è¨€ç›¸å…³çš„é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºè¯­è¨€å­¦ä¹ ã€ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ç­‰ã€‚æˆ‘ä½¿ç”¨äº†Transformeræ¨¡å‹å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œå¹¶ä½¿ç”¨äº†è¯­è¨€æ¨¡å‹ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œæ¬¢è¿éšæ—¶å‘æˆ‘æé—®ã€‚</td><td>æˆ‘æ˜¯ä¼é²œåŒå¿—çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„1.8Bå¤§æ¨¡å‹å“¦</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ç½‘é¡µ</td><td><img src=\"images/image-11.png\"/></td><td><img src=\"images/image-12.png\"/></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e73522",
   "metadata": {},
   "source": [
    "- å¯¼å…¥å¿…è¦çš„åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac38de7",
   "metadata": {},
   "source": [
    "- å®šä¹‰æ¨¡å‹åŠ è½½æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89372375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, trust_remote_code=True).cuda()\n",
    "    model = model.eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c202f10f",
   "metadata": {},
   "source": [
    "- å®šä¹‰å¯¹è¯æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd93c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "\n",
    "def chat(input_text):\n",
    "    length = 0\n",
    "    for response, _ in model.stream_chat(tokenizer, input_text, messages):\n",
    "        if response is not None:\n",
    "            print(response[length:], flush=True, end=\"\")\n",
    "            length = len(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71689ca",
   "metadata": {},
   "source": [
    "### å¾®è°ƒå‰çš„æ¨¡å‹å¯¹è¯\n",
    "\n",
    "é¦–å…ˆæ¥çœ‹çœ‹ `internlm2-chat-1_8b` çš„å¯¹è¯æ¼”ç¤ºã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec582d90",
   "metadata": {},
   "source": [
    "- æ¨¡å‹åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a69d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model(\"Shanghai_AI_Laboratory/internlm2-chat-1_8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041fcc4",
   "metadata": {},
   "source": [
    "- å¯¹è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e60c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469eda9",
   "metadata": {},
   "source": [
    "- é‡Šæ”¾ç¼“å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21380207",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenizer, model\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fe137d",
   "metadata": {},
   "source": [
    "### æŒ‡ä»¤è·Ÿéšå¾®è°ƒ\n",
    "\n",
    "ä¸‹é¢æˆ‘ä»¬å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè®©æ¨¡å‹è®¤è¯†åˆ°è‡ªå·±çš„å¼Ÿä½ï¼Œäº†è§£å®ƒè‡ªå·±æ˜¯ä½ çš„ä¸€ä¸ªåŠ©æ‰‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697b5614",
   "metadata": {},
   "source": [
    "#### å‡†æ•°æ®æ–‡ä»¶\n",
    "\n",
    "ä¸ºäº†è®©æ¨¡å‹èƒ½å¤Ÿè®¤æ¸…è‡ªå·±çš„èº«ä»½å¼Ÿä½ï¼Œåœ¨è¯¢é—®è‡ªå·±æ˜¯è°çš„æ—¶å€™æŒ‰ç…§æˆ‘ä»¬é¢„æœŸçš„ç»“æœè¿›è¡Œå›å¤ï¼Œæˆ‘ä»¬å°±éœ€è¦é€šè¿‡åœ¨å¾®è°ƒæ•°æ®é›†ä¸­å¤§é‡åŠ å…¥è¿™æ ·çš„æ•°æ®ã€‚æˆ‘ä»¬å‡†å¤‡ä¸€ä¸ªæ•°æ®é›†æ–‡ä»¶`datas/assistant.json`ï¼Œæ–‡ä»¶å†…å®¹ä¸ºå¯¹è¯æ•°æ®ã€‚ä¸ºäº†å¢å¼ºå¾®è°ƒæ•ˆæœï¼Œå¯ä»¥å°†å¯¹è¯æ•°æ®å¤åˆ¶å¤šæ¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a30477",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {\"conversation\": [{\"input\": \"è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\", \"output\": \"æˆ‘æ˜¯ä¼é²œåŒå¿—çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„1.8Bå¤§æ¨¡å‹å“¦\"}]},\n",
    "    {\"conversation\": [{\"input\": \"ä½ åœ¨å®æˆ˜è¥åšä»€ä¹ˆ\", \"output\": \"æˆ‘åœ¨è¿™é‡Œå¸®åŠ©ä¼é²œåŒå¿—å®ŒæˆXTunerå¾®è°ƒä¸ªäººå°åŠ©æ‰‹çš„ä»»åŠ¡\"}]},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012f0d4",
   "metadata": {},
   "source": [
    "å‡†å¤‡å¥½æ•°æ®æ–‡ä»¶åï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚\n",
    "\n",
    "<details>\n",
    "<summary>ç›®å½•ç»“æ„</summary>\n",
    "\n",
    "```\n",
    "â”œâ”€â”€ Shanghai_AI_Laboratory\n",
    "â”‚   â”œâ”€â”€ internlm2-1_8b\n",
    "â”‚   â”‚   â”œâ”€â”€ README.md\n",
    "â”‚   â”‚   â”œâ”€â”€ config.json\n",
    "â”‚   â”‚   â”œâ”€â”€ configuration.json\n",
    "â”‚   â”‚   â”œâ”€â”€ configuration_internlm2.py\n",
    "â”‚   â”‚   â”œâ”€â”€ generation_config.json\n",
    "â”‚   â”‚   â”œâ”€â”€ modeling_internlm2.py\n",
    "â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin\n",
    "â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2.py\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2_fast.py\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenizer.json\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenizer.model\n",
    "â”‚   â”‚   â””â”€â”€ tokenizer_config.json\n",
    "â”‚   â””â”€â”€ internlm2-chat-1_8b -> /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b\n",
    "â”‚       â”œâ”€â”€ README.md\n",
    "â”‚       â”œâ”€â”€ config.json\n",
    "â”‚       â”œâ”€â”€ configuration.json\n",
    "â”‚       â”œâ”€â”€ configuration_internlm2.py\n",
    "â”‚       â”œâ”€â”€ generation_config.json\n",
    "â”‚       â”œâ”€â”€ model-00001-of-00002.safetensors\n",
    "â”‚       â”œâ”€â”€ model-00002-of-00002.safetensors\n",
    "â”‚       â”œâ”€â”€ model.safetensors.index.json\n",
    "â”‚       â”œâ”€â”€ modeling_internlm2.py\n",
    "â”‚       â”œâ”€â”€ special_tokens_map.json\n",
    "â”‚       â”œâ”€â”€ tokenization_internlm2.py\n",
    "â”‚       â”œâ”€â”€ tokenization_internlm2_fast.py\n",
    "â”‚       â”œâ”€â”€ tokenizer.model\n",
    "â”‚       â””â”€â”€ tokenizer_config.json\n",
    "â”œâ”€â”€ datas\n",
    "â”‚   â”œâ”€â”€ assistant.json\n",
    "â”‚   â””â”€â”€ pretrain.json\n",
    "â”œâ”€â”€ internlm2_1_8b_full_custom_pretrain_e1_copy.py\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e570d88",
   "metadata": {},
   "source": [
    "#### å‡†å¤‡é…ç½®æ–‡ä»¶\n",
    "\n",
    "åœ¨å‡†å¤‡å¥½äº†æ¨¡å‹å’Œæ•°æ®é›†åï¼Œæˆ‘ä»¬å°±è¦æ ¹æ®æˆ‘ä»¬é€‰æ‹©çš„å¾®è°ƒæ–¹æ³•ç»“åˆå¾®è°ƒæ–¹æ¡ˆæ¥æ‰¾åˆ°ä¸æˆ‘ä»¬æœ€åŒ¹é…çš„é…ç½®æ–‡ä»¶äº†ï¼Œä»è€Œå‡å°‘æˆ‘ä»¬å¯¹é…ç½®æ–‡ä»¶çš„ä¿®æ”¹é‡ã€‚\n",
    "\n",
    "è¿™é‡Œæˆ‘ä»¬é€‰æ‹©ä½¿ç”¨ `internlm2_chat_1_8b_qlora_alpaca_e3` é…ç½®æ–‡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb34b7f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!xtuner copy-cfg internlm2_chat_1_8b_qlora_alpaca_e3 ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f06741",
   "metadata": {},
   "source": [
    "å¤åˆ¶å¥½é…ç½®æ–‡ä»¶åï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚\n",
    "\n",
    "<details>\n",
    "<summary>ç›®å½•ç»“æ„</summary>\n",
    "\n",
    "```\n",
    "â”œâ”€â”€ Shanghai_AI_Laboratory\n",
    "â”‚   â”œâ”€â”€ internlm2-1_8b\n",
    "â”‚   â”‚   â”œâ”€â”€ README.md\n",
    "â”‚   â”‚   â”œâ”€â”€ config.json\n",
    "â”‚   â”‚   â”œâ”€â”€ configuration.json\n",
    "â”‚   â”‚   â”œâ”€â”€ configuration_internlm2.py\n",
    "â”‚   â”‚   â”œâ”€â”€ generation_config.json\n",
    "â”‚   â”‚   â”œâ”€â”€ modeling_internlm2.py\n",
    "â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin\n",
    "â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2.py\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenization_internlm2_fast.py\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenizer.json\n",
    "â”‚   â”‚   â”œâ”€â”€ tokenizer.model\n",
    "â”‚   â”‚   â””â”€â”€ tokenizer_config.json\n",
    "â”‚   â””â”€â”€ internlm2-chat-1_8b -> /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b\n",
    "â”‚       â”œâ”€â”€ README.md\n",
    "â”‚       â”œâ”€â”€ config.json\n",
    "â”‚       â”œâ”€â”€ configuration.json\n",
    "â”‚       â”œâ”€â”€ configuration_internlm2.py\n",
    "â”‚       â”œâ”€â”€ generation_config.json\n",
    "â”‚       â”œâ”€â”€ model-00001-of-00002.safetensors\n",
    "â”‚       â”œâ”€â”€ model-00002-of-00002.safetensors\n",
    "â”‚       â”œâ”€â”€ model.safetensors.index.json\n",
    "â”‚       â”œâ”€â”€ modeling_internlm2.py\n",
    "â”‚       â”œâ”€â”€ special_tokens_map.json\n",
    "â”‚       â”œâ”€â”€ tokenization_internlm2.py\n",
    "â”‚       â”œâ”€â”€ tokenization_internlm2_fast.py\n",
    "â”‚       â”œâ”€â”€ tokenizer.model\n",
    "â”‚       â””â”€â”€ tokenizer_config.json\n",
    "â”œâ”€â”€ datas\n",
    "â”‚   â”œâ”€â”€ assistant.json\n",
    "â”‚   â””â”€â”€ pretrain.json\n",
    "â”œâ”€â”€ internlm2_1_8b_full_custom_pretrain_e1_copy.py\n",
    "â”œâ”€â”€ internlm2_1_8b_qlora_alpaca_e3_copy.py\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30045904",
   "metadata": {},
   "source": [
    "ä¸‹é¢æˆ‘ä»¬å°†æ ¹æ®é¡¹ç›®çš„éœ€æ±‚ä¸€æ­¥æ­¥çš„è¿›è¡Œä¿®æ”¹å’Œè°ƒæ•´å§ï¼\n",
    "\n",
    "åœ¨ PART 1 çš„éƒ¨åˆ†ï¼Œç”±äºæˆ‘ä»¬ä¸å†éœ€è¦åœ¨ HuggingFace ä¸Šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å…ˆè¦æ›´æ¢æ¨¡å‹çš„è·¯å¾„ä»¥åŠæ•°æ®é›†çš„è·¯å¾„ä¸ºæˆ‘ä»¬æœ¬åœ°çš„è·¯å¾„ã€‚\n",
    "\n",
    "ä¸ºäº†è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿå®æ—¶è§‚å¯Ÿåˆ°æ¨¡å‹çš„å˜åŒ–æƒ…å†µï¼ŒXTuner è´´å¿ƒçš„æ¨å‡ºäº†ä¸€ä¸ª `evaluation_inputs` çš„å‚æ•°æ¥è®©æˆ‘ä»¬èƒ½å¤Ÿè®¾ç½®å¤šä¸ªé—®é¢˜æ¥ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–æ˜¯æœç€æˆ‘ä»¬æƒ³è¦çš„æ–¹å‘å‰è¿›çš„ã€‚æˆ‘ä»¬å¯ä»¥æ·»åŠ è‡ªå·±çš„è¾“å…¥ã€‚\n",
    "\n",
    "åœ¨ PART 3 çš„éƒ¨åˆ†ï¼Œç”±äºæˆ‘ä»¬å‡†å¤‡çš„æ•°æ®é›†æ˜¯ JSON æ ¼å¼çš„æ•°æ®ï¼Œå¹¶ä¸”å¯¹è¯å†…å®¹å·²ç»æ˜¯ `input` å’Œ `output` çš„æ•°æ®å¯¹ï¼Œæ‰€ä»¥ä¸éœ€è¦è¿›è¡Œæ ¼å¼è½¬æ¢ã€‚\n",
    "\n",
    "```diff\n",
    "#######################################################################\n",
    "#                          PART 1  Settings                           #\n",
    "#######################################################################\n",
    "- pretrained_model_name_or_path = 'internlm/internlm2-chat-1_8b'\n",
    "+ pretrained_model_name_or_path = 'Shanghai_AI_Laboratory/internlm2-chat-1_8b'\n",
    "\n",
    "- alpaca_en_path = 'tatsu-lab/alpaca'\n",
    "+ alpaca_en_path = 'datas/assistant.json'\n",
    "\n",
    "evaluation_inputs = [\n",
    "-    'è¯·ç»™æˆ‘ä»‹ç»äº”ä¸ªä¸Šæµ·çš„æ™¯ç‚¹', 'Please tell me five scenic spots in Shanghai'\n",
    "+    'è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', 'è¯·ç»™æˆ‘ä»‹ç»äº”ä¸ªä¸Šæµ·çš„æ™¯ç‚¹', 'Please tell me five scenic spots in Shanghai'\n",
    "]\n",
    "+ evaluation_inputs = ['ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥ç¬¬ä¸‰æœŸæ˜¯', 'ä¸Šæµ·æ˜¯', 'Shanghai is']\n",
    "\n",
    "#######################################################################\n",
    "#                      PART 3  Dataset & Dataloader                   #\n",
    "#######################################################################\n",
    "alpaca_en = dict(\n",
    "    type=process_hf_dataset,\n",
    "-   dataset=dict(type=load_dataset, path=alpaca_en_path),\n",
    "+   dataset=dict(type=load_dataset, path='json', data_files=dict(train=alpaca_en_path)),\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "-   dataset_map_fn=alpaca_map_fn,\n",
    "+   dataset_map_fn=None,\n",
    "    template_map_fn=dict(\n",
    "        type=template_map_fn_factory, template=prompt_template),\n",
    "    remove_unused_columns=True,\n",
    "    shuffle_before_pack=True,\n",
    "    pack_to_max_length=pack_to_max_length,\n",
    "    use_varlen_attn=use_varlen_attn)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3335937",
   "metadata": {},
   "source": [
    "ä¿®æ”¹å®Œåçš„å®Œæ•´çš„é…ç½®æ–‡ä»¶æ˜¯ï¼š\n",
    "\n",
    "<details>\n",
    "<summary>internlm2_chat_1_8b_qlora_alpaca_e3_copy.py</summary>\n",
    "\n",
    "```python\n",
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from mmengine.dataset import DefaultSampler\n",
    "from mmengine.hooks import (CheckpointHook, DistSamplerSeedHook, IterTimerHook,\n",
    "                            LoggerHook, ParamSchedulerHook)\n",
    "from mmengine.optim import AmpOptimWrapper, CosineAnnealingLR, LinearLR\n",
    "from peft import LoraConfig\n",
    "from torch.optim import AdamW\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer,\n",
    "                          BitsAndBytesConfig)\n",
    "\n",
    "from xtuner.dataset import process_hf_dataset\n",
    "from xtuner.dataset.collate_fns import default_collate_fn\n",
    "from xtuner.dataset.map_fns import alpaca_map_fn, template_map_fn_factory\n",
    "from xtuner.engine.hooks import (DatasetInfoHook, EvaluateChatHook,\n",
    "                                 VarlenAttnArgsToMessageHubHook)\n",
    "from xtuner.engine.runner import TrainLoop\n",
    "from xtuner.model import SupervisedFinetune\n",
    "from xtuner.parallel.sequence import SequenceParallelSampler\n",
    "from xtuner.utils import PROMPT_TEMPLATE, SYSTEM_TEMPLATE\n",
    "\n",
    "#######################################################################\n",
    "#                          PART 1  Settings                           #\n",
    "#######################################################################\n",
    "# Model\n",
    "pretrained_model_name_or_path = 'Shanghai_AI_Laboratory/internlm2-chat-1_8b'\n",
    "use_varlen_attn = False\n",
    "\n",
    "# Data\n",
    "alpaca_en_path = 'datas/assistant.json'\n",
    "prompt_template = PROMPT_TEMPLATE.internlm2_chat\n",
    "max_length = 2048\n",
    "pack_to_max_length = True\n",
    "\n",
    "# parallel\n",
    "sequence_parallel_size = 1\n",
    "\n",
    "# Scheduler & Optimizer\n",
    "batch_size = 1  # per_device\n",
    "accumulative_counts = 16\n",
    "accumulative_counts *= sequence_parallel_size\n",
    "dataloader_num_workers = 0\n",
    "max_epochs = 3\n",
    "optim_type = AdamW\n",
    "lr = 2e-4\n",
    "betas = (0.9, 0.999)\n",
    "weight_decay = 0\n",
    "max_norm = 1  # grad clip\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Save\n",
    "save_steps = 500\n",
    "save_total_limit = 2  # Maximum checkpoints to keep (-1 means unlimited)\n",
    "\n",
    "# Evaluate the generation performance during the training\n",
    "evaluation_freq = 500\n",
    "SYSTEM = SYSTEM_TEMPLATE.alpaca\n",
    "evaluation_inputs = [\n",
    "    'è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', 'è¯·ç»™æˆ‘ä»‹ç»äº”ä¸ªä¸Šæµ·çš„æ™¯ç‚¹', 'Please tell me five scenic spots in Shanghai'\n",
    "]\n",
    "\n",
    "#######################################################################\n",
    "#                      PART 2  Model & Tokenizer                      #\n",
    "#######################################################################\n",
    "tokenizer = dict(\n",
    "    type=AutoTokenizer.from_pretrained,\n",
    "    pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "    trust_remote_code=True,\n",
    "    padding_side='right')\n",
    "\n",
    "model = dict(\n",
    "    type=SupervisedFinetune,\n",
    "    use_varlen_attn=use_varlen_attn,\n",
    "    llm=dict(\n",
    "        type=AutoModelForCausalLM.from_pretrained,\n",
    "        pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=dict(\n",
    "            type=BitsAndBytesConfig,\n",
    "            load_in_4bit=True,\n",
    "            load_in_8bit=False,\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type='nf4')),\n",
    "    lora=dict(\n",
    "        type=LoraConfig,\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias='none',\n",
    "        task_type='CAUSAL_LM'))\n",
    "\n",
    "#######################################################################\n",
    "#                      PART 3  Dataset & Dataloader                   #\n",
    "#######################################################################\n",
    "alpaca_en = dict(\n",
    "    type=process_hf_dataset,\n",
    "    dataset=dict(type=load_dataset, path='json', data_files=dict(train=alpaca_en_path)),\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    dataset_map_fn=None,\n",
    "    template_map_fn=dict(\n",
    "        type=template_map_fn_factory, template=prompt_template),\n",
    "    remove_unused_columns=True,\n",
    "    shuffle_before_pack=True,\n",
    "    pack_to_max_length=pack_to_max_length,\n",
    "    use_varlen_attn=use_varlen_attn)\n",
    "\n",
    "sampler = SequenceParallelSampler \\\n",
    "    if sequence_parallel_size > 1 else DefaultSampler\n",
    "train_dataloader = dict(\n",
    "    batch_size=batch_size,\n",
    "    num_workers=dataloader_num_workers,\n",
    "    dataset=alpaca_en,\n",
    "    sampler=dict(type=sampler, shuffle=True),\n",
    "    collate_fn=dict(type=default_collate_fn, use_varlen_attn=use_varlen_attn))\n",
    "\n",
    "#######################################################################\n",
    "#                    PART 4  Scheduler & Optimizer                    #\n",
    "#######################################################################\n",
    "# optimizer\n",
    "optim_wrapper = dict(\n",
    "    type=AmpOptimWrapper,\n",
    "    optimizer=dict(\n",
    "        type=optim_type, lr=lr, betas=betas, weight_decay=weight_decay),\n",
    "    clip_grad=dict(max_norm=max_norm, error_if_nonfinite=False),\n",
    "    accumulative_counts=accumulative_counts,\n",
    "    loss_scale='dynamic',\n",
    "    dtype='float16')\n",
    "\n",
    "# learning policy\n",
    "# More information: https://github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/param_scheduler.md  # noqa: E501\n",
    "param_scheduler = [\n",
    "    dict(\n",
    "        type=LinearLR,\n",
    "        start_factor=1e-5,\n",
    "        by_epoch=True,\n",
    "        begin=0,\n",
    "        end=warmup_ratio * max_epochs,\n",
    "        convert_to_iter_based=True),\n",
    "    dict(\n",
    "        type=CosineAnnealingLR,\n",
    "        eta_min=0.0,\n",
    "        by_epoch=True,\n",
    "        begin=warmup_ratio * max_epochs,\n",
    "        end=max_epochs,\n",
    "        convert_to_iter_based=True)\n",
    "]\n",
    "\n",
    "# train, val, test setting\n",
    "train_cfg = dict(type=TrainLoop, max_epochs=max_epochs)\n",
    "\n",
    "#######################################################################\n",
    "#                           PART 5  Runtime                           #\n",
    "#######################################################################\n",
    "# Log the dialogue periodically during the training process, optional\n",
    "custom_hooks = [\n",
    "    dict(type=DatasetInfoHook, tokenizer=tokenizer),\n",
    "    dict(\n",
    "        type=EvaluateChatHook,\n",
    "        tokenizer=tokenizer,\n",
    "        every_n_iters=evaluation_freq,\n",
    "        evaluation_inputs=evaluation_inputs,\n",
    "        system=SYSTEM,\n",
    "        prompt_template=prompt_template)\n",
    "]\n",
    "\n",
    "if use_varlen_attn:\n",
    "    custom_hooks += [dict(type=VarlenAttnArgsToMessageHubHook)]\n",
    "\n",
    "# configure default hooks\n",
    "default_hooks = dict(\n",
    "    # record the time of every iteration.\n",
    "    timer=dict(type=IterTimerHook),\n",
    "    # print log every 10 iterations.\n",
    "    logger=dict(type=LoggerHook, log_metric_by_epoch=False, interval=10),\n",
    "    # enable the parameter scheduler.\n",
    "    param_scheduler=dict(type=ParamSchedulerHook),\n",
    "    # save checkpoint per `save_steps`.\n",
    "    checkpoint=dict(\n",
    "        type=CheckpointHook,\n",
    "        by_epoch=False,\n",
    "        interval=save_steps,\n",
    "        max_keep_ckpts=save_total_limit),\n",
    "    # set sampler seed in distributed evrionment.\n",
    "    sampler_seed=dict(type=DistSamplerSeedHook),\n",
    ")\n",
    "\n",
    "# configure environment\n",
    "env_cfg = dict(\n",
    "    # whether to enable cudnn benchmark\n",
    "    cudnn_benchmark=False,\n",
    "    # set multi process parameters\n",
    "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n",
    "    # set distributed parameters\n",
    "    dist_cfg=dict(backend='nccl'),\n",
    ")\n",
    "\n",
    "# set visualizer\n",
    "visualizer = None\n",
    "\n",
    "# set log level\n",
    "log_level = 'INFO'\n",
    "\n",
    "# load from which checkpoint\n",
    "load_from = None\n",
    "\n",
    "# whether to resume training from the loaded checkpoint\n",
    "resume = False\n",
    "\n",
    "# Defaults to use random seed and disable `deterministic`\n",
    "randomness = dict(seed=None, deterministic=False)\n",
    "\n",
    "# set log processor\n",
    "log_processor = dict(by_epoch=False)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c751c8",
   "metadata": {},
   "source": [
    "#### å¯åŠ¨å¾®è°ƒ\n",
    "\n",
    "å®Œæˆäº†æ‰€æœ‰çš„å‡†å¤‡å·¥ä½œåï¼Œæˆ‘ä»¬å°±å¯ä»¥æ­£å¼çš„å¼€å§‹æˆ‘ä»¬ä¸‹ä¸€é˜¶æ®µçš„æ—…ç¨‹ï¼šXTuner å¯åŠ¨~ï¼\n",
    "\n",
    "å½“æˆ‘ä»¬å‡†å¤‡å¥½äº†æ‰€æœ‰å†…å®¹ï¼Œæˆ‘ä»¬åªéœ€è¦å°†ä½¿ç”¨ `xtuner train` å‘½ä»¤ä»¤å³å¯å¼€å§‹è®­ç»ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edaf71d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!xtuner train ./internlm2_chat_1_8b_qlora_alpaca_e3_copy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab922880",
   "metadata": {},
   "source": [
    "åœ¨è®­ç»ƒå®Œåï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚\n",
    "\n",
    "<details>\n",
    "<summary>ç›®å½•ç»“æ„</summary>\n",
    "\n",
    "```\n",
    "â”œâ”€â”€ work_dirs\n",
    "â”‚   â””â”€â”€ internlm2_chat_1_8b_qlora_alpaca_e3_copy\n",
    "â”‚       â”œâ”€â”€ 20240626_222727\n",
    "â”‚       â”‚   â”œâ”€â”€ 20240626_222727.log\n",
    "â”‚       â”‚   â””â”€â”€ vis_data\n",
    "â”‚       â”‚       â”œâ”€â”€ 20240626_222727.json\n",
    "â”‚       â”‚       â”œâ”€â”€ config.py\n",
    "â”‚       â”‚       â”œâ”€â”€ eval_outputs_iter_95.txt\n",
    "â”‚       â”‚       â””â”€â”€ scalars.json\n",
    "â”‚       â”œâ”€â”€ internlm2_chat_1_8b_qlora_alpaca_e3_copy.py\n",
    "â”‚       â”œâ”€â”€ iter_96.pth\n",
    "â”‚       â””â”€â”€ last_checkpoint\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b3e7a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!tree -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7429b876",
   "metadata": {},
   "source": [
    "#### æ¨¡å‹æ ¼å¼è½¬æ¢\n",
    "\n",
    "æ¨¡å‹è½¬æ¢çš„æœ¬è´¨å…¶å®å°±æ˜¯å°†åŸæœ¬ä½¿ç”¨ Pytorch è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æƒé‡æ–‡ä»¶è½¬æ¢ä¸ºç›®å‰é€šç”¨çš„ HuggingFace æ ¼å¼æ–‡ä»¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¥å®ç°ä¸€é”®è½¬æ¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9870c4f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pth_file=`ls -t ./work_dirs/internlm2_chat_1_8b_qlora_alpaca_e3_copy/*.pth | head -n 1` && MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU xtuner convert pth_to_hf ./internlm2_chat_1_8b_qlora_alpaca_e3_copy.py ${pth_file} ./hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86722f73",
   "metadata": {},
   "source": [
    "æ¨¡å‹æ ¼å¼è½¬æ¢å®Œæˆåï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚\n",
    "\n",
    "<details>\n",
    "<summary>ç›®å½•ç»“æ„</summary>\n",
    "\n",
    "```\n",
    "â”œâ”€â”€ hf\n",
    "â”‚   â”œâ”€â”€ README.md\n",
    "â”‚   â”œâ”€â”€ adapter_config.json\n",
    "â”‚   â”œâ”€â”€ adapter_model.bin\n",
    "â”‚   â””â”€â”€ xtuner_config.py\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f8cfb",
   "metadata": {},
   "source": [
    "#### æ¨¡å‹åˆå¹¶\n",
    "\n",
    "å¯¹äº LoRA æˆ–è€… QLoRA å¾®è°ƒå‡ºæ¥çš„æ¨¡å‹å…¶å®å¹¶ä¸æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹ï¼Œè€Œæ˜¯ä¸€ä¸ªé¢å¤–çš„å±‚ï¼ˆAdapterï¼‰ï¼Œè®­ç»ƒå®Œçš„è¿™ä¸ªå±‚æœ€ç»ˆè¿˜æ˜¯è¦ä¸åŸæ¨¡å‹è¿›è¡Œåˆå¹¶æ‰èƒ½è¢«æ­£å¸¸çš„ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a440c9",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU xtuner convert merge Shanghai_AI_Laboratory/internlm2-chat-1_8b ./hf ./merged --max-shard-size 2GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e0379c",
   "metadata": {},
   "source": [
    "æ¨¡å‹åˆå¹¶å®Œæˆåï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚\n",
    "\n",
    "<details>\n",
    "<summary>ç›®å½•ç»“æ„</summary>\n",
    "\n",
    "```\n",
    "â”œâ”€â”€ merged\n",
    "â”‚   â”œâ”€â”€ config.json\n",
    "â”‚   â”œâ”€â”€ configuration_internlm2.py\n",
    "â”‚   â”œâ”€â”€ generation_config.json\n",
    "â”‚   â”œâ”€â”€ modeling_internlm2.py\n",
    "â”‚   â”œâ”€â”€ pytorch_model-00001-of-00002.bin\n",
    "â”‚   â”œâ”€â”€ pytorch_model-00002-of-00002.bin\n",
    "â”‚   â”œâ”€â”€ pytorch_model.bin.index.json\n",
    "â”‚   â”œâ”€â”€ special_tokens_map.json\n",
    "â”‚   â”œâ”€â”€ tokenization_internlm2.py\n",
    "â”‚   â”œâ”€â”€ tokenization_internlm2_fast.py\n",
    "â”‚   â”œâ”€â”€ tokenizer.json\n",
    "â”‚   â”œâ”€â”€ tokenizer.model\n",
    "â”‚   â””â”€â”€ tokenizer_config.json\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9923d51f",
   "metadata": {},
   "source": [
    "### å¾®è°ƒåçš„æ¨¡å‹å¯¹è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model(\"./merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e768f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"ä½ åœ¨å®æˆ˜è¥åšä»€ä¹ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b53c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"ä»‹ç»ä¸€ä¸‹æˆéƒ½\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a185d8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenizer, model\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2326897e-7594-4082-8bc7-57681f837cdc",
   "metadata": {},
   "source": [
    "## DeepSpeedä»‹ç»\n",
    "\n",
    "DeepSpeedæ˜¯ä¸€ä¸ªç”±å¾®è½¯å¼€å‘çš„å¼€æºæ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡å’Œé€Ÿåº¦ã€‚\n",
    "\n",
    "XTuner ä¹Ÿå†…ç½®äº† `deepspeed` æ¥åŠ é€Ÿæ•´ä½“çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå…±æœ‰ä¸‰ç§ä¸åŒçš„ `deepspeed` ç±»å‹å¯è¿›è¡Œé€‰æ‹©ï¼Œåˆ†åˆ«æ˜¯ `deepspeed_zero1`, `deepspeed_zero2` å’Œ `deepspeed_zero3`ã€‚\n",
    "\n",
    "<details>\n",
    "<summary>DeepSpeedä¼˜åŒ–å™¨åŠå…¶é€‰æ‹©æ–¹æ³•</summary>\n",
    "\n",
    "DeepSpeedæ˜¯ä¸€ä¸ªç”±å¾®è½¯å¼€å‘çš„å¼€æºæ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡å’Œé€Ÿåº¦ã€‚å®ƒé€šè¿‡å‡ ç§å…³é”®æŠ€æœ¯æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¨¡å‹åˆ†å‰²ã€æ¢¯åº¦ç´¯ç§¯ã€ä»¥åŠå†…å­˜å’Œå¸¦å®½ä¼˜åŒ–ç­‰ï¼Œèƒ½å¤Ÿé™ä½è®­ç»ƒè¶…å¤§è§„æ¨¡æ¨¡å‹çš„å¤æ‚æ€§å’Œèµ„æºéœ€æ±‚ï¼Œä½¿è®­ç»ƒå˜å¾—æ›´å¿«ã€æ›´é«˜æ•ˆã€‚DeepSpeedç‰¹åˆ«é€‚ç”¨äºéœ€è¦å·¨å¤§è®¡ç®—èµ„æºçš„å¤§å‹æ¨¡å‹å’Œæ•°æ®é›†ã€‚\n",
    "\n",
    "åœ¨DeepSpeedä¸­ï¼Œå¼•å…¥äº†ZeROï¼ˆZero Redundancy Optimizerï¼‰æŠ€æœ¯ï¼Œæ˜¯ä¸€ç§æ—¨åœ¨é™ä½è®­ç»ƒå¤§å‹æ¨¡å‹æ‰€éœ€å†…å­˜å ç”¨çš„ä¼˜åŒ–å™¨ï¼Œé€šè¿‡åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­åˆ†å‰²ä¼˜åŒ–å™¨çš„çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°ï¼Œå‡å°‘å†—ä½™çš„å†…å­˜å ç”¨ï¼Œå…è®¸æ›´å¤§çš„æ¨¡å‹å’Œæ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ã€‚ZeRO åˆ†ä¸ºå‡ ä¸ªä¸åŒçš„çº§åˆ«ï¼Œä¸»è¦åŒ…æ‹¬ï¼š\n",
    "\n",
    "- **deepspeed_zero1**ï¼šè¿™æ˜¯ZeROçš„åŸºæœ¬ç‰ˆæœ¬ï¼Œå®ƒä¼˜åŒ–äº†æ¨¡å‹å‚æ•°çš„å­˜å‚¨ï¼Œä¸»è¦é€šè¿‡åˆ†åŒºå­˜å‚¨ä¼˜åŒ–å™¨çŠ¶æ€æ¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚æ¯ä¸ªGPUè®¾å¤‡åªä¿å­˜ä¸€éƒ¨åˆ†ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œä»è€Œæ˜¾è‘—å‡å°‘å†…å­˜æ¶ˆè€—ã€‚\n",
    "\n",
    "- **deepspeed_zero2**ï¼šåœ¨deepspeed_zero1çš„åŸºç¡€ä¸Šï¼Œdeepspeed_zero2è¿›ä¸€æ­¥ä¼˜åŒ–äº†æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€çš„å­˜å‚¨ï¼Œå°†æ¢¯åº¦ä¹Ÿè¿›è¡Œåˆ†åŒºå­˜å‚¨ã€‚è¿™æ ·ï¼Œæ¯ä¸ªGPUè®¾å¤‡åªéœ€è¦ä¿å­˜ä¸€éƒ¨åˆ†çš„ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ï¼Œè¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚\n",
    "\n",
    "- **deepspeed_zero3**ï¼šè¿™æ˜¯ç›®å‰æœ€é«˜çº§çš„ä¼˜åŒ–ç­‰çº§ï¼Œå®ƒåŒ…æ‹¬äº†deepspeed_zero1å’Œdeepspeed_zero2çš„ä¼˜åŒ–ï¼Œé™¤äº†ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ï¼Œè¿˜å°†æ¨¡å‹å‚æ•°è¿›è¡Œåˆ†åŒºå­˜å‚¨ã€‚æ¯ä¸ªGPUè®¾å¤‡åªéœ€è¦ä¿å­˜ä¸€éƒ¨åˆ†çš„ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œæ¨¡å‹å‚æ•°ï¼Œä»è€Œæœ€å¤§é™åº¦åœ°å‡å°‘å†…å­˜ä½¿ç”¨ã€‚\n",
    "\n",
    "é€‰æ‹©å“ªç§deepspeedç±»å‹ä¸»è¦å–å†³äºä½ çš„å…·ä½“éœ€æ±‚ï¼ŒåŒ…æ‹¬æ¨¡å‹çš„å¤§å°ã€å¯ç”¨çš„ç¡¬ä»¶èµ„æºï¼ˆç‰¹åˆ«æ˜¯GPUå†…å­˜ï¼‰ä»¥åŠè®­ç»ƒçš„æ•ˆç‡éœ€æ±‚ã€‚ä¸€èˆ¬æ¥è¯´ï¼š\n",
    "\n",
    "- å¦‚æœä½ çš„æ¨¡å‹è¾ƒå°ï¼Œæˆ–è€…å†…å­˜èµ„æºå……è¶³ï¼Œå¯èƒ½ä¸éœ€è¦ä½¿ç”¨æœ€é«˜çº§åˆ«çš„ä¼˜åŒ–ã€‚\n",
    "- å¦‚æœä½ éœ€è¦å¿«é€Ÿè®­ç»ƒæ¨¡å‹ï¼Œå¯èƒ½éœ€è¦æƒè¡¡å†…å­˜ä¼˜åŒ–å’Œè®¡ç®—æ•ˆç‡ã€‚deepspeed_zero1æä¾›äº†è¾ƒä½çš„å†…å­˜å ç”¨ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„è®¡ç®—æ•ˆç‡ã€‚\n",
    "- å¦‚æœä½ æ­£åœ¨å°è¯•è®­ç»ƒéå¸¸å¤§çš„æ¨¡å‹ï¼Œæˆ–è€…ä½ çš„ç¡¬ä»¶èµ„æºæœ‰é™ï¼Œä½¿ç”¨deepspeed_zero2æˆ–deepspeed_zero3å¯èƒ½æ›´åˆé€‚ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥æ˜¾è‘—é™ä½å†…å­˜å ç”¨ï¼Œå…è®¸æ›´å¤§æ¨¡å‹çš„è®­ç»ƒã€‚\n",
    "- é€‰æ‹©æ—¶ä¹Ÿè¦è€ƒè™‘åˆ°å®ç°çš„å¤æ‚æ€§å’Œè¿è¡Œæ—¶çš„å¼€é”€ï¼Œæ›´é«˜çº§çš„ä¼˜åŒ–å¯èƒ½éœ€è¦æ›´å¤æ‚çš„è®¾ç½®ï¼Œæ›´é¢‘ç¹çš„è·¨GPUé€šä¿¡ï¼Œè¿™å¯èƒ½éœ€è¦æ›´é«˜çš„ç½‘ç»œå¸¦å®½ï¼Œå¹¶å¯èƒ½å¢åŠ ä¸€äº›è®¡ç®—å¼€é”€ã€‚\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74dd2a4",
   "metadata": {},
   "source": [
    "## å¤šå¡å¾®è°ƒ\n",
    "\n",
    "æ¨¡å‹çš„è§„æ¨¡å’Œå¤æ‚åº¦ä¸æ–­å¢åŠ ï¼Œå•å¼ GPUçš„æ˜¾å­˜å¾€å¾€æ— æ³•æ»¡è¶³å¤§æ¨¡å‹çš„è®­ç»ƒéœ€æ±‚ã€‚æ­¤æ—¶ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦å¤šå¡å¾®è°ƒï¼Œä»¥åº”å¯¹å¤§æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¾å­˜å’Œè®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cc6567",
   "metadata": {},
   "source": [
    "\n",
    "XTuner ä¸­ä½¿ç”¨å¤šå¡å¾®è°ƒï¼Œåªéœ€è¦è®¾ç½® `NPROC_PER_NODE` ç¯å¢ƒå˜é‡ï¼Œå¹¶ä½¿ç”¨ `DeepSpeed` æ¥è¿›è¡ŒåŠ é€Ÿå°±å¯ä»¥äº†ï¼Œå…¶ä½™å‘½ä»¤å†…å®¹ä¸å•å¡å¾®è°ƒæ—¶ä¸€æ ·ã€‚\n",
    "\n",
    "> ç”±äºå¼€å‘æœºåªæœ‰ä¸¤å¼ æ˜¾å¡ï¼Œæ‰€ä»¥æˆ‘ä»¬è®¾ç½®`NPROC_PER_NODE=2`ï¼Œå¹¶ä¸”é€‰æ‹©ä½¿ç”¨`deepspeed_zero3`ä¼˜åŒ–ç­‰çº§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a213cd",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU NPROC_PER_NODE=2 xtuner train ./internlm2_chat_1_8b_qlora_alpaca_e3_copy.py --deepspeed deepspeed_zero3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c81ca6f",
   "metadata": {},
   "source": [
    "åœ¨æ‰§è¡Œå¾®è°ƒçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤å¼ æ˜¾å¡éƒ½æœ‰å†…å­˜ä½¿ç”¨ã€‚\n",
    "\n",
    "![](images/image-06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922b48a6",
   "metadata": {},
   "source": [
    "åœ¨è®­ç»ƒå®Œåï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ã€‚\n",
    "\n",
    "<details>\n",
    "<summary>ç›®å½•ç»“æ„</summary>\n",
    "\n",
    "```\n",
    "â”œâ”€â”€ work_dirs\n",
    "â”‚   â””â”€â”€ internlm2_chat_1_8b_qlora_alpaca_e3_copy\n",
    "â”‚       â”œâ”€â”€ 20240627_205957\n",
    "â”‚       â”‚   â”œâ”€â”€ 20240627_205957.log\n",
    "â”‚       â”‚   â””â”€â”€ vis_data\n",
    "â”‚       â”‚       â”œâ”€â”€ 20240627_205957.json\n",
    "â”‚       â”‚       â”œâ”€â”€ config.py\n",
    "â”‚       â”‚       â”œâ”€â”€ eval_outputs_iter_236.txt\n",
    "â”‚       â”‚       â””â”€â”€ scalars.json\n",
    "â”‚       â”œâ”€â”€ internlm2_chat_1_8b_qlora_alpaca_e3_copy.py\n",
    "â”‚       â”œâ”€â”€ iter_237.pth\n",
    "â”‚       â”‚   â”œâ”€â”€ bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
    "â”‚       â”‚   â”œâ”€â”€ bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt\n",
    "â”‚       â”‚   â”œâ”€â”€ zero_pp_rank_0_mp_rank_00_model_states.pt\n",
    "â”‚       â”‚   â””â”€â”€ zero_pp_rank_1_mp_rank_00_model_states.pt\n",
    "â”‚       â”œâ”€â”€ last_checkpoint\n",
    "â”‚       â””â”€â”€ zero_to_fp32.py\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡ `deepspeed` æ¥è®­ç»ƒåå¾—åˆ°çš„æƒé‡æ–‡ä»¶å’ŒåŸæœ¬çš„æƒé‡æ–‡ä»¶æ˜¯æœ‰æ‰€å·®åˆ«çš„ï¼ŒåŸæœ¬çš„ä»…ä»…æ˜¯ä¸€ä¸ª .pth çš„æ–‡ä»¶ï¼Œè€Œä½¿ç”¨äº† `deepspeed` åˆ™æ˜¯ä¸€ä¸ªåå­—å¸¦æœ‰ .pth çš„æ–‡ä»¶å¤¹ï¼Œåœ¨è¯¥æ–‡ä»¶å¤¹é‡Œä¿å­˜äº† .pt æ–‡ä»¶ã€‚è¿™ä¸¤è€…åœ¨å…·ä½“çš„ä½¿ç”¨ä¸Šå¹¶æ²¡æœ‰å¤ªå¤§çš„å·®åˆ«ï¼Œè½¬æ¢å’Œåˆå¹¶çš„è¿‡ç¨‹éƒ½æ˜¯ä¸€æ ·çš„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced7e55",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pth_file=`ls -t ./work_dirs/internlm2_chat_1_8b_qlora_alpaca_e3_copy | grep pth | head -n 1` && MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU xtuner convert pth_to_hf ./internlm2_chat_1_8b_qlora_alpaca_e3_copy.py ./work_dirs/internlm2_chat_1_8b_qlora_alpaca_e3_copy/${pth_file} ./hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c58d5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU xtuner convert merge Shanghai_AI_Laboratory/internlm2-chat-1_8b ./hf ./merged --max-shard-size 2GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9197c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model(\"./merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bfa713",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f12d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"ä½ åœ¨å®æˆ˜è¥åšä»€ä¹ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"ä»‹ç»ä¸€ä¸‹æˆéƒ½\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91787f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenizer, model\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cfecbd-c125-4fa6-98ed-76da8015241e",
   "metadata": {},
   "source": [
    "## åˆ†å¸ƒå¼å¾®è°ƒ\n",
    "\n",
    "å¦‚æœæ¨¡å‹çš„è§„æ¨¡å’Œå¤æ‚åº¦ç»§ç»­å¢åŠ ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨åˆ†å¸ƒå¼å¾®è°ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a025a904",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!apt-get install -y net-tools\n",
    "!ifconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95189de",
   "metadata": {},
   "source": [
    "åˆ†å¸ƒå¼å¾®è°ƒæ˜¯ä¸»ä»æ¶æ„çš„ã€‚ä¸»èŠ‚ç‚¹åè°ƒæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ï¼Œç®¡ç†æ•°æ®å’Œä»»åŠ¡åˆ°å·¥ä½œèŠ‚ç‚¹çš„åˆ†é…ã€‚å·¥ä½œèŠ‚ç‚¹æ‰§è¡Œè®­ç»ƒæ­¥éª¤çš„å®é™…è®¡ç®—ï¼Œå¤„ç†æ•°æ®çš„å­é›†å¹¶è®¡ç®—æ¢¯åº¦ã€‚æœ‰æ—¶å€™åœ¨ä¸€äº›æ¶æ„ä¸­è¿˜éœ€è¦å‚æ•°æœåŠ¡å™¨åè°ƒæ‰€æœ‰å·¥ä½œèŠ‚ç‚¹ä¹‹é—´çš„æ¨¡å‹æ›´æ–°åŒæ­¥ï¼Œç”¨äºèšåˆæ¥è‡ªå·¥ä½œèŠ‚ç‚¹çš„æ¢¯åº¦å¹¶æ›´æ–°æ¨¡å‹å‚æ•°ã€‚\n",
    "\n",
    "> æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªèŠ‚ç‚¹è¿›è¡Œåˆ†å¸ƒå¼å¾®è°ƒï¼Œå®é™…ä¸Šéœ€è¦å¯åŠ¨ä¸‰ä¸ªèŠ‚ç‚¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195212b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU OMP_NUM_THREADS=1 MKL_NUM_THREADS=1 NPROC_PER_NODE=1 NNODES=2 xtuner train internlm2_chat_1_8b_qlora_alpaca_e3_copy.py\n",
    "\n",
    "!MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU OMP_NUM_THREADS=1 MKL_NUM_THREADS=1 NPROC_PER_NODE=1 NNODES=2 NODE_RANK=0 TRITON_CACHE_DIR=node0 PORT=20821 ADDR=192.168.230.182 xtuner train internlm2_chat_1_8b_qlora_alpaca_e3_copy.py --work-dir work_dir_node0\n",
    "\n",
    "!MKL_SERVICE_FORCE_INTEL=1 MKL_THREADING_LAYER=GNU OMP_NUM_THREADS=1 MKL_NUM_THREADS=1 NPROC_PER_NODE=1 NNODES=2 NODE_RANK=1 TRITON_CACHE_DIR=node1 PORT=20821 ADDR=192.168.230.182 xtuner train internlm2_chat_1_8b_qlora_alpaca_e3_copy.py --work-dir work_dir_node1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbcffbe",
   "metadata": {},
   "source": [
    "é¦–å…ˆå¯åŠ¨ä¸»èŠ‚ç‚¹ï¼Œç„¶åä¾æ¬¡å¯åŠ¨å…¶ä»–èŠ‚ç‚¹ã€‚ä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œéœ€è¦åœ¨ä¸€ä¸ªæ—¶é—´é˜ˆå€¼å†…å¯åŠ¨ç›¸å…³çš„èŠ‚ç‚¹ï¼Œå¦‚æœè¶…è¿‡æ—¶é—´é˜ˆå€¼è¿˜æ²¡å¯åŠ¨æ‰€æœ‰èŠ‚ç‚¹ï¼Œåˆ™å…¶ä»–èŠ‚ç‚¹ä¼šå› è¶…æ—¶è€ŒæŠ¥é”™é€€å‡ºã€‚\n",
    "\n",
    "æ¯”å¦‚ï¼Œåœ¨ä¸¤ä¸ªèŠ‚ç‚¹çš„åˆ†å¸ƒå¼å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åªå¯åŠ¨ä¸»èŠ‚ç‚¹å’Œä¸€ä¸ªå·¥ä½œèŠ‚ç‚¹ï¼Œå¦ä¸€ä¸ªèŠ‚ç‚¹ä¸å¯åŠ¨ï¼Œåˆ™å·²å¯åŠ¨çš„èŠ‚ç‚¹ä¼šè¶…æ—¶æŠ¥é”™é€€å‡ºã€‚\n",
    "\n",
    "![](images/image-07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953e85aa",
   "metadata": {},
   "source": [
    "å¦‚æœæ‰€æœ‰èŠ‚ç‚¹éƒ½æ­£å¸¸å¯åŠ¨ã€è®­ç»ƒï¼Œåˆ™å¯ä»¥çœ‹åˆ°æ¯ä¸ªèŠ‚ç‚¹çš„æ˜¾å¡å‡æœ‰å†…å­˜ä½¿ç”¨ã€‚\n",
    "\n",
    "![](images/image-08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93beb75",
   "metadata": {},
   "source": [
    "åœ¨è®­ç»ƒå®Œåï¼Œæˆ‘ä»¬çš„ç›®å½•ç»“æ„åº”è¯¥æ˜¯è¿™æ ·å­çš„ï¼Œè®­ç»ƒçš„æ¨¡å‹åœ¨å·¥ä½œèŠ‚ç‚¹ä¸Šã€‚\n",
    "\n",
    "<details>\n",
    "<summary>ç›®å½•ç»“æ„</summary>\n",
    "\n",
    "```\n",
    "â”œâ”€â”€ work_dir_node0\n",
    "â”‚   â”œâ”€â”€ 20240627_213009\n",
    "â”‚   â”‚   â”œâ”€â”€ 20240627_213009.log\n",
    "â”‚   â”‚   â””â”€â”€ vis_data\n",
    "â”‚   â”‚       â”œâ”€â”€ 20240627_213009.json\n",
    "â”‚   â”‚       â”œâ”€â”€ config.py\n",
    "â”‚   â”‚       â”œâ”€â”€ eval_outputs_iter_233.txt\n",
    "â”‚   â”‚       â””â”€â”€ scalars.json\n",
    "â”‚   â”œâ”€â”€ internlm2_chat_1_8b_qlora_alpaca_e3_copy.py\n",
    "â”‚   â”œâ”€â”€ iter_234.pth\n",
    "â”‚   â””â”€â”€ last_checkpoint\n",
    "â”œâ”€â”€ work_dir_node1\n",
    "â”‚   â””â”€â”€ 20240627_213009\n",
    "â”œâ”€â”€ work_dirs\n",
    "â”‚   â””â”€â”€ internlm2_chat_1_8b_qlora_alpaca_e3_copy\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820ec02",
   "metadata": {},
   "source": [
    "## Web Demo éƒ¨ç½²\n",
    "\n",
    "é™¤äº†åœ¨ç»ˆç«¯ä¸­å¯¹æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Œæˆ‘ä»¬å…¶å®è¿˜å¯ä»¥åœ¨ç½‘é¡µç«¯çš„ Demo è¿›è¡Œå¯¹è¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdb5c60",
   "metadata": {},
   "source": [
    "é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…æ‰€éœ€è¦çš„ä¾èµ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e166d8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2ef9c5",
   "metadata": {},
   "source": [
    "å…¶æ¬¡ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡ä¸€ä¸ªStreamlitç¨‹åºçš„è„šæœ¬ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b39a748",
   "metadata": {},
   "source": [
    "Streamlitç¨‹åºçš„å®Œæ•´ä»£ç ã€‚\n",
    "\n",
    "<details>\n",
    "<summary>streamlit_demo.py</summary>\n",
    "\n",
    "```python\n",
    "import copy\n",
    "import warnings\n",
    "from dataclasses import asdict, dataclass\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "import streamlit as st\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers.generation.utils import (LogitsProcessorList,\n",
    "                                           StoppingCriteriaList)\n",
    "from transformers.utils import logging\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # isort: skip\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "model_name_or_path = \"./merged\"\n",
    "\n",
    "@dataclass\n",
    "class GenerationConfig:\n",
    "    # this config is used for chat to provide more diversity\n",
    "    max_length: int = 2048\n",
    "    top_p: float = 0.75\n",
    "    temperature: float = 0.1\n",
    "    do_sample: bool = True\n",
    "    repetition_penalty: float = 1.000\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_interactive(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    generation_config: Optional[GenerationConfig] = None,\n",
    "    logits_processor: Optional[LogitsProcessorList] = None,\n",
    "    stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "    prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor],\n",
    "                                                List[int]]] = None,\n",
    "    additional_eos_token_id: Optional[int] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    inputs = tokenizer([prompt], padding=True, return_tensors='pt')\n",
    "    input_length = len(inputs['input_ids'][0])\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = v.cuda()\n",
    "    input_ids = inputs['input_ids']\n",
    "    _, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]\n",
    "    if generation_config is None:\n",
    "        generation_config = model.generation_config\n",
    "    generation_config = copy.deepcopy(generation_config)\n",
    "    model_kwargs = generation_config.update(**kwargs)\n",
    "    bos_token_id, eos_token_id = (  # noqa: F841  # pylint: disable=W0612\n",
    "        generation_config.bos_token_id,\n",
    "        generation_config.eos_token_id,\n",
    "    )\n",
    "    if isinstance(eos_token_id, int):\n",
    "        eos_token_id = [eos_token_id]\n",
    "    if additional_eos_token_id is not None:\n",
    "        eos_token_id.append(additional_eos_token_id)\n",
    "    has_default_max_length = kwargs.get(\n",
    "        'max_length') is None and generation_config.max_length is not None\n",
    "    if has_default_max_length and generation_config.max_new_tokens is None:\n",
    "        warnings.warn(\n",
    "            f\"Using 'max_length''s default ({repr(generation_config.max_length)}) \\\n",
    "                to control the generation length. \"\n",
    "            'This behaviour is deprecated and will be removed from the \\\n",
    "                config in v5 of Transformers -- we'\n",
    "            ' recommend using `max_new_tokens` to control the maximum \\\n",
    "                length of the generation.',\n",
    "            UserWarning,\n",
    "        )\n",
    "    elif generation_config.max_new_tokens is not None:\n",
    "        generation_config.max_length = generation_config.max_new_tokens + \\\n",
    "            input_ids_seq_length\n",
    "        if not has_default_max_length:\n",
    "            logger.warn(  # pylint: disable=W4902\n",
    "                f\"Both 'max_new_tokens' (={generation_config.max_new_tokens}) \"\n",
    "                f\"and 'max_length'(={generation_config.max_length}) seem to \"\n",
    "                \"have been set. 'max_new_tokens' will take precedence. \"\n",
    "                'Please refer to the documentation for more information. '\n",
    "                '(https://huggingface.co/docs/transformers/main/'\n",
    "                'en/main_classes/text_generation)',\n",
    "                UserWarning,\n",
    "            )\n",
    "\n",
    "    if input_ids_seq_length >= generation_config.max_length:\n",
    "        input_ids_string = 'input_ids'\n",
    "        logger.warning(\n",
    "            f\"Input length of {input_ids_string} is {input_ids_seq_length}, \"\n",
    "            f\"but 'max_length' is set to {generation_config.max_length}. \"\n",
    "            'This can lead to unexpected behavior. You should consider'\n",
    "            \" increasing 'max_new_tokens'.\")\n",
    "\n",
    "    # 2. Set generation parameters if not already defined\n",
    "    logits_processor = logits_processor if logits_processor is not None \\\n",
    "        else LogitsProcessorList()\n",
    "    stopping_criteria = stopping_criteria if stopping_criteria is not None \\\n",
    "        else StoppingCriteriaList()\n",
    "\n",
    "    logits_processor = model._get_logits_processor(\n",
    "        generation_config=generation_config,\n",
    "        input_ids_seq_length=input_ids_seq_length,\n",
    "        encoder_input_ids=input_ids,\n",
    "        prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "        logits_processor=logits_processor,\n",
    "    )\n",
    "\n",
    "    stopping_criteria = model._get_stopping_criteria(\n",
    "        generation_config=generation_config,\n",
    "        stopping_criteria=stopping_criteria)\n",
    "    logits_warper = model._get_logits_warper(generation_config)\n",
    "\n",
    "    unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)\n",
    "    scores = None\n",
    "    while True:\n",
    "        model_inputs = model.prepare_inputs_for_generation(\n",
    "            input_ids, **model_kwargs)\n",
    "        # forward pass to get next token\n",
    "        outputs = model(\n",
    "            **model_inputs,\n",
    "            return_dict=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # pre-process distribution\n",
    "        next_token_scores = logits_processor(input_ids, next_token_logits)\n",
    "        next_token_scores = logits_warper(input_ids, next_token_scores)\n",
    "\n",
    "        # sample\n",
    "        probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
    "        if generation_config.do_sample:\n",
    "            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "        else:\n",
    "            next_tokens = torch.argmax(probs, dim=-1)\n",
    "\n",
    "        # update generated ids, model inputs, and length for next step\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "        model_kwargs = model._update_model_kwargs_for_generation(\n",
    "            outputs, model_kwargs, is_encoder_decoder=False)\n",
    "        unfinished_sequences = unfinished_sequences.mul(\n",
    "            (min(next_tokens != i for i in eos_token_id)).long())\n",
    "\n",
    "        output_token_ids = input_ids[0].cpu().tolist()\n",
    "        output_token_ids = output_token_ids[input_length:]\n",
    "        for each_eos_token_id in eos_token_id:\n",
    "            if output_token_ids[-1] == each_eos_token_id:\n",
    "                output_token_ids = output_token_ids[:-1]\n",
    "        response = tokenizer.decode(output_token_ids)\n",
    "\n",
    "        yield response\n",
    "        # stop when each sentence is finished\n",
    "        # or if we exceed the maximum length\n",
    "        if unfinished_sequences.max() == 0 or stopping_criteria(\n",
    "                input_ids, scores):\n",
    "            break\n",
    "\n",
    "\n",
    "def on_btn_click():\n",
    "    del st.session_state.messages\n",
    "\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    model = (AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                                  trust_remote_code=True).to(\n",
    "                                                      torch.bfloat16).cuda())\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\n",
    "                                              trust_remote_code=True)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def prepare_generation_config():\n",
    "    with st.sidebar:\n",
    "        max_length = st.slider('Max Length',\n",
    "                               min_value=8,\n",
    "                               max_value=32768,\n",
    "                               value=2048)\n",
    "        top_p = st.slider('Top P', 0.0, 1.0, 0.75, step=0.01)\n",
    "        temperature = st.slider('Temperature', 0.0, 1.0, 0.1, step=0.01)\n",
    "        st.button('Clear Chat History', on_click=on_btn_click)\n",
    "\n",
    "    generation_config = GenerationConfig(max_length=max_length,\n",
    "                                         top_p=top_p,\n",
    "                                         temperature=temperature)\n",
    "\n",
    "    return generation_config\n",
    "\n",
    "\n",
    "user_prompt = '<|im_start|>user\\n{user}<|im_end|>\\n'\n",
    "robot_prompt = '<|im_start|>assistant\\n{robot}<|im_end|>\\n'\n",
    "cur_query_prompt = '<|im_start|>user\\n{user}<|im_end|>\\n\\\n",
    "    <|im_start|>assistant\\n'\n",
    "\n",
    "\n",
    "def combine_history(prompt):\n",
    "    messages = st.session_state.messages\n",
    "    meta_instruction = ('')\n",
    "    total_prompt = f\"<s><|im_start|>system\\n{meta_instruction}<|im_end|>\\n\"\n",
    "    for message in messages:\n",
    "        cur_content = message['content']\n",
    "        if message['role'] == 'user':\n",
    "            cur_prompt = user_prompt.format(user=cur_content)\n",
    "        elif message['role'] == 'robot':\n",
    "            cur_prompt = robot_prompt.format(robot=cur_content)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "        total_prompt += cur_prompt\n",
    "    total_prompt = total_prompt + cur_query_prompt.format(user=prompt)\n",
    "    return total_prompt\n",
    "\n",
    "\n",
    "def main():\n",
    "    # torch.cuda.empty_cache()\n",
    "    print('load model begin.')\n",
    "    model, tokenizer = load_model()\n",
    "    print('load model end.')\n",
    "\n",
    "\n",
    "    st.title('InternLM2-Chat-1.8B')\n",
    "\n",
    "    generation_config = prepare_generation_config()\n",
    "\n",
    "    # Initialize chat history\n",
    "    if 'messages' not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "\n",
    "    # Display chat messages from history on app rerun\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message['role'], avatar=message.get('avatar')):\n",
    "            st.markdown(message['content'])\n",
    "\n",
    "    # Accept user input\n",
    "    if prompt := st.chat_input('What is up?'):\n",
    "        # Display user message in chat message container\n",
    "        with st.chat_message('user'):\n",
    "            st.markdown(prompt)\n",
    "        real_prompt = combine_history(prompt)\n",
    "        # Add user message to chat history\n",
    "        st.session_state.messages.append({\n",
    "            'role': 'user',\n",
    "            'content': prompt,\n",
    "        })\n",
    "\n",
    "        with st.chat_message('robot'):\n",
    "            message_placeholder = st.empty()\n",
    "            for cur_response in generate_interactive(\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    prompt=real_prompt,\n",
    "                    additional_eos_token_id=92542,\n",
    "                    **asdict(generation_config),\n",
    "            ):\n",
    "                # Display robot response in chat message container\n",
    "                message_placeholder.markdown(cur_response + 'â–Œ')\n",
    "            message_placeholder.markdown(cur_response)\n",
    "        # Add robot response to chat history\n",
    "        st.session_state.messages.append({\n",
    "            'role': 'robot',\n",
    "            'content': cur_response,  # pylint: disable=undefined-loop-variable\n",
    "        })\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f312e9d5",
   "metadata": {},
   "source": [
    "ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥å¯åŠ¨åº”ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01401d9",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!streamlit run streamlit_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e537d",
   "metadata": {},
   "source": [
    "è¿è¡Œåï¼Œåœ¨è®¿é—®å‰ï¼Œæˆ‘ä»¬è¿˜éœ€è¦åšçš„å°±æ˜¯å°†ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ã€‚\n",
    "\n",
    "é€šè¿‡å¦‚å›¾æ‰€ç¤ºçš„åœ°æ–¹ï¼Œè·å–å¼€å‘æœºçš„ç«¯å£å’Œå¯†ç ã€‚\n",
    "\n",
    "![](images/image-09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9db84f",
   "metadata": {},
   "source": [
    "ç„¶ååœ¨æœ¬åœ°ä½¿ç”¨ PowerShell æˆ–è€…å‘½ä»¤è¡Œç»ˆç«¯ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\n",
    "\n",
    "> å…¶ä¸­ï¼Œ`8501`æ˜¯Streamlitç¨‹åºçš„æœåŠ¡ç«¯å£ï¼Œ`40471`éœ€è¦æ›¿æ¢ä¸ºè‡ªå·±çš„å¼€å‘æœºçš„ç«¯å£ã€‚\n",
    "\n",
    "```bash\n",
    "ssh -CNg -L 8501:127.0.0.1:8501 root@ssh.intern-ai.org.cn -p 40471\n",
    "```\n",
    "\n",
    "ç„¶åå†è¾“å…¥å¼€å‘æœºçš„rootå¯†ç ã€‚\n",
    "\n",
    "![](images/image-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cd9dd4",
   "metadata": {},
   "source": [
    "æœ€åï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨æœ¬åœ°é€šè¿‡æµè§ˆå™¨è®¿é—®ï¼šhttp://127.0.0.1:8501 æ¥è¿›è¡Œå¯¹è¯äº†ã€‚\n",
    "\n",
    "![](images/image-12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150db94f",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "ç»è¿‡æœ¬èŠ‚çš„å­¦ä¹ ï¼Œå¸¦é¢†ç€å¤§å®¶è·‘é€šäº† XTuner çš„å®Œæ•´æµç¨‹ï¼Œæˆ‘ä»¬å­¦ä¼šäº†å¢é‡é¢„è®­ç»ƒå¾®è°ƒã€æŒ‡ä»¤è·Ÿéšå¾®è°ƒã€å¤šå¡å¾®è°ƒå’Œåˆ†å¸ƒå¼å¾®è°ƒï¼Œå¹¶ä¸”è®­ç»ƒå‡ºäº†ä¸€ä¸ªè‡ªå·±å°åŠ©æ‰‹ï¼Œæ˜¯ä¸æ˜¯å¾ˆæœ‰æ„æ€ï¼\n",
    "\n",
    "æ˜¯ä¸æ˜¯æ„Ÿè§‰å…¶å®å¾®è°ƒä¹Ÿä¸è¿‡å¦‚æ­¤ï¼äº‹å®ä¸Šç¡®å®æ˜¯è¿™æ ·çš„ï¼å…¶å®åœ¨å¾®è°ƒçš„æ—¶å€™æœ€é‡è¦çš„è¿˜æ˜¯è¦è‡ªå·±å‡†å¤‡ä¸€ä»½é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œè¿™ä¸ªæ‰æ˜¯ä½ èƒ½å¦çœŸå¾®è°ƒå‡ºæ•ˆæœæœ€æ ¸å¿ƒçš„åˆ©å™¨ã€‚\n",
    "\n",
    "å½“æˆ‘ä»¬åœ¨æµ‹è¯•å®Œæ¨¡å‹è®¤ä¸ºå…¶æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚åï¼Œå°±å¯ä»¥å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–éƒ¨ç½²ç­‰æ“ä½œäº†ï¼Œè¿™éƒ¨åˆ†çš„å†…å®¹åœ¨ä¹‹åå…³äº LMDeploy çš„è¯¾ç¨‹ä¸­å°†ä¼šè¯¦ç»†çš„è¿›è¡Œè®²è§£ï¼Œæ•¬è¯·æœŸå¾…åç»­çš„è¯¾ç¨‹å§ï¼\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
