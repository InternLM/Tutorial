{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zugbHBO8zAYz"
      },
      "outputs": [],
      "source": [
        "# 左边能显示的目录是/content，在里面创建一个名为temp的工作目录\n",
        "%cd /content\n",
        "!mkdir temp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 下载源码\n",
        "%cd /content/temp\n",
        "!git clone https://github.com/internlm/InternLM.git"
      ],
      "metadata": {
        "id": "CGX5Zbn_0HaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 从hf下载InternLM2-Chat-1.8B 模型到自建的/content/model路径（备用）\n",
        "# %cd /content\n",
        "# !mkdir model\n",
        "# !pip install huggingface-hub==0.17.3\n",
        "# !huggingface-cli download --resume-download internlm/internlm2-chat-1_8b --local-dir /content/model\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4HMpIADt1EFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 安装依赖\n",
        "# 由于colab一旦关机，工作目录和系统目录里文件都会删除，所以没必要在工作目录里创建一个虚拟环境\n",
        "# 因此直接在系统环境里安装InternLM2-Chat-1.8B 模型所需依赖\n",
        "\n",
        "# !pip install transformers==4.34 # 系统自带transformers-4.41.2能正常运行该模型，强行更换版本要restart session一次，不优雅\n",
        "# !pip install psutil==5.9.8 #系统环境已经有5.9.5版本的，且5.9.5版本能正常运行该项目，升级要restart session\n",
        "!pip install accelerate==0.24.1\n",
        "!pip install streamlit==1.32.2\n",
        "# !pip install matplotlib==3.8.3 # 系统环境已经有matplotlib==2.1.5，升级要restart session\n",
        "!pip install modelscope==1.9.5\n",
        "!pip install sentencepiece==0.1.99"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hncGIJgRzRDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 下载对话模型，运行后在命令行里进行对话\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "model_name_or_path = \"internlm/internlm2-chat-1_8b\"\n",
        "# 直接填写hf的相应模型仓库名，自动下载模型到InternLM源代码的模型存放目录\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, device_map='cuda:0')\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='cuda:0')\n",
        "model = model.eval()\n",
        "\n",
        "system_prompt = \"\"\"You are an AI assistant whose name is InternLM (书生·浦语).\n",
        "- InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.\n",
        "- InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.\n",
        "\"\"\"\n",
        "\n",
        "messages = [(system_prompt, '')]\n",
        "\n",
        "print(\"=============Welcome to InternLM chatbot, type 'exit' to exit.=============\")\n",
        "\n",
        "while True:\n",
        "    input_text = input(\"\\nUser  >>> \")\n",
        "    input_text = input_text.replace(' ', '')\n",
        "    if input_text == \"exit\":\n",
        "        break\n",
        "\n",
        "    length = 0\n",
        "    for response, _ in model.stream_chat(tokenizer, input_text, messages):\n",
        "        if response is not None:\n",
        "            print(response[length:], flush=True, end=\"\")\n",
        "            length = len(response)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YaPYP2Yoz2iC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1使用ngrok进行内网穿透，用它帮助我们用自己笔记本来访问colab本地的127.0.0.1:6006\n",
        "!pip install pyngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 此处字符串里的token就是ngrok的个人密钥\n",
        "# 去https://ngrok.com/注册后，左栏有个Your Authtoken，复制了粘贴在下方即可\n",
        "YOUR_NGROK_AUTHTOKEN = \"2ZDjz2GRqYOh66SkdzOjF8BmtIJ_2QyLsvabaLwvni1c7qKqU\"\n",
        "\n",
        "ngrok.set_auth_token(YOUR_NGROK_AUTHTOKEN)\n",
        "\n",
        "# 完成colab端口的穿透，可以在公网访问colab本地的6006端口\n",
        "public_url = ngrok.connect(6006)\n",
        "print(\"Tunnel URL:\", public_url)\n"
      ],
      "metadata": {
        "id": "wqKy7IWgAKXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HvfB2iu5Hm0X"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.2 web demo里进行模型对话\n",
        "%cd /content/temp/InternLM/chat\n",
        "# web_demo.py里默认用7b的模型，需要替换为1.8b的模型，title也替换为1.8b\n",
        "!sed -i 's/internlm2-chat-7b/internlm2-chat-1_8b/g' /content/temp/InternLM/chat/web_demo.py\n",
        "!sed -i 's/InternLM2-Chat-7B/InternLM2-Chat-1.8B/g' /content/temp/InternLM/chat/web_demo.py\n",
        "\n",
        "#相对路径替换为绝对路径，解决No such file or directory: 'assets/user.png'的问题\n",
        "!sed -i \"s#user_avator = 'assets/user.png'#user_avator = '/content/temp/InternLM/assets/user.png'#g\" /content/temp/InternLM/chat/web_demo.py\n",
        "!sed -i \"s#robot_avator = 'assets/robot.png'#robot_avator = '/content/temp/InternLM/assets/robot.png'#g\" /content/temp/InternLM/chat/web_demo.py\n",
        "\n",
        "!streamlit run web_demo.py --server.address 127.0.0.1 --server.port 6006\n",
        "# 等下方出现URL: http://127.0.0.1:6006之后，点击上方的Tunnel URL: NgrokTunnel:\"https://xxxxx.app\"里的网址，\n",
        "# 然后点击Visit site进入web demo的网页，之后等待模型加载完成，即可开始对话"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Wuui0TuA2KFT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}