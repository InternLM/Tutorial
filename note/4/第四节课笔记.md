
## 1. 前言

前面的课程学习完外挂知识库后，本节课学习模型微调

## 2. 概述

### 1.1 XTuner

一个大语言模型微调工具箱。由 MMRazor 和 MMDeploy 联合开发。

### 1.2 微调原理

想象一下，你有一个超大的玩具，现在你想改造这个超大的玩具。但是，对整个玩具进行全面的改动会非常昂贵。因此，你找到了一种叫 LoRA 的方法：只对玩具中的某些零件进行改动，而不是对整个玩具进行全面改动而 QLoRA 是 LoRA 的一种改进：如果你手里只有一把生锈的螺丝刀，也能改造你的玩具。

LoRA论文链接: http://arxiv.org/abs/2106.09685
QLoRA论文链接: http://arxiv.org/abs/2305.14314

## 3. 开发流程中

重点在于教程2.3.6中的两个概念：

- 将得到的 PTH 模型转换为 HuggingFace 模型，即：生成 Adapter 文件夹
- 此时，HuggingFace 模型文件夹即为我们平时所理解的所谓 “LoRA 模型文件”，可以简单理解：LoRA 模型文件 = Adapter