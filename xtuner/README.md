![m3qx](imgs/head2.png)

> æ€ä¹ˆç¡•å‘¢ï¼Œç¥å¤§å®¶ç‚¼ä¸¹æ„‰å¿«å§~ ğŸ˜™

## 1 æ¦‚è¿°

### 1.1 XTuner

ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹å¾®è°ƒå·¥å…·ç®±ã€‚*ç”±* *MMRazor* *å’Œ* *MMDeploy* *è”åˆå¼€å‘ã€‚*

### 1.2 æ”¯æŒçš„å¼€æºLLM (2023.11.01)

- **[InternLM](https://huggingface.co/internlm/internlm-7b)** âœ…
- [Llamaï¼ŒLlama2](https://huggingface.co/meta-llama)
- [ChatGLM2](https://huggingface.co/THUDM/chatglm2-6b)ï¼Œ[ChatGLM3](https://huggingface.co/THUDM/chatglm3-6b-base)
- [Qwen](https://huggingface.co/Qwen/Qwen-7B)
- [Baichuan](https://huggingface.co/baichuan-inc/Baichuan-7B)ï¼Œ[Baichuan2](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base)
- ......
- [Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) 

### 1.3 ç‰¹è‰² 

- ğŸ¤“ **å‚»ç“œåŒ–ï¼š** ä»¥ é…ç½®æ–‡ä»¶ çš„å½¢å¼å°è£…äº†å¤§éƒ¨åˆ†å¾®è°ƒåœºæ™¯ï¼Œ**0åŸºç¡€çš„éä¸“ä¸šäººå‘˜ä¹Ÿèƒ½ä¸€é”®å¼€å§‹å¾®è°ƒ**ã€‚
- ğŸƒ **è½»é‡çº§ï¼š** å¯¹äº 7B å‚æ•°é‡çš„LLMï¼Œ**å¾®è°ƒæ‰€éœ€çš„æœ€å°æ˜¾å­˜ä»…ä¸º 8GB** ï¼š **æ¶ˆè´¹çº§æ˜¾å¡âœ…ï¼Œcolabâœ…**

### 1.4 å¾®è°ƒåŸç†

> æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æœ‰ä¸€ä¸ªè¶…å¤§çš„ç©å…·ï¼Œç°åœ¨ä½ æƒ³æ”¹é€ è¿™ä¸ªè¶…å¤§çš„ç©å…·ã€‚ä½†æ˜¯ï¼Œ**å¯¹æ•´ä¸ªç©å…·è¿›è¡Œå…¨é¢çš„æ”¹åŠ¨ä¼šéå¸¸æ˜‚è´µ**ã€‚

â€» å› æ­¤ï¼Œä½ æ‰¾åˆ°äº†ä¸€ç§å« **LoRA** çš„æ–¹æ³•ï¼š**åªå¯¹ç©å…·ä¸­çš„æŸäº›é›¶ä»¶è¿›è¡Œæ”¹åŠ¨ï¼Œè€Œä¸æ˜¯å¯¹æ•´ä¸ªç©å…·è¿›è¡Œå…¨é¢æ”¹åŠ¨**ã€‚

â€» è€Œ **QLoRA** æ˜¯ LoRA çš„ä¸€ç§æ”¹è¿›ï¼šå¦‚æœä½ æ‰‹é‡Œåªæœ‰ä¸€æŠŠç”Ÿé”ˆçš„èºä¸åˆ€ï¼Œä¹Ÿèƒ½æ”¹é€ ä½ çš„ç©å…·ã€‚

- **Full** :       ğŸ˜³ â†’ ğŸš²
- **[LoRA](http://arxiv.org/abs/2106.09685)** :     ğŸ˜³ â†’ ğŸ›µ
- **[QLoRA](http://arxiv.org/abs/2305.14314)** :   ğŸ˜³ â†’ ğŸ

![WOZJXUtaKlEk9S4.png](imgs/cat_fly.png)


## 2 å¿«é€Ÿä¸Šæ‰‹

### 2.1 å¹³å°

Ubuntu + Anaconda + CUDA/CUDNN + 8GB nvidiaæ˜¾å¡

### 2.2 å®‰è£…

```bash
# å¦‚æœä½ æ˜¯åœ¨ InternStudio å¹³å°ï¼Œåˆ™ä»æœ¬åœ° clone ä¸€ä¸ªå·²æœ‰ pytorch 2.0.1 çš„ç¯å¢ƒï¼š
/root/share/install_conda_env_internlm_base.sh xtuner0.1.9
# å¦‚æœä½ æ˜¯åœ¨å…¶ä»–å¹³å°ï¼š
conda create --name xtuner0.1.9 python=3.10 -y

# æ¿€æ´»ç¯å¢ƒ
conda activate xtuner0.1.9
# è¿›å…¥å®¶ç›®å½• ï¼ˆ~çš„æ„æ€æ˜¯ â€œå½“å‰ç”¨æˆ·çš„homeè·¯å¾„â€ï¼‰
cd ~
# åˆ›å»ºç‰ˆæœ¬æ–‡ä»¶å¤¹å¹¶è¿›å…¥ï¼Œä»¥è·Ÿéšæœ¬æ•™ç¨‹
mkdir xtuner019 && cd xtuner019


# æ‹‰å– 0.1.9 çš„ç‰ˆæœ¬æºç 
git clone -b v0.1.9  https://github.com/InternLM/xtuner
# æ— æ³•è®¿é—®githubçš„ç”¨æˆ·è¯·ä» gitee æ‹‰å–:
# git clone -b v0.1.9 https://gitee.com/Internlm/xtuner

# è¿›å…¥æºç ç›®å½•
cd xtuner

# ä»æºç å®‰è£… XTuner
pip install -e '.[all]'
```

å®‰è£…å®Œåï¼Œå°±å¼€å§‹ææå‡†å¤‡å·¥ä½œäº†ã€‚ï¼ˆå‡†å¤‡åœ¨ oasst1 æ•°æ®é›†ä¸Šå¾®è°ƒ internlm-7b-chatï¼‰

```bash
# åˆ›å»ºä¸€ä¸ªå¾®è°ƒ oasst1 æ•°æ®é›†çš„å·¥ä½œè·¯å¾„ï¼Œè¿›å…¥
mkdir ~/ft-oasst1 && cd ~/ft-oasst1
```

### 2.3 å¾®è°ƒ

#### 2.3.1 å‡†å¤‡é…ç½®æ–‡ä»¶

XTuner æä¾›å¤šä¸ªå¼€ç®±å³ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä¸‹åˆ—å‘½ä»¤æŸ¥çœ‹ï¼š

```Bash
# åˆ—å‡ºæ‰€æœ‰å†…ç½®é…ç½®
xtuner list-cfg
```
> å‡å¦‚æ˜¾ç¤ºbash: xtuner: command not foundçš„è¯å¯ä»¥è€ƒè™‘åœ¨ç»ˆç«¯è¾“å…¥ export PATH=$PATH:'/root/.local/bin'

![QCgmlv1VpU3fZPk.png](imgs/cfgs.png)

æ‹·è´ä¸€ä¸ªé…ç½®æ–‡ä»¶åˆ°å½“å‰ç›®å½•ï¼š
`# xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}`

åœ¨æœ¬æ¡ˆä¾‹ä¸­å³ï¼šï¼ˆæ³¨æ„æœ€åæœ‰ä¸ªè‹±æ–‡å¥å·ï¼Œä»£è¡¨å¤åˆ¶åˆ°å½“å‰è·¯å¾„ï¼‰
```Bash
cd ~/ft-oasst1
xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .
```

é…ç½®æ–‡ä»¶åçš„è§£é‡Šï¼š

> xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .

| æ¨¡å‹å   | internlm_chat_7b |
| -------- | ---------------- |
| ä½¿ç”¨ç®—æ³• | qlora            |
| æ•°æ®é›†   | oasst1           |
| æŠŠæ•°æ®é›†è·‘å‡ æ¬¡    | è·‘3æ¬¡ï¼še3 (epoch 3 )   |

*æ—  chatæ¯”å¦‚ `internlm-7b` ä»£è¡¨æ˜¯åŸºåº§(base)æ¨¡å‹


#### 2.3.2 æ¨¡å‹ä¸‹è½½

> ç”±äºä¸‹è½½æ¨¡å‹å¾ˆæ…¢ï¼Œç”¨æ•™å­¦å¹³å°çš„åŒå­¦å¯ä»¥ç›´æ¥å¤åˆ¶æ¨¡å‹ã€‚

```Bash
cp -r /root/share/temp/model_repos/internlm-chat-7b ~/ft-oasst1/
```

> ä»¥ä¸‹æ˜¯è‡ªå·±ä¸‹è½½æ¨¡å‹çš„æ­¥éª¤ã€‚

ä¸ç”¨ xtuner é»˜è®¤çš„`ä» huggingface æ‹‰å–æ¨¡å‹`ï¼Œè€Œæ˜¯æå‰ä» ~~OpenXLab~~ ModelScope ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°

```Bash
# åˆ›å»ºä¸€ä¸ªç›®å½•ï¼Œæ”¾æ¨¡å‹æ–‡ä»¶ï¼Œé˜²æ­¢æ•£è½ä¸€åœ°
mkdir ~/ft-oasst1/internlm-chat-7b

# è£…ä¸€ä¸‹æ‹‰å–æ¨¡å‹æ–‡ä»¶è¦ç”¨çš„åº“
pip install modelscope

# ä» modelscope ä¸‹è½½ä¸‹è½½æ¨¡å‹æ–‡ä»¶
cd ~/ft-oasst1
apt install git git-lfs -y
git lfs install
git lfs clone https://modelscope.cn/Shanghai_AI_Laboratory/internlm-chat-7b.git -b v1.0.3
```

#### 2.3.3 æ•°æ®é›†ä¸‹è½½
> https://huggingface.co/datasets/timdettmers/openassistant-guanaco/tree/main

ç”±äº huggingface ç½‘ç»œé—®é¢˜ï¼Œå’±ä»¬å·²ç»ç»™å¤§å®¶æå‰ä¸‹è½½å¥½äº†ï¼Œå¤åˆ¶åˆ°æ­£ç¡®ä½ç½®å³å¯ï¼š

```bash
cd ~/ft-oasst1
# ...-guanaco åé¢æœ‰ä¸ªç©ºæ ¼å’Œè‹±æ–‡å¥å·å•Š
cp -r /root/share/temp/datasets/openassistant-guanaco .
```

æ­¤æ—¶ï¼Œå½“å‰è·¯å¾„çš„æ–‡ä»¶åº”è¯¥é•¿è¿™æ ·ï¼š

```bash
|-- internlm-chat-7b
|   |-- README.md
|   |-- config.json
|   |-- configuration.json
|   |-- configuration_internlm.py
|   |-- generation_config.json
|   |-- modeling_internlm.py
|   |-- pytorch_model-00001-of-00008.bin
|   |-- pytorch_model-00002-of-00008.bin
|   |-- pytorch_model-00003-of-00008.bin
|   |-- pytorch_model-00004-of-00008.bin
|   |-- pytorch_model-00005-of-00008.bin
|   |-- pytorch_model-00006-of-00008.bin
|   |-- pytorch_model-00007-of-00008.bin
|   |-- pytorch_model-00008-of-00008.bin
|   |-- pytorch_model.bin.index.json
|   |-- special_tokens_map.json
|   |-- tokenization_internlm.py
|   |-- tokenizer.model
|   `-- tokenizer_config.json
|-- internlm_chat_7b_qlora_oasst1_e3_copy.py
`-- openassistant-guanaco
    |-- openassistant_best_replies_eval.jsonl
    `-- openassistant_best_replies_train.jsonl
```

#### 2.3.4 ä¿®æ”¹é…ç½®æ–‡ä»¶

ä¿®æ”¹å…¶ä¸­çš„æ¨¡å‹å’Œæ•°æ®é›†ä¸º æœ¬åœ°è·¯å¾„

```bash
cd ~/ft-oasst1
vim internlm_chat_7b_qlora_oasst1_e3_copy.py
```
> åœ¨vimç•Œé¢å®Œæˆä¿®æ”¹åï¼Œè¯·è¾“å…¥:wqé€€å‡ºã€‚å‡å¦‚è®¤ä¸ºæ”¹é”™äº†å¯ä»¥ç”¨:q!é€€å‡ºä¸”ä¸ä¿å­˜ã€‚å½“ç„¶æˆ‘ä»¬ä¹Ÿå¯ä»¥è€ƒè™‘æ‰“å¼€pythonæ–‡ä»¶ç›´æ¥ä¿®æ”¹ï¼Œä½†æ³¨æ„ä¿®æ”¹å®Œåéœ€è¦æŒ‰ä¸‹Ctrl+Sè¿›è¡Œä¿å­˜ã€‚

å‡å·ä»£è¡¨è¦åˆ é™¤çš„è¡Œï¼ŒåŠ å·ä»£è¡¨è¦å¢åŠ çš„è¡Œã€‚
```diff
# ä¿®æ”¹æ¨¡å‹ä¸ºæœ¬åœ°è·¯å¾„
- pretrained_model_name_or_path = 'internlm/internlm-chat-7b'
+ pretrained_model_name_or_path = './internlm-chat-7b'

# ä¿®æ”¹è®­ç»ƒæ•°æ®é›†ä¸ºæœ¬åœ°è·¯å¾„
- data_path = 'timdettmers/openassistant-guanaco'
+ data_path = './openassistant-guanaco'
```

**å¸¸ç”¨è¶…å‚**

| å‚æ•°å | è§£é‡Š |
| ------------------- | ------------------------------------------------------ |
| **data_path**       | æ•°æ®è·¯å¾„æˆ– HuggingFace ä»“åº“å                          |
| max_length          | å•æ¡æ•°æ®æœ€å¤§ Token æ•°ï¼Œè¶…è¿‡åˆ™æˆªæ–­                      |
| pack_to_max_length  | æ˜¯å¦å°†å¤šæ¡çŸ­æ•°æ®æ‹¼æ¥åˆ° max_lengthï¼Œæé«˜ GPU åˆ©ç”¨ç‡     |
| accumulative_counts | æ¢¯åº¦ç´¯ç§¯ï¼Œæ¯å¤šå°‘æ¬¡ backward æ›´æ–°ä¸€æ¬¡å‚æ•°               |
| evaluation_inputs   | è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¼šæ ¹æ®ç»™å®šçš„é—®é¢˜è¿›è¡Œæ¨ç†ï¼Œä¾¿äºè§‚æµ‹è®­ç»ƒçŠ¶æ€ |
| evaluation_freq     | Evaluation çš„è¯„æµ‹é—´éš” iter æ•°                          |
| ...... | ...... |

> å¦‚æœæƒ³æŠŠæ˜¾å¡çš„ç°å­˜åƒæ»¡ï¼Œå……åˆ†åˆ©ç”¨æ˜¾å¡èµ„æºï¼Œå¯ä»¥å°† `max_length` å’Œ `batch_size` è¿™ä¸¤ä¸ªå‚æ•°è°ƒå¤§ã€‚

#### 2.3.5 å¼€å§‹å¾®è°ƒ

**è®­ç»ƒï¼š**

xtuner train ${CONFIG_NAME_OR_PATH}

**ä¹Ÿå¯ä»¥å¢åŠ  deepspeed è¿›è¡Œè®­ç»ƒåŠ é€Ÿï¼š**

xtuner train ${CONFIG_NAME_OR_PATH} --deepspeed deepspeed_zero2


ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ QLoRA ç®—æ³•åœ¨ oasst1 æ•°æ®é›†ä¸Šå¾®è°ƒ InternLM-7Bï¼š

```Bash
# å•å¡
## ç”¨åˆšæ‰æ”¹å¥½çš„configæ–‡ä»¶è®­ç»ƒ
xtuner train ./internlm_chat_7b_qlora_oasst1_e3_copy.py

# å¤šå¡
NPROC_PER_NODE=${GPU_NUM} xtuner train ./internlm_chat_7b_qlora_oasst1_e3_copy.py

# è‹¥è¦å¼€å¯ deepspeed åŠ é€Ÿï¼Œå¢åŠ  --deepspeed deepspeed_zero2 å³å¯
```

> å¾®è°ƒå¾—åˆ°çš„ PTH æ¨¡å‹æ–‡ä»¶å’Œå…¶ä»–æ‚ä¸ƒæ‚å…«çš„æ–‡ä»¶éƒ½é»˜è®¤åœ¨å½“å‰çš„ `./work_dirs` ä¸­ã€‚

è·‘å®Œè®­ç»ƒåï¼Œå½“å‰è·¯å¾„åº”è¯¥é•¿è¿™æ ·ï¼š
```Bash
|-- internlm-chat-7b
|-- internlm_chat_7b_qlora_oasst1_e3_copy.py
|-- openassistant-guanaco
|   |-- openassistant_best_replies_eval.jsonl
|   `-- openassistant_best_replies_train.jsonl
`-- work_dirs
    `-- internlm_chat_7b_qlora_oasst1_e3_copy
        |-- 20231101_152923
        |   |-- 20231101_152923.log
        |   `-- vis_data
        |       |-- 20231101_152923.json
        |       |-- config.py
        |       `-- scalars.json
        |-- epoch_1.pth
        |-- epoch_2.pth
        |-- epoch_3.pth
        |-- internlm_chat_7b_qlora_oasst1_e3_copy.py
        `-- last_checkpoint
```

#### 2.3.6 å°†å¾—åˆ°çš„ PTH æ¨¡å‹è½¬æ¢ä¸º HuggingFace æ¨¡å‹ï¼Œ**å³ï¼šç”Ÿæˆ Adapter æ–‡ä»¶å¤¹**

`xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH_file_dir} ${SAVE_PATH}`

åœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œä¸ºï¼š
```bash
mkdir hf
export MKL_SERVICE_FORCE_INTEL=1
export MKL_THREADING_LAYER=GNU
xtuner convert pth_to_hf ./internlm_chat_7b_qlora_oasst1_e3_copy.py ./work_dirs/internlm_chat_7b_qlora_oasst1_e3_copy/epoch_1.pth ./hf
```
æ­¤æ—¶ï¼Œè·¯å¾„ä¸­åº”è¯¥é•¿è¿™æ ·ï¼š

```Bash
|-- internlm-chat-7b
|-- internlm_chat_7b_qlora_oasst1_e3_copy.py
|-- openassistant-guanaco
|   |-- openassistant_best_replies_eval.jsonl
|   `-- openassistant_best_replies_train.jsonl
|-- hf
|   |-- README.md
|   |-- adapter_config.json
|   |-- adapter_model.bin
|   `-- xtuner_config.py
`-- work_dirs
    `-- internlm_chat_7b_qlora_oasst1_e3_copy
        |-- 20231101_152923
        |   |-- 20231101_152923.log
        |   `-- vis_data
        |       |-- 20231101_152923.json
        |       |-- config.py
        |       `-- scalars.json
        |-- epoch_1.pth
        |-- epoch_2.pth
        |-- epoch_3.pth
        |-- internlm_chat_7b_qlora_oasst1_e3_copy.py
        `-- last_checkpoint
```

<span style="color: red;">**æ­¤æ—¶ï¼Œhf æ–‡ä»¶å¤¹å³ä¸ºæˆ‘ä»¬å¹³æ—¶æ‰€ç†è§£çš„æ‰€è°“ â€œLoRA æ¨¡å‹æ–‡ä»¶â€**</span>

> å¯ä»¥ç®€å•ç†è§£ï¼šLoRA æ¨¡å‹æ–‡ä»¶ = Adapter



### 2.4 éƒ¨ç½²ä¸æµ‹è¯•

#### 2.4.1 å°† HuggingFace adapter åˆå¹¶åˆ°å¤§è¯­è¨€æ¨¡å‹ï¼š

```Bash
xtuner convert merge ./internlm-chat-7b ./hf ./merged --max-shard-size 2GB
# xtuner convert merge \
#     ${NAME_OR_PATH_TO_LLM} \
#     ${NAME_OR_PATH_TO_ADAPTER} \
#     ${SAVE_PATH} \
#     --max-shard-size 2GB
```

#### 2.4.2 ä¸åˆå¹¶åçš„æ¨¡å‹å¯¹è¯ï¼š
```Bash
# åŠ è½½ Adapter æ¨¡å‹å¯¹è¯ï¼ˆFloat 16ï¼‰
xtuner chat ./merged --prompt-template internlm_chat

# 4 bit é‡åŒ–åŠ è½½
# xtuner chat ./merged --bits 4 --prompt-template internlm_chat
```

#### 2.4.3 Demo

- ä¿®æ”¹ `cli_demo.py` ä¸­çš„æ¨¡å‹è·¯å¾„
```diff
- model_name_or_path = "/root/model/Shanghai_AI_Laboratory/internlm-chat-7b"
+ model_name_or_path = "merged"
```
- è¿è¡Œ `cli_demo.py` ä»¥ç›®æµ‹å¾®è°ƒæ•ˆæœ
```bash
python ./cli_demo.py
```

**æ•ˆæœï¼š**

| å¾®è°ƒå‰ | å¾®è°ƒå |
| --- | --- |
| ![O23QD48iFSZMfbr.png](imgs/beforeFT.png) | ![L1sqmGgE6h2exWP.png](imgs/afterFT.png) |

**`xtuner chat`** **çš„å¯åŠ¨å‚æ•°**

| å¯åŠ¨å‚æ•°              | å¹²å“ˆæ»´                                                       |
| --------------------- | ------------------------------------------------------------ |
| **--prompt-template** | æŒ‡å®šå¯¹è¯æ¨¡æ¿                                                 |
| --system              | æŒ‡å®šSYSTEMæ–‡æœ¬                                               |
| --system-template     | æŒ‡å®šSYSTEMæ¨¡æ¿                                               |
| -**-bits**            | LLMä½æ•°                                                      |
| --bot-name            | botåç§°                                                      |
| --with-plugins        | æŒ‡å®šè¦ä½¿ç”¨çš„æ’ä»¶                                             |
| **--no-streamer**     | æ˜¯å¦å¯ç”¨æµå¼ä¼ è¾“                                             |
| **--lagent**          | æ˜¯å¦ä½¿ç”¨lagent                                               |
| --command-stop-word   | å‘½ä»¤åœæ­¢è¯                                                   |
| --answer-stop-word    | å›ç­”åœæ­¢è¯                                                   |
| --offload-folder      | å­˜æ”¾æ¨¡å‹æƒé‡çš„æ–‡ä»¶å¤¹ï¼ˆæˆ–è€…å·²ç»å¸è½½æ¨¡å‹æƒé‡çš„æ–‡ä»¶å¤¹ï¼‰         |
| --max-new-tokens      | ç”Ÿæˆæ–‡æœ¬ä¸­å…è®¸çš„æœ€å¤§ `token` æ•°é‡                                |
| **--temperature**     | æ¸©åº¦å€¼                                                       |
| --top-k               | ä¿ç•™ç”¨äºé¡¶kç­›é€‰çš„æœ€é«˜æ¦‚ç‡è¯æ±‡æ ‡è®°æ•°                          |
| --top-p               | å¦‚æœè®¾ç½®ä¸ºå°äº1çš„æµ®ç‚¹æ•°ï¼Œä»…ä¿ç•™æ¦‚ç‡ç›¸åŠ é«˜äº `top_p` çš„æœ€å°ä¸€ç»„æœ€æœ‰å¯èƒ½çš„æ ‡è®° |
| --seed                | ç”¨äºå¯é‡ç°æ–‡æœ¬ç”Ÿæˆçš„éšæœºç§å­                                 |



## 3 è‡ªå®šä¹‰å¾®è°ƒ
> ä»¥ **[Medication QA](https://github.com/abachaa/Medication_QA_MedInfo2019)** **æ•°æ®é›†**ä¸ºä¾‹

### 3.1 æ¦‚è¿°

#### 3.1.1 **åœºæ™¯éœ€æ±‚**

   åŸºäº InternLM-chat-7B æ¨¡å‹ï¼Œç”¨ MedQA æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œå°†å…¶å¾€`åŒ»å­¦é—®ç­”`é¢†åŸŸå¯¹é½ã€‚

#### 3.1.2 **çœŸå®æ•°æ®é¢„è§ˆ**

| é—®é¢˜                                                       | ç­”æ¡ˆ                                                         |
| ---------------------------------------------------------- | ------------------------------------------------------------ |
| What are ketorolac eye drops?ï¼ˆä»€ä¹ˆæ˜¯é…®å’¯é…¸æ»´çœ¼æ¶²ï¼Ÿï¼‰      | Ophthalmic   ketorolac is used to treat itchy eyes caused by allergies. It also is used to   treat swelling and redness (inflammation) that can occur after cataract   surgery. Ketorolac is in a class of medications called nonsteroidal   anti-inflammatory drugs (NSAIDs). It works by stopping the release of   substances that cause allergy symptoms and inflammation. |
| What medicines raise blood sugar? ï¼ˆä»€ä¹ˆè¯ç‰©ä¼šå‡é«˜è¡€ç³–ï¼Ÿï¼‰ | Some   medicines for conditions other than diabetes can raise your blood sugar   level. This is a concern when you have diabetes. Make sure every doctor you   see knows about all of the medicines, vitamins, or herbal supplements you   take. This means anything you take with or without a prescription. Examples include:     Barbiturates.     Thiazide diuretics.     Corticosteroids.     Birth control pills (oral contraceptives) and progesterone.     Catecholamines.     Decongestants that contain beta-adrenergic agents, such as pseudoephedrine.     The B vitamin niacin. The risk of high blood sugar from niacin lowers after you have taken it for a few months. The antipsychotic medicine olanzapine (Zyprexa). |

### 3.2 æ•°æ®å‡†å¤‡ 

> **ä»¥** **[Medication QA](https://github.com/abachaa/Medication_QA_MedInfo2019)** **æ•°æ®é›†ä¸ºä¾‹**

**åŸæ ¼å¼ï¼š(.xlsx)**

![gjKLFUNWAx2dZDS.png](imgs/medqa2019samples.png)

| **é—®é¢˜** | è¯ç‰©ç±»å‹ | é—®é¢˜ç±»å‹ | **å›ç­”** | ä¸»é¢˜ | URL  |
| -------- | -------- | -------- | -------- | ---- | ---- |
| aaa      | bbb      | ccc      | ddd      | eee  | fff  |

#### 3.2.1 å°†æ•°æ®è½¬ä¸º XTuner çš„æ•°æ®æ ¼å¼

**ç›®æ ‡æ ¼å¼ï¼š(.jsonL)**

```JSON
[{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
},
{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
}]
```

ğŸ§ é€šè¿‡ pytho nè„šæœ¬ï¼šå°† `.xlsx` ä¸­çš„ é—®é¢˜ å’Œ å›ç­” ä¸¤åˆ— æå–å‡ºæ¥ï¼Œå†æ”¾å…¥ `.jsonL` æ–‡ä»¶çš„æ¯ä¸ª conversation çš„ input å’Œ output ä¸­ã€‚

> è¿™ä¸€æ­¥çš„ python è„šæœ¬å¯ä»¥è¯· ChatGPT æ¥å®Œæˆã€‚

```text
Write a python file for me. using openpyxl. input file name is MedQA2019.xlsx
Step1: The input file is .xlsx. Exact the column A and column D in the sheet named "DrugQA" .
Step2: Put each value in column A into each "input" of each "conversation". Put each value in column D into each "output" of each "conversation".
Step3: The output file is .jsonL. It looks like:
[{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
},
{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
}]
Step4: All "system" value changes to "You are a professional, highly experienced doctor professor. You always provide accurate, comprehensive, and detailed answers based on the patients' questions."
```

> ChatGPT ç”Ÿæˆçš„ python ä»£ç è§æœ¬ä»“åº“çš„ [xlsx2jsonl.py](./xlsx2jsonl.py)


æ‰§è¡Œ python è„šæœ¬ï¼Œè·å¾—æ ¼å¼åŒ–åçš„æ•°æ®é›†ï¼š
```bash
python xlsx2jsonl.py
```

**æ ¼å¼åŒ–åçš„æ•°æ®é›†é•¿è¿™æ ·ï¼š**
![uOCJXwbfzKRWSBE.png](imgs/dataProcessed.png)

æ­¤æ—¶ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥å¯¹æ•°æ®è¿›è¡Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ†å‰²ï¼ŒåŒæ ·å¯ä»¥è®© ChatGPT å†™ python ä»£ç ã€‚å½“ç„¶å¦‚æœä½ æ²¡æœ‰ä¸¥æ ¼çš„ç§‘ç ”éœ€æ±‚ã€ä¸åœ¨ä¹â€œè®­ç»ƒé›†æ³„éœ²â€çš„é—®é¢˜ï¼Œä¹Ÿå¯ä»¥ä¸åšè®­ç»ƒé›†ä¸æµ‹è¯•é›†çš„åˆ†å‰²ã€‚

#### 3.2.2 åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†

```text
my .jsonL file looks like:
[{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
},
{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
}]
Step1, read the .jsonL file.
Step2, count the amount of the "conversation" elements.
Step3, randomly split all "conversation" elements by 7:3. Targeted structure is same as the input.
Step4, save the 7/10 part as train.jsonl. save the 3/10 part as test.jsonl
```
ç”Ÿæˆçš„pythonä»£ç è§ [split2train_and_test.py](./split2train_and_test.py)


### 3.3 å¼€å§‹è‡ªå®šä¹‰å¾®è°ƒ

æ­¤æ—¶ï¼Œæˆ‘ä»¬é‡æ–°å»ºä¸€ä¸ªæ–‡ä»¶å¤¹æ¥ç©â€œå¾®è°ƒè‡ªå®šä¹‰æ•°æ®é›†â€
```bash
mkdir ~/ft-medqa && cd ~/ft-medqa
```

æŠŠå‰é¢ä¸‹è½½å¥½çš„internlm-chat-7bæ¨¡å‹æ–‡ä»¶å¤¹æ‹·è´è¿‡æ¥ã€‚

```bash
cp -r ~/ft-oasst1/internlm-chat-7b .
```
åˆ«å¿˜äº†æŠŠè‡ªå®šä¹‰æ•°æ®é›†ï¼Œå³å‡ ä¸ª `.jsonL`ï¼Œä¹Ÿä¼ åˆ°æœåŠ¡å™¨ä¸Šã€‚

```bash
git clone https://github.com/InternLM/tutorial
```

```bash
cp ~/tutorial/xtuner/MedQA2019-structured-train.jsonl .
```



#### 3.3.1 å‡†å¤‡é…ç½®æ–‡ä»¶
```bash
# å¤åˆ¶é…ç½®æ–‡ä»¶åˆ°å½“å‰ç›®å½•
xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .
# æ”¹ä¸ªæ–‡ä»¶å
mv internlm_chat_7b_qlora_oasst1_e3_copy.py internlm_chat_7b_qlora_medqa2019_e3.py

# ä¿®æ”¹é…ç½®æ–‡ä»¶å†…å®¹
vim internlm_chat_7b_qlora_medqa2019_e3.py
```

å‡å·ä»£è¡¨è¦åˆ é™¤çš„è¡Œï¼ŒåŠ å·ä»£è¡¨è¦å¢åŠ çš„è¡Œã€‚
```diff
# ä¿®æ”¹importéƒ¨åˆ†
- from xtuner.dataset.map_fns import oasst1_map_fn, template_map_fn_factory
+ from xtuner.dataset.map_fns import template_map_fn_factory

# ä¿®æ”¹æ¨¡å‹ä¸ºæœ¬åœ°è·¯å¾„
- pretrained_model_name_or_path = 'internlm/internlm-chat-7b'
+ pretrained_model_name_or_path = './internlm-chat-7b'

# ä¿®æ”¹è®­ç»ƒæ•°æ®ä¸º MedQA2019-structured-train.jsonl è·¯å¾„
- data_path = 'timdettmers/openassistant-guanaco'
+ data_path = 'MedQA2019-structured-train.jsonl'

# ä¿®æ”¹ train_dataset å¯¹è±¡
train_dataset = dict(
    type=process_hf_dataset,
-   dataset=dict(type=load_dataset, path=data_path),
+   dataset=dict(type=load_dataset, path='json', data_files=dict(train=data_path)),
    tokenizer=tokenizer,
    max_length=max_length,
-   dataset_map_fn=alpaca_map_fn,
+   dataset_map_fn=None,
    template_map_fn=dict(
        type=template_map_fn_factory, template=prompt_template),
    remove_unused_columns=True,
    shuffle_before_pack=True,
    pack_to_max_length=pack_to_max_length)
```
#### 3.3.2 **XTunerï¼å¯åŠ¨ï¼**

![tH8udZzECYl5are.png](imgs/ysqd.png)

```bash
xtuner train internlm_chat_7b_qlora_medqa2019_e3.py --deepspeed deepspeed_zero2
```

#### 3.3.3 pth è½¬ huggingface

åŒå‰è¿°ï¼Œè¿™é‡Œä¸èµ˜è¿°äº†ã€‚[å°†å¾—åˆ°çš„-pth-æ¨¡å‹è½¬æ¢ä¸º-huggingface-æ¨¡å‹å³ç”Ÿæˆadapteræ–‡ä»¶å¤¹](#236-å°†å¾—åˆ°çš„-pth-æ¨¡å‹è½¬æ¢ä¸º-huggingface-æ¨¡å‹å³ç”Ÿæˆadapteræ–‡ä»¶å¤¹)  

#### 3.3.4 éƒ¨ç½²ä¸æµ‹è¯•

åŒå‰è¿°ã€‚[éƒ¨ç½²ä¸æµ‹è¯•](#24-éƒ¨ç½²ä¸æµ‹è¯•)


## 4ã€è¡¥å……ã€‘ç”¨ MS-Agent æ•°æ®é›† èµ‹äºˆ LLM ä»¥ Agent èƒ½åŠ›
### 4.1 æ¦‚è¿°

MSAgent æ•°æ®é›†æ¯æ¡æ ·æœ¬åŒ…å«ä¸€ä¸ªå¯¹è¯åˆ—è¡¨ï¼ˆconversationsï¼‰ï¼Œå…¶é‡Œé¢åŒ…å«äº† systemã€userã€assistant ä¸‰ç§å­—æ®µã€‚å…¶ä¸­ï¼š

- system: è¡¨ç¤ºç»™æ¨¡å‹å‰ç½®çš„äººè®¾è¾“å…¥ï¼Œå…¶ä¸­æœ‰å‘Šè¯‰æ¨¡å‹å¦‚ä½•è°ƒç”¨æ’ä»¶ä»¥åŠç”Ÿæˆè¯·æ±‚

- user: è¡¨ç¤ºç”¨æˆ·çš„è¾“å…¥ promptï¼Œåˆ†ä¸ºä¸¤ç§ï¼Œé€šç”¨ç”Ÿæˆçš„promptå’Œè°ƒç”¨æ’ä»¶éœ€æ±‚çš„ prompt

- assistant: ä¸ºæ¨¡å‹çš„å›å¤ã€‚å…¶ä¸­ä¼šåŒ…æ‹¬æ’ä»¶è°ƒç”¨ä»£ç å’Œæ‰§è¡Œä»£ç ï¼Œè°ƒç”¨ä»£ç æ˜¯è¦ LLM ç”Ÿæˆçš„ï¼Œè€Œæ‰§è¡Œä»£ç æ˜¯è°ƒç”¨æœåŠ¡æ¥ç”Ÿæˆç»“æœçš„

ä¸€æ¡è°ƒç”¨ç½‘é¡µæœç´¢æ’ä»¶æŸ¥è¯¢â€œä¸Šæµ·æ˜å¤©å¤©æ°”â€çš„æ•°æ®æ ·æœ¬ç¤ºä¾‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![BlgfEqpiRFO5G6L.png](imgs/msagent_data.png)

### 4.2 å¾®è°ƒæ­¥éª¤

#### 4.2.1 å‡†å¤‡å·¥ä½œ
> xtuner æ˜¯ä»å›½å†…çš„ ModelScope å¹³å°ä¸‹è½½ MS-Agent æ•°æ®é›†ï¼Œå› æ­¤ä¸ç”¨æå‰æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†æ–‡ä»¶ã€‚

```bash
# å‡†å¤‡å·¥ä½œ
mkdir ~/ft-msagent && cd ~/ft-msagent
cp -r ~/ft-oasst1/internlm-chat-7b .

# æŸ¥çœ‹é…ç½®æ–‡ä»¶
xtuner list-cfg | grep msagent

# å¤åˆ¶é…ç½®æ–‡ä»¶åˆ°å½“å‰ç›®å½•
xtuner copy-cfg internlm_7b_qlora_msagent_react_e3_gpu8 .

# ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹ä¸ºæœ¬åœ°è·¯å¾„
vim ./internlm_7b_qlora_msagent_react_e3_gpu8_copy.py 
```

```diff
- pretrained_model_name_or_path = 'internlm/internlm-chat-7b'
+ pretrained_model_name_or_path = './internlm-chat-7b'
```

#### 4.2.2 å¼€å§‹å¾®è°ƒ
```Bash
xtuner train ./internlm_7b_qlora_msagent_react_e3_gpu8_copy.py --deepspeed deepspeed_zero2
```

### 4.3 ç›´æ¥ä½¿ç”¨

> ç”±äº msagent çš„è®­ç»ƒéå¸¸è´¹æ—¶ï¼Œå¤§å®¶å¦‚æœæƒ³å°½å¿«æŠŠè¿™ä¸ªæ•™ç¨‹è·Ÿå®Œï¼Œå¯ä»¥ç›´æ¥ä» modelScope æ‹‰å–å’±ä»¬å·²ç»å¾®è°ƒå¥½äº†çš„ Adapterã€‚å¦‚ä¸‹æ¼”ç¤ºã€‚

#### 4.3.1 ä¸‹è½½ Adapter
```Bash
cd ~/ft-msagent
apt install git git-lfs
git lfs install
git lfs clone https://www.modelscope.cn/xtuner/internlm-7b-qlora-msagent-react.git
```

OKï¼Œç°åœ¨ç›®å½•åº”è¯¥é•¿è¿™æ ·ï¼š
- internlm_7b_qlora_msagent_react_e3_gpu8_copy.py
- internlm-7b-qlora-msagent-react
- internlm-chat-7b
- work_dirï¼ˆå¯æœ‰å¯æ— ï¼‰

æœ‰äº†è¿™ä¸ªåœ¨ msagent ä¸Šè®­ç»ƒå¾—åˆ°çš„Adapterï¼Œæ¨¡å‹ç°åœ¨å·²ç»æœ‰ agent èƒ½åŠ›äº†ï¼å°±å¯ä»¥åŠ  --lagent ä»¥è°ƒç”¨æ¥è‡ª lagent çš„ä»£ç†åŠŸèƒ½äº†ï¼

#### 4.3.2 æ·»åŠ  serper ç¯å¢ƒå˜é‡

> **å¼€å§‹ chat ä¹‹å‰ï¼Œè¿˜è¦åŠ ä¸ª serper çš„ç¯å¢ƒå˜é‡ï¼š**
> 
> å» serper.dev å…è´¹æ³¨å†Œä¸€ä¸ªè´¦å·ï¼Œç”Ÿæˆè‡ªå·±çš„ api keyã€‚è¿™ä¸ªä¸œè¥¿æ˜¯ç”¨æ¥ç»™ lagent å»è·å– google æœç´¢çš„ç»“æœçš„ã€‚ç­‰äºæ˜¯ serper.dev å¸®ä½ å»è®¿é—® googleï¼Œè€Œä¸æ˜¯ä»ä½ è‡ªå·±æœ¬åœ°å»è®¿é—® google äº†ã€‚

![kDSdpQrhHfTWYsc.png](imgs/serper.png)

æ·»åŠ  serper api key åˆ°ç¯å¢ƒå˜é‡ï¼š

```bash
export SERPER_API_KEY=abcdefg
```

#### 4.3.3 xtuner + agentï¼Œå¯åŠ¨ï¼

```bash
xtuner chat ./internlm-chat-7b --adapter internlm-7b-qlora-msagent-react --lagent
```


#### 4.3.4 æŠ¥é”™å¤„ç†

xtuner chat å¢åŠ  --lagent å‚æ•°åï¼ŒæŠ¥é”™ ```TypeError: transfomers.modelsauto.auto factory. BaseAutoModelClass.from pretrained() got multiple values for keyword argument "trust remote code"```	

æ³¨é‡Šæ‰å·²å®‰è£…åŒ…ä¸­çš„ä»£ç ï¼š

```bash
vim /root/xtuner019/xtuner/xtuner/tools/chat.py
```



![NfHAV1b4zqYv5kR.png](imgs/bugfix1.png)

![YTpz1qemiojk5Bg.png](imgs/bugfix2.png)


## 5 å…¶ä»–å·²çŸ¥é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼š
https://docs.qq.com/doc/DY1d2ZVFlbXlrUERj


å°ä½œä¸šåŠ©æ•™è€å¸ˆä¼šåœ¨ç¤¾ç¾¤ä¸­å…¬å¸ƒã€‚
Have fun!



## 6 æ³¨æ„äº‹é¡¹

æœ¬æ•™ç¨‹ä½¿ç”¨ xtuner 0.1.9 ç‰ˆæœ¬
è‹¥éœ€è¦è·Ÿç€æœ¬æ•™ç¨‹ä¸€æ­¥ä¸€æ­¥å®Œæˆï¼Œå»ºè®®ä¸¥æ ¼éµå¾ªæœ¬æ•™ç¨‹çš„æ­¥éª¤ï¼



è‹¥å‡ºç°è«åå…¶å¦™æŠ¥é”™ï¼Œè¯·å°è¯•æ›´æ¢ä¸ºä»¥ä¸‹åŒ…çš„ç‰ˆæœ¬ï¼šï¼ˆå¦‚æœæœ‰æŠ¥é”™å†æ£€æŸ¥ï¼Œæ²¡æŠ¥é”™ä¸ç”¨çœ‹ï¼‰
```
torch                         2.1.1
transformers                  4.34.0
transformers-stream-generator 0.0.4
```
```bash
pip install torch==2.1.1
pip install transformers==4.34.0
pip install transformers-stream-generator=0.0.4
```
CUDA ç›¸å…³ï¼šï¼ˆå¦‚æœæœ‰æŠ¥é”™å†æ£€æŸ¥ï¼Œæ²¡æŠ¥é”™ä¸ç”¨çœ‹ï¼‰
```
NVIDIA-SMI 535.54.03              
Driver Version: 535.54.03    
CUDA Version: 12.2

nvidia-cuda-cupti-cu12        12.1.105
nvidia-cuda-nvrtc-cu12        12.1.105
nvidia-cuda-runtime-cu12      12.1.105
```

## 7 ä½œä¸š

**åŸºç¡€ä½œä¸šï¼š**

æ„å»ºæ•°æ®é›†ï¼Œä½¿ç”¨ XTuner å¾®è°ƒ InternLM-Chat-7B æ¨¡å‹, è®©æ¨¡å‹å­¦ä¹ åˆ°å®ƒæ˜¯ä½ çš„æ™ºèƒ½å°åŠ©æ‰‹ï¼Œæ•ˆæœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæœ¬ä½œä¸šè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹çš„è¾“å‡ºéœ€è¦**å°†ä¸è¦è‘±å§œè’œå¤§ä½¬**æ›¿æ¢æˆè‡ªå·±åå­—æˆ–æ˜µç§°ï¼

**å¾®è°ƒå‰**ï¼ˆå›ç­”æ¯”è¾ƒå®˜æ–¹ï¼‰
![web_show_2.png](imgs%2Fweb_show_2.png)


**å¾®è°ƒå**ï¼ˆå¯¹è‡ªå·±çš„èº«ä»½æœ‰äº†æ¸…æ™°çš„è®¤çŸ¥ï¼‰
![web_show_1.png](imgs%2Fweb_show_1.png)

ä½œä¸šå‚è€ƒç­”æ¡ˆï¼šhttps://github.com/InternLM/tutorial/blob/main/xtuner/self.md

**è¿›é˜¶ä½œä¸šï¼š**

- å°†è®­ç»ƒå¥½çš„Adapteræ¨¡å‹æƒé‡ä¸Šä¼ åˆ° OpenXLabã€Hugging Face æˆ–è€… MoelScope ä»»ä¸€ä¸€å¹³å°ã€‚
- å°†è®­ç»ƒå¥½åçš„æ¨¡å‹åº”ç”¨éƒ¨ç½²åˆ° OpenXLab å¹³å°ï¼Œå‚è€ƒéƒ¨ç½²æ–‡æ¡£è¯·è®¿é—®ï¼šhttps://aicarrier.feishu.cn/docx/MQH6dygcKolG37x0ekcc4oZhnCe

**æ•´ä½“å®è®­è¥é¡¹ç›®ï¼š**

æ—¶é—´å‘¨æœŸï¼šå³æ—¥èµ·è‡´è¯¾ç¨‹ç»“æŸ

å³æ—¥å¼€å§‹å¯ä»¥åœ¨ç­çº§ç¾¤ä¸­éšæœºç»„é˜Ÿå®Œæˆä¸€ä¸ªå¤§ä½œä¸šé¡¹ç›®ï¼Œä¸€äº›å¯æä¾›çš„é€‰é¢˜å¦‚ä¸‹ï¼š

- äººæƒ…ä¸–æ•…å¤§æ¨¡å‹ï¼šä¸€ä¸ªå¸®åŠ©ç”¨æˆ·æ’°å†™æ–°å¹´ç¥ç¦æ–‡æ¡ˆçš„äººæƒ…äº‹æ•…å¤§æ¨¡å‹
- ä¸­å°å­¦æ•°å­¦å¤§æ¨¡å‹ï¼šä¸€ä¸ªæ‹¥æœ‰ä¸€å®šæ•°å­¦è§£é¢˜èƒ½åŠ›çš„å¤§æ¨¡å‹
- å¿ƒç†å¤§æ¨¡å‹ï¼šä¸€ä¸ªæ²»æ„ˆçš„å¿ƒç†å¤§æ¨¡å‹
- å·¥å…·è°ƒç”¨ç±»é¡¹ç›®ï¼šç»“åˆ Lagent æ„å»ºæ•°æ®é›†è®­ç»ƒ InternLM æ¨¡å‹ï¼Œæ”¯æŒå¯¹ MMYOLO ç­‰å·¥å…·çš„è°ƒç”¨

å…¶ä»–åŸºäºä¹¦ç”ŸÂ·æµ¦è¯­å·¥å…·é“¾çš„å°é¡¹ç›®éƒ½åœ¨èŒƒå›´å†…ï¼Œæ¬¢è¿å¤§å®¶å……åˆ†å‘æŒ¥æƒ³è±¡åŠ›ã€‚

